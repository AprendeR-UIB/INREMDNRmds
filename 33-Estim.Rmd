# Distribuciones muestrales

## Estimadores

En un problema típico de **estadística inferencial**:

* Queremos conocer el valor de una característica en el total de una población, pero no podemos medir esta característica en **todos** los individuos de la población.

* Entonces, extraemos una muestra de la población, medimos la característica en los individuos de esta muestra, calculamos algo con los datos obtenidos e **inferimos** el valor de la característica en el global de la población.

Inmediatamente surgen varias preguntas, que responderemos entre esta lección y la próxima:

* ¿Cómo tiene que ser la muestra?
* ¿Qué tenemos que calcular?
* ¿Con qué precisión podemos inferir la característica de la población?

¿Qué tipo de muestra tenemos que tomar? Vamos a suponer de ahora en adelante que tomamos **muestras aleatorias simples**. Esto incluye las muestras aleatorias sin reposición si la población es mucho más grande que la muestra, ya que entonces no hay diferencia práctica entre permitir y prohibir las repeticiones. En algunos casos muy concretos permitiremos muestras aleatorias sin reposición en general.

```{block2,type="rmdromans"}
Sí, ya sabemos que en la práctica casi nunca tomamos muestras aleatorias, sino oportunistas. En este caso, recordad lo que os explicábamos en la Sección \@ref(sec:oport). Lo que hay que hacer es describir en detalle las características de la muestra para justificar que, pese a no ser aleatoria, es razonablemente representativa de la población y podría pasar por  aleatoria.
```


¿Qué calculamos? Pues un **estimador**: alguna función adecuada aplicada a los valores de la muestra, y que dependerá de lo que queramos estimar.  Por ejemplo:

* Si queremos estimar la altura media de los estudiantes de la UIB, tomaremos una muestra aleatoria de estudiantes de la UIB, mediremos sus alturas y calcularemos su **media aritmética**.

* Si queremos estimar la proporción de estudiantes de la UIB que han pasado la COVID-19, tomaremos una muestra aleatoria de estudiantes de la UIB, les haremos un test de anticuerpos y calcularemos la **proporción muestral** de positivos en la muestra.

* Si queremos estimar el riesgo relativo para un estudiante de la UIB de suspender alguna asignatura si es fumador, tomaremos una muestra aleatoria de estudiantes de la UIB, anotaremos si fuman o no y si han suspendido alguna asignatura o no, y calcularemos el **cociente entre las proporciones muestrales** de suspensos entre los fumadores y los   no fumadores de la muestra.

```{block2,type="rmdimportant"}
**Un estimador es una variable aleatoria**, definida sobre la población formada por las muestras de la población de partida. Por lo tanto, tiene función de densidad, función de distribución (que genéricamente llamaremos **distribución muestral**, para indicar que mide la probabilidad de que le pase algo al valor del **estimador sobre una muestra**), esperanza, desviación típica, etc.
```

## La media muestral

Cuando queremos estimar el valor medio de una variable sobre una población, tomamos una muestra de valores y calculamos su media aritmética, ¿verdad? Pues eso es la media muestral. 

Dada una variable aleatoria $X$, llamamos **media muestral de** (muestras de) **tamaño $n$** a la variable aleatoria $\overline{X}$ "Tomamos una *muestra aleatoria simple* de tamaño $n$ de $X$ y calculamos la media aritmética de sus valores".

```{block2,type="rmdcaution"}
Fijaos en que definimos la media muestral solo para muestras aleatorias simples. Naturalmente, tiene sentido definirla para muestras cualesquiera, pero entonces su distribución muestral dejaría de cumplir las propiedades que damos en esta sección. La misma advertencia vale para los estimadores que definimos en las próximas secciones.
```

Veamos algunas propiedades de la distribución muestral de  $\overline{X}$:

```{theorem,mitjmostgral}
Sea $X$ una variable aleatoria cualquiera de media $\mu_X$ y desviación típica $\sigma_X$, y sea $\overline{X}$ la media muestral de tamaño $n$ de $X$. Entonces:

* $E(\overline{X})=\mu_X$

* $\sigma(\overline{X})=\dfrac{\sigma_X}{\sqrt{n}}$

```


```{block2,type="rmdcorbes"}
Formalmente, la media muestral de tamaño $n$ de una variable aleatoria $X$ se  define como la variable aleatoria 
$$
\overline{X}=\frac{X_1+\cdots+X_n}{n}
$$
donde  $X_1,\ldots,X_n$  son $n$ copias independientes de la variable $X$.

Entonces, por la linealidad de la esperanza 
$$
  E(\overline{X})=\frac{E(X_1)+\cdots+E(X_n)}{n}=\frac{n\cdot \mu_X}{n}=\mu_X
$$
porque, como $X_1,\ldots,X_n$ son copias de $X$,
$E(X_1)=\cdots=E(X_n)=\mu_X$.

Y por la linealidad de la varianza de la suma de variables **independientes**
$$
\sigma(\overline{X})^2=\frac{\sigma(X_1)^2+\cdots+\sigma(X_n)^2}{n^2}=\frac{n\cdot \sigma_X^2}{n^2}=\frac{\sigma_X^2}{n}
$$
porque, de nuevo, como $X_1,\ldots,X_n$ son copias de $X$,
$\sigma(X_1)^2=\cdots=\sigma(X_n)^2=\sigma_X^2$.

```



Que $E(\overline{X})$ sea $\mu_X$ nos indica que $\overline{X}$ sirve para estimar $\mu_X$, porque **su valor esperado es $\mu_X$**:

> Si calculáramos muchas medias de muestras aleatorias de $X$, es muy probable que, de media, obtuviéramos un valor muy cercano a $\mu_X$.

Cuando el valor esperado de un estimador es precisamente el parámetro poblacional que se quiere estimar, se dice que el estimador es **insesgado**. Así, el primer punto del teorema anterior dice que la media muestral $\overline{X}$ es un estimador insesgado de la media poblacional $\mu_X$.

Que $\sigma(\overline{X})$ sea $\sigma_X/\sqrt{n}$ implica que la variabilidad de las medias muestrales crece con la variabilidad de $X$ y decrece si tomamos muestras de mayor tamaño. Esto último es razonable. Aunque la variabilidad de $X$ sea grande, si tomamos muestras grandes, es  de esperar que los valores extremos se compensen al calcular sus medias y que estas últimas  tengan por lo tanto menos variabilidad que la variable $X$ original. 

A $\sigma_X/\sqrt{n}$ se le llama el **error típico de la media muestral** (para la variable aleatoria $X$ y muestras de tamaño $n$).



```{example,simulacion1}
Vamos a realizar un experimento. Vamos a tomar una población de 10^6^ sujetos y una variable aleatoria $X$ que sobre cada sujeto toma un valor real entre 0 y 1, todos estos valores con la misma probabilidad. Llamaremos `X` al vector con los 10^6^ valores de esta variable aleatoria, y dibujaremos un histograma de este vector de números para que veáis que los valores salen muy dispersos. Mostramos el código de R para que podáis repetir el experimento por vuestra cuenta; como es una simulación, cada vez que lo ejecutéis dará resultados diferentes, pero el mismo efecto global.

```

```{r,echo=FALSE}
set.seed(1000)
```


```{r}
X=runif(10^6)
hist(X,freq=FALSE,main="Histograma de X",xlab="",ylab="Densidad",col="light blue")
```

La desviación típica $\sigma_X$ de la variable $X$ sobre nuestra población es
```{r}
sd(X)*sqrt((10^6-1)/10^6)
```

```{block2, type="rmdnote"}
La función `sd` calcula la desviación típica muestral, con denominador $\sqrt{n-1}$. Para calcular la desviación típica de verdad, con denominador $\sqrt{n}$ hay que multiplicarla por $\sqrt{n-1}$ y dividirla por $\sqrt{n}$.
```

Ahora vamos a tomar 1000 medias muestrales de tamaño 100 de esta población, las organizaremos en un vector que llamaremos `Medias` y dibujaremos un histograma de este vector de medias.

```{r}
Medias=replicate(1000,mean(sample(X,100,replace=TRUE)))
hist(Medias, breaks=15,freq=FALSE,main="Histograma de las medias muestrales",xlab="",ylab="Densidad",col="light blue",xlim=c(0.4,0.6))
```

Podéis observar cómo los valores de estas medias se concentran alrededor de 0.5. Veamos su desviación típica:

```{r}
sd(Medias)*sqrt((1000-1)/1000)
```

Fijaos cómo se acerca mucho al valor $\sigma_X/\sqrt{100}=`r sd(X)*sqrt((10^6-1)/10^6)/10`$ predicho por el teorema anterior. No coinciden exactamente, porque $\sigma_X/\sqrt{100}$ es el valor de la desviación típica **poblacional** de $\overline{X}$, es decir, para toda la "población" de medias muestrales de muestras aleatorias simples de tamaño 100 de nuestra población de partida, y nosotros hemos tomado una muestra de "solo" 1000 medias de estas.

La media muestral $\overline{X}$ de tamaño $n$ de una variable aleatoria $X$ se  interpreta formalmente como la variable aleatoria obtenida tomando $n$ copias independientes $X_1,\ldots,X_n$ de $X$ y calculando
$$
\overline{X}=\frac{X_1+\cdots+X_n}{n}.
$$

Por lo tanto, es una combinación lineal de $n$ copias independientes de $X$. 
Recordando que una combinación de variables aleatorias normales independientes es normal, tenemos el resultado siguiente:

```{theorem}
Si $X$ es $N(\mu_X,\sigma_X)$, entonces $\overline{X}$ es $N(\mu_X,\sigma_X/\sqrt{n})$,
y por lo tanto
$$
Z=\frac{\overline{X}-\mu_X}{\sigma_X/\sqrt{n}}
$$
es $N(0,1)$. 

  
```

Si $X$ no es normal, la tesis del teorema anterior sigue siendo cierta "aproximadamente" siempre y cuando $n$ sea grande. Este resultado, llamado el **Teorema Central del Límite** es, como su nombre indica, uno de los más importantes en estadística y el motivo de la importancia de la distribución normal.

```{theorem}
Sea $X$ una variable aleatoria **cualquiera** de esperanza $\mu_X$ y desviación típica $\sigma_X$. Si $n$ es **muy grande**,
$\overline{X}$ es **aproximadamente** $N(\mu_X, {\sigma_X}/{\sqrt{n}})$
y por lo tanto
$$
Z=\frac{\overline{X}-\mu_X}{{\sigma_X}/{\sqrt{n}}}
$$
es aproximadamente $N(0,1)$.  

  
```

Dos observaciones:

* ¿Cuándo una muestra es lo bastante grande como para poder invocar el Teorema Central del Límite? En realidad, depende de la $X$. Cuánto más se parezca $X$ a una variable normal, más pequeñas pueden ser la muestras. Por fijar un valor, aceptaremos que "muy grande" en este teorema es $n\geq 40$.

* ¿Qué quiere decir que una variable aleatoria sea "aproximadamente" normal? Pues que su función de distribución $F_X$ toma valores muy cercanos a la función de distribución de una normal. Recordad cómo una $B(n,p)$ con $n$ grande era "aproximadamente normal" en la lección anterior.

```{block2,type="rmdnote"}
Si miráis el histograma de las 1000 medias muestrales del Ejemplo \@ref(exm:simulacion1), veréis que se parece al de una muestra de una variable normal. Es que $\overline{X}$ es aproximadamente normal, por el Teorema Central del Límite, aunque la variable $X$ sea muy diferente de una normal. Para verlo, en la figura que sigue superponemos al histograma de las medias la gráfica de la densidad de una variable normal de media y desviación típica las predichas por el Teorema Central del Límite.
```

```{r,echo=FALSE}
hist(Medias, breaks=15,freq=FALSE,main="Histograma de las medias muestrales",xlab="",ylab="Densidad",col="light blue",xlim=c(0.4,0.6))
curve(dnorm(x,mean(X),sd(X)*sqrt((10^6-1)/10^6)/sqrt(100)),lwd=2,col="red",add=TRUE)
```


```{block2,type="rmdrecordau"}
En resumen:

* Si $X$ es normal, $\overline{X}$ es $N(\mu_X,{\sigma_X}/{\sqrt{n}})$.

* Si $X$ no es normal pero $n$ es grande (pongamos $n\geq 40$, aunque puede ser menor si $X$ se parece a una normal y seguramente tendrá que ser mayor si $X$ es muy diferente de una normal),  $\overline{X}$ es *aproximadamente* $N(\mu_X,{\sigma_X}/{\sqrt{n}})$.

```

```{block2,type="rmdcaution"}
Las afirmaciones del bloque anterior son verdaderas para medias muestrales de muestras aleatorias simples. Si no podemos suponer que la muestra sea aleatoria simple, ninguno de los dos resultados es válido. 
``` 

## La proporción muestral

Cuando queremos estimar la proporción de sujetos de una población que tienen una determinada característica, tomamos una muestra y calculamos la proporción de sujetos de la muestra con esta característica. Esta será la **proporción muestral** de sujetos  con esta característica en nuestra muestra.

Dada una variable aleatoria $X$  de Bernoulli $Be(p_X)$, la **proporción muestral de** (muestras de) **tamaño $n$**,  $\widehat{p}_X$, es la variable aleatoria consistente en tomar una muestra aleatoria de tamaño $n$ de $X$ y calcular la proporción de éxitos en la muestra: es decir, contar el número total de éxitos  y dividir el resultado por $n$.

Fijaos en que $\widehat{p}_X$ es un caso particular de media muestral $\overline{X}$: estamos calculando medias muestrales de muestras aleatorias simples de la variable de Bernoulli $X$. Por lo tanto, todo lo que hemos dicho  para medias muestrales vale también para proporciones muestrales:

```{theorem}
Si $X$ es una variable aleatoria de Bernoulli con probabilidad poblacional de éxito $p_X$ y  $\widehat{p}_X$ es la proporción muestral de tamaño $n$:
    
* $E(\widehat{p}_X)=p_X$

*  $\sigma({\widehat{p}_X})=\sqrt{\dfrac{p_X(1-p_X)}{n}}$


```    



$E(\widehat{p}_X)=p_X$ nos dice que $\widehat{p}_X$ es un estimador insesgado de $p_X$. Si calculáramos muchas proporciones muestrales de muestras aleatorias de $X$, es muy probable que, de media, obtuviéramos un valor muy cercano a  la proporción poblacional de éxitos $p_X$. 

$\sigma({\widehat{p}_X})=\sqrt{\dfrac{p_X(1-p_X)}{n}}$ nos dice que, fijada la variable $X$, si tomamos muestras de tamaño mayor, la variabilidad de los resultados de $\widehat{p}_X$ disminuye.
    
```{block2,type="rmdimportant"}
En el caso de la proporción muestral, a veces vamos a permitir tomar **muestras aleatorias sin reposición**. En este caso, seguimos teniendo que $E(\widehat{p}_X)=p_X$, pero ahora,
si $N$ es el tamaño de la población, 
$$
\sigma({\widehat{p}_X})=\sqrt{\frac{p_X(1-p_X)}{n}}\cdot
\sqrt{\frac{\vphantom{(p_X}N-n}{N-1}}.
$$
El factor
$$
\sqrt{\frac{N-n}{N-1}}
$$ 
que transforma $\sigma({\widehat{p}_X})$ para muestras aleatorias simples en la desviación típica de ${\widehat{p}_X}$ para muestras aleatorias sin reposición es el **factor de población finita**  que transformaba la desviación típica de una variable binomial (que cuenta éxitos en muestras aleatorias simples) en la desviación típica de una variable hipergeométrica  (que cuenta éxitos en muestras aleatorias sin reposición).
```

```{block2,type="rmdrecordau"}
Y recordad que si el tamaño de la población $N$ es muy grande comparado con $n$, podemos suponer que una muestra aleatoria sin reposición es simple.
```
    
 
 
Si tomamos muestras aleatorias simples de tamaño $n$ de una variable aleatoria Bernoulli $X$:

* $\sqrt{\dfrac{p_X(1-p_X)}{n}}$ es el **error típico** de la variable aleatoria $\widehat{p}_X$: su desviación típica.

* Para cada muestra, $\sqrt{\dfrac{\widehat{p}_X(1-\widehat{p}_X)}{n}}$ es el **error típico** de la muestra, que estima el error típico de $\widehat{p}_X$.


Y como la proporción muestral es un caso particular de media muestral, por el Teorema Central del Límite tenemos el resultado siguiente:

```{theorem}
Si $n$ es grande y las muestras aleatorias  son simples, 
$\widehat{p}_X$ es aproximadamente  $N\big (p_X,\sqrt{{p_X(1-p_X)}/{n}}\big)$
y por lo tanto
$$
\frac{\widehat{p}_X-p_X}{\sqrt{\frac{{p}_X(1-{p}_X)}{n}}}
$$
es aproximadamente $N(0,1)$.

```

## La varianza muestral

Dada una variable aleatoria $X$, llamamos:

* **Varianza muestral de** (muestras de) **tamaño $n$**, $\widetilde{S}_{X}^2$, a la variable aleatoria consistente en tomar una muestra aleatoria simple de tamaño $n$ de $X$ y calcular la varianza muestral de sus valores.

* **Desviación típica muestral de** (muestras de) **tamaño $n$**, $\widetilde{S}_{X}$, a la variable aleatoria consistente en tomar una muestra aleatoria simple de tamaño $n$ de $X$ y calcular la desviación típica muestral de sus valores.



Formalmente, estas variables se definen tomando $n$ copias independientes $X_1,\ldots,X_n$ de $X$ y calculando
$$
\widetilde{S}_{X}^2=\frac{\sum_{i=1}^n (X_{i}-\overline{X})^2}{n-1},\quad 
\widetilde{S}_{X}=+\sqrt{\widetilde{S}_{X}^2}
$$

Tenemos los dos resultados siguientes. El primero nos dice que $\widetilde{S}_{X}^2$ es un estimador insesgado de la varianza poblacional $\sigma_{X}^2$.

```{theorem}
$E(\widetilde{S}_{X}^2)=\sigma_{X}^2$.


```

Por lo tanto, **esperamos** que la varianza muestral de una muestra aleatoria simple de $X$ valga $\sigma_{X}^2$, en el sentido usual de que si tomamos muestras aleatorias simples de $X$ de tamaño $n$ grande y calculamos sus varianzas muestrales, muy probablemente obtengamos de media un valor muy cercano a $\sigma_{X}^2$.

```{block2,type="rmdcaution"}
Y por lo tanto **no esperamos** que la varianza "a secas" de una muestra aleatoria simple valga $\sigma_{X}^2$, porque la varianza muestral y la varianza "a secas" dan valores diferentes (tienen el mismo numerador y denominadores diferentes).
```

El segundo resultado nos dice que si la variable $X$ es **normal**, un  múltiplo adecuado de $\widetilde{S}_{X}^2$ tiene  distribución muestral conocida, lo que nos permitirá calcular probabilidades de sucesos relativos a  $\widetilde{S}_{X}^2$.


```{theorem,khi2}
Si $X$ es $N(\mu_X,\sigma_X)$ y tomamos muestras de tamaño $n$, la variable aleatoria 
$$
\chi^2=  \dfrac{(n-1)\widetilde{S}_{X}^2}{\sigma_{X}^2}
$$ 
tiene una distribución conocida, llamada **ji cuadrado con $n-1$ grados de libertad**, $\chi_{n-1}^2$.


```

```{block2,type="rmdrecordau"}
La letra griega $\chi$  en castellano se lee **ji**; en catalán, **khi**;  en inglés, **chi**, pronunciado "xai".
```



```{r, echo=FALSE, out.width="40%", fig.cap="Chi es un gatito, la distribución és ji."}
knitr::include_graphics("INREMDN_files/figure-html/chihogar.png")
```

La distribución $\chi_\nu^2$, donde $\nu$ es un parámetro llamado sus **grados de libertad**, es la distribución de probabilidad de la suma de los cuadrados de $\nu$ variables aleatorias normales estándar independientes. Para R es `chisq`. Os puede interesar recordar que una variable $\chi_\nu^2$ de tipo ji cuadrado con $\nu$ grados de libertad:

* Tiene valor esperado $E(\chi_\nu^2)=\nu$ y  varianza  $\sigma(\chi_\nu^2)^2=2 \nu$.

* Su función de distribución es estrictamente creciente.

* Tiene una distribución asimétrica a la derecha, como muestra el gráfico siguiente:

```{r,echo=FALSE}
curve(dchisq(x,1),col=1,lwd=2,xlim=c(0,20),xlab="",ylab="",ylim=c(0,0.3),main="Algunas ji quadrado")
curve(dchisq(x,2),col=2,lwd=2,add=TRUE)
curve(dchisq(x,3),col=3,lwd=2,add=TRUE)
curve(dchisq(x,4),col=4,lwd=2,add=TRUE)
curve(dchisq(x,5),col=5,lwd=2,add=TRUE)
curve(dchisq(x,10),col=6,lwd=2,add=TRUE)
legend("topright",col=1:6,lty=c(1,1),
       lwd=c(2,2),legend=paste("g.l.=",c(1:5,10),sep=""),cex=0.8)
```

A medida que el número de grados de libertad $\nu$ crece, la asimetría tiende a desaparecer y, por el Teorema Central del Límite, si $\nu$ es lo bastante grande, la distribución $\chi_\nu^2$ se aproxima a la de una variable normal $N(\nu,\sqrt{2\nu})$.

```{r,echo=FALSE}
curve(dchisq(x,300),xlim=c(150,450),lwd=2,xlab="",ylab="",main="Ji quadrado vs Normal")
curve(dnorm(x,300,sqrt(600)),lwd=2,col="red",add=TRUE)
legend("topleft",col=c("black","red"),lty=c(1,1),
       lwd=c(2,2),legend=c("Ji quadrado con 300 g.l.","Normal"),cex=0.7)
```

```{block2,type="rmdcaution"}
Tened cuidado:

* Si la variable poblacional $X$ no es normal, la conclusión del Teorema \@ref(thm:khi2) no es verdadera.

* Aunque $X$ sea normal, $E(\widetilde{S}_{X})\neq \sigma_{X}$. La desviación típica muestral es un estimador **sesgado** de $\sigma_{X}$ (pero tiene otras buenas propiedades que hacen que la usemos igualmente).

* Ya lo hemos comentado antes. Si $S^2_{X}$ es la varianza "a secas" (dividiendo por $n$ en vez de por $n-1$), $E(S^2_{X})\neq \sigma^2_{X}$. Esto lo podéis comprobar fácilmente, porque $S_X^2$ se obtiene a partir de $\widetilde{S}_{X}^2$ cambiando el denominador,
$$
  S_X^2=\frac{n-1}{n} \widetilde{S}_{X}^2
$$
y por lo tanto
$$
E(S_X^2)=\frac{n-1}{n}E(\widetilde{S}_{X}^2)=\frac{n-1}{n}\sigma^2_{X}
$$

```  



## La distribución t de Student

Recordad que si la variable poblacional $X$ es $N(\mu_X,\sigma_X)$  y tomamos muestras aleatorias simples de tamaño $n$, entonces la variable
$$
\frac{\overline{X}-\mu_X}{\sigma_{X}/\sqrt{n}}
$$
es normal estándar. Esto no nos sirve de mucho para calcular la probabilidad de que $\overline{X}$ se aleje mucho de $\mu_X$, porque casi nunca sabemos la desviación típica poblacional $\sigma_{X}$. ¿Qué pasa si la estimamos por medio de $\widetilde{S}_{X}$ con la misma muestra con la que calculamos $\overline{X}$? Pues que el resultado siguiente nos salva el día, porque la variable que resulta tiene distribución conocida y por lo tanto podemos calcular probabilidades con ella.


```{theorem}
Sea $X$ una variable $N(\mu_X,\sigma_X)$. Si tomamos muestras aleatorias simples de tamaño $n$, la variable aleatoria
$$
T=\frac{\overline{X}-\mu_X}{\widetilde{S}_{X}/\sqrt{n}}
$$
tiene una distribución conocida, llamada **t de Student con $n-1$ grados de libertad**, $t_{n-1}$.
  
```

Al denominador $\widetilde{S}_{X}/\sqrt{n}$ de la $T$ del teorema anterior se le llama el **error típico** de la muestra, y estima el error típico $\sigma_X/\sqrt{n}$ de la media muestral $\overline{X}$.

```{block2,type="rmdimportant"}
Fijaos en que el teorema anterior es solo para variables poblacionales $X$ **normales**. Si $X$ no es normal, es necesario que $n$ sea muy grande para que, por el Teorema Central del Límite, $T$ sea aproximadanente $t_{n-1}$.
```

Para R, la distribución t de Student es `t`, a secas. Algunas propiedades que conviene que recordéis de las variables $T_\nu$ que tienen distribución $t$ de Student con $\nu$ grados de libertad, $t_\nu$:


* Su valor esperado es $E(T_\nu)=0$  y su varianza es $\sigma(T_\nu)=\dfrac{\nu}{\nu-2}$ (en realidad esto solo es verdad si $\nu\geq 3$, pero no hace falta recordarlo).

* Su función de distribución es estrictamente creciente.


* Su función de distribución es simétrica respecto de 0 (como la de una $N(0,1)$):
$$
P(T_\nu\leq -x)=P(T_\nu\geq x)=1-P(T_\nu\leq x)
$$

* Si $\nu$ es grande  (digamos, de nuevo, $\nu\geq 40$), $T_\nu$ es aproximadamente una $N(0,1)$ (pero con un poco más de varianza, porque $\nu/(\nu-2)>1$, y por lo tanto un poco más achatada). Esto es consecuencia del Teorema Central del Límite.

```{r,echo=FALSE}
curve(dnorm(x),col=1,lwd=2,xlim=c(-4,4),xlab="",ylab="",ylim=c(0,0.4),
      main="Algunas t de Student")
curve(dt(x,2),col=2,lwd=2,add=TRUE)
curve(dt(x,3),col=3,lwd=2,add=TRUE)
curve(dt(x,4),col=4,lwd=2,add=TRUE)
curve(dt(x,5),col=5,lwd=2,add=TRUE)
curve(dt(x,10),col=6,lwd=2,add=TRUE)
legend("topleft",col=1:6,lty=rep(1,6), lwd=rep(2,6),
legend=c("Normal estandar", paste("Student con g.l.=",c(2:5,10),sep="")),cex=0.7)
```

```{r,echo=FALSE}
curve(dnorm(x),col=1,lwd=2,xlim=c(-4,4),xlab="",ylab="",ylim=c(0,0.4),
      main="t vs Normal estandar")
curve(dt(x,40),col=2,lwd=2,add=TRUE)
legend("topleft",col=1:2,lty=rep(1,2), lwd=rep(2,2),
       legend=c("Normal estandar", "Student con g.l.=40"),cex=0.7)
```

Denotaremos por $t_{\nu,q}$ el  $q$-cuantil de una  variable aleatoria  $T_{\nu}$ con distribución   $t_\nu$. Es decir, $t_{\nu,q}$ es el valor tal que
$$
P(T_{\nu}\leq t_{\nu,q})=q
$$
Entonces:

* Por la simetría de la distribución $t_\nu$,
$$
t_{\nu,q}=-t_{\nu,1-q}.
$$
    Exactamente lo mismo que pasaba con la normal estándar

* Si $\nu$ es grande, $T_\nu$ será aproximadamente una $N(0,1)$ y por lo tanto $t_{\nu,q}$ es aproximadamente igual a $z_q$.

```{block2,type="rmdcaution"}
No confundáis:

* **Desviación típica de una variable aleatoria**: El parámetro poblacional, normalmente desconocido. Es $\sigma_X$.

* **Desviación típica**  (muestral o no) **de una muestra**: El estadístico que calculamos sobre la muestra. Es $\widetilde{S}_X$ (la muestral) o ${S}_X$ (la "a secas").

* **Error típico de la media muestral**: La desviación típica de la variable media muestral. Es $\sigma_X/\sqrt{n}$, con $n$ el tamaño de las muestras.

* **Error típico de una muestra**: Estimación del error típico del estimador a partir de la muestra. Es $\widetilde{S}_X/\sqrt{n}$, con $n$ el tamaño de la muestra.

Fijaos en que el denominador $\sqrt{n}$ hace que, en general, los errores típicos sean mucho más pequeños que las desviaciones típicas. Id con cuidado, porque esto se usa a menudo en artículos para enmascarar los resultados. Si una muestra ha salido con una dispersión muy grande, se da su error típico en vez de su desviación típica y parece que ha salido más concentrada.


```





## Test

**(1)** Si el tamaño de una muestra aleatoria simple aumenta (marca todas las afirmaciones correctas):

1. La media muestral siempre disminuye. 
1. El error típico de la media muestral siempre disminuye.
1. El error típico de la muestra siempre disminuye.  
1. La varianza muestral siempre aumenta. 
1. El número de grados de libertad del estimador $\chi^2$ asociado a la varianza muestral siempre aumenta. 
1. Ninguna de las otras afirmaciones es correcta

 
**(2)** Si queremos disminuir a la mitad el error típico de la media muestral: 

1. Tenemos que aumentar en un 50% el tamaño de las muestras.
1. Tenemos que doblar el tamaño  de las muestras.
1. Tenemos que cuadruplicar el tamaño  de las muestras.  
1. Tenemos que dividir por 2 el tamaño  de las muestras.
1. Tenemos que dividir por 4 el tamaño  de las muestras.
1. Ninguna de las otras respuestas es correcta.



**(3)** La prevalencia de una afección en una población es del 10%. Si estimamos dicha prevalencia repetidamente mediante las proporciones muestrales de muestras aleatorias simples de tamaño 1000, estas estimaciones siguen una distribución que  (marca todas las afirmaciones correctas): 
 
1. Es una distribución muestral. 
1. Es aproximadamente normal.  
1. Es binomial. 
1. Tiene media 0.1.  
1. Tiene media 900. 
1. Ninguna de las otras afirmaciones es correcta

**(4)** Sobre una muestra de 100 mujeres  se obtuvo una concentración media de la hemoglobina de 10 con una desviación típica de 2. ¿Qué vale el error típico de la muestra (para la media muestral, se entiende)? 

1. 0.02
1. 0.04
1. 0.2 
1. 0.4
1. 1
1. Ninguno de los anteriores





**(5)** ¿Cuáles de las afirmaciones siguientes sobre la media muestral son verdaderas? Marca todas las respuestas correctas.

1. Si la distribución poblacional es normal, siempre coincide con la media de la distribución poblacional.
1. Si la distribución poblacional es normal, siempre coincide con la mediana de la distribución poblacional.
1. Siempre sirve para estimar la media poblacional, aunque la distribución poblacional no sea normal.  
1. Si la distribución poblacional es normal, sirve para estimar la mediana poblacional. 
1. Se calcula sumando todos los valores de la muestra y dividiendo por $n-1$, donde $n$ indica el tamaño de la muestra.
1. Ninguna de las otras respuestas es correcta.


**(6)** La concentración de un cierto metabolito en sangre tiene un valor medio $\mu$. Si tomamos muestras aleatorias simples de 20 individuos, calculamos su media muestral $\overline{X}$ y su desviación típica muestral $\widetilde{S}_X$ (marca la continuación más correcta):

1. El estadístico $\frac{\overline{X}-\mu}{\widetilde{S}_X/\sqrt{n}}$ tiene siempre  distribución normal.
1. El estadístico $\frac{\overline{X}-\mu}{\widetilde{S}_X/\sqrt{n}}$ tiene siempre  distribución t de Student.
1. El estadístico $\frac{\overline{X}-\mu}{\widetilde{S}_X/\sqrt{n}}$ tiene distribución normal si la concentración sigue una ley normal.
1. El estadístico $\frac{\overline{X}-\mu}{\widetilde{S}_X/\sqrt{n}}$ tiene distribución  t de Student si la concentración tiene distribución normal.  
1. El estadístico $\frac{\overline{X}-\mu}{\widetilde{S}_X/\sqrt{n}}$ no tiene nunca ni distribución normal ni distribución t de Student, porque las muestras no son lo suficientemente grandes.


**(7)** Tenemos una variable aleatoria $X$ normal de media $\mu$ y desviación típica $\sigma$. Tomamos muestras aleatorias simples de tamaño $n$, y denotamos por $\widetilde{S}_X$ su desviación típica muestral. ¿Cuáles de las afirmaciones siguientes son verdaderas? Marca todas las respuestas verdaderas:

1. $E(\widetilde{S}_X^2)=\sigma^2$. 
1. $E(\widetilde{S}_X)=\sigma$.
1. $\widetilde{S}_X^2$ sigue una distribución ji cuadrado con $n-1$ grados de libertad.
1. $(n-1)\widetilde{S}_X^2/\sigma^2$ sigue una distribución ji cuadrado con $n-1$ grados de libertad. 
1. $(n-1)\widetilde{S}_X/\sigma$ sigue una distribución ji cuadrado con $n-1$ grados de libertad. 
1. Todas las otras respuestas son falsas.



