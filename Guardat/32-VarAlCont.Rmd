## Variables aleatorias continuas: Conceptos generales

### Densidad y distribución

En este curso nos vamos a restringir variables aleatorias continuas $X: \Omega\to \mathbb{R}$ que satisfacen la siguiente propiedad extra: su **función de distribución**
$$
\begin{array}{rcl}
F_X: \mathbb{R} & \to & [0,1]\\
x &\mapsto &P(X\leq x)
\end{array}
$$ 
es continua.

Fijaos entonces que, si $X$ es una variable aleatoria continua, 
$$
P(X=a)=0 \text{ para todo $a\in \mathbb{R}$.
$$

```{block2,type="rmdcorbes"}
En efecto,
$$
\begin{array}{rl}
P(X=a)\!\!\!\!\! & =P(X\leq a)-P(X<a)=P(X\leq a)- P\Big(\bigcup_{n\geq 1}\Big(X\leq a-\dfrac{1}{n}\Big)\Big)\\
& \displaystyle = P(X\leq a)-\lim_{n\geq 1} P\Big(X\leq a-\dfrac{1}{n}\Big)\\
& \displaystyle = F_X(a)-\lim_{n\geq 1} F_X\Big(a-\dfrac{1}{n}\Big)=0
\end{array}
$$
por la continuidad de la $F_X$.

```

En particular: **probabilidad 0 no significa imposible**. Cada suceso posible tiene probabilidad 0, pero algo ha de pasar, por lo que alguno ha de ser posible.

De $P(X=a)=0$ se deduce que la probabilidad de un suceso definido con una desigualdad es exactamente la misma que la del suceso correspondiente definido con una desigualdad estricta. Por ejemplo:

* $P(X\geq a)=P(X> a)+P(X=a)=P(X> a)$
* $P(a\leq X\leq a)=P(a<X<b)+P(X=a)+P(X=b)=P(a<X<b)$


Como $P(X=a)=0$, no podemos definir la densidad como $f_X(a)=P(X=a)$. Pero recordad que, en variables aleatorias discretas
$$
F_X(a)=\sum_{x\leq a} f_X(x)
$$

En el contexto de matemáticas "continuas", la suma $\sum$ se traduce en la integral $\int$. Se define entonces la  **función de densidad** de una variable aleatoria continua $X$ como la función $f_X:\mathbb{R}\to \mathbb{R}$ tal que $f_X(x)\geq 0$, para todo $x\in \mathbb{R}$, y
$$
F_X(a)=\int_{-\infty}^a f_{X}(x)\, dx\quad \text{para todo $a\in \mathbb{R}$}
$$

La función de densidad $f_X$ es la función tal que $y=f_X(x)$ es la curva tal que $F_X(a)$ es el **área bajo esta curva** (entre la curva y el eje de abscisas) a la izquierda de $x=a$.


```{r echo=FALSE, out.width="60%"}
knitr::include_graphics("INREMDN_files/figure-html/graficadensidad3.png")
```



La función de densidad $f_X$ de una variable aleatoria continua puede no ser continua. En cambio, **por definición**, su función de distribución sí que lo es.

Como $P(X\leq a)$ es el área  bajo la curva $y=f_X(x)$ a la izquierda de $x=a$,
$$
\begin{array}{rl}
P(a\leq X\leq b)\!\!\!\! & =P(X\leq b)-P(X<a)\\
&=P(X\leq b)-P(X\leq a)
\end{array}
$$
es el área  bajo la curva $y=f_X(x)$ a la izquierda de $x=b$ **menos** el área  bajo la curva $y=f_X(x)$ a la izquierda de $x=a$, es decir, 
**$P(a\leq X\leq b)$ es igual al área  bajo la curva $y=f_X(x)$ entre $x=a$ y $x=b$.**


```{r echo=FALSE, out.width="60%"}
knitr::include_graphics("INREMDN_files/figure-html/entreaib.png")
```

Sabemos que $P(X=a)=0$, pero si $\varepsilon>0$ es pequeño,
el área bajo $y=f_X(x)$ entre $a-\varepsilon$ y $a+\varepsilon$ es aproximadamente  $2\varepsilon\cdot f_X(a)$


```{r echo=FALSE, out.width="60%"}
knitr::include_graphics("INREMDN_files/figure-html/density.png")
```

Por lo tanto $f_X(a)$ "mide" $P(X=a)$ (pero **no es** $P(X=a)$, que vale 0).

Como $P(\Omega)=1$,
$$
P(X<\infty)=\int_{-\infty}^{\infty} f_X(x)\,dx=1
$$
**El área total bajo la curva $y=f_X(x)$ es 1.**


### Esperanza, varianza, cuantiles...

La esperanza y la varianza de una variable aleatoria continua $X$, con densidad $f_X$, se definen como en el caso discreto, substituyendo la suma $\sum_{x\in D_x}$ por una integral.



La **esperanza** (**media**, **valor esperado**...) de $X$ es 
$$
E(X)=\int_{-\infty}^{\infty}x \cdot f_{X}(x)\, dx
$$
También se escribe $\mu_X$ o simplemente $\mu$.

Este valor tiene la misma interpretación que en el caso discreto: 
representa el valor medio de $X$ sobre el total de la población, y es 
(con probabilidad 1) el límite de la media aritmética de los valores de $X$ sobre muestras aleatorias simples de tamaño $n$, cuando $n\to \infty$.

Si $g:D_X\to \mathbb{R}$ es una función continua,
la **esperanza** de $g(X)$ es
$$
E(g(X))=\int_{-\infty}^{+\infty} g(x) f_X(x)dx
$$

La **varianza** de $X$ es
$$
Var(X)=E((X-E(X))^2)
$$
y se puede demostrar que es igual a
$$
Var(X)=E(X^2)-E(X)^2
$$
También se escribe $\sigma_X^2$ o simplemente $\sigma^2$.


La **desviación típica** de $X$ es 
$$
\sigma(X)=+\sqrt{Var(X)}
$$
y también se escribe $\sigma_X$ o $\sigma$.


Como en el caso discreto, la varianza y la desviación típica miden la variabilidad de los resultados de $X$ respecto de su valor medio.

Estos parámetros de $X$ tienen las **mismas propiedades** en el caso continuo que en el discreto. Las recordamos:

* $E(b)=b$, si $b$ es una variable aleatoria constante.


* $E(a X+b)=a E(X)+b$.


* $ E(X+Y)=E(X)+E(Y)$.


* Si $X\leq Y$, entonces $E(X)\leq E(Y)$.


* $Var(aX+b)=a^2 Var(X)$, donde $a,b$ son constantes reales.


* $\sigma(aX+b)=|a|\cdot \sigma(X)$.


* $Var(b)=0$, si $b$ es una variable aleatoria constante


* $Var(X+Y)=Var(X)+Var(Y)$ si $X,Y$ son **independientes**


El **cuantil de orden $p$** (o **$p$-cuantil**) de una variable aleatoria continua $X$ es el valor $x_p\in \mathbb{R}$ más pequeño tal que 
$$
F_X(x_p)=P(X\leq x_p)=p
$$
(que ahora siempre existe por ser $F_X$ continua)

La **mediana** de $X$ es su 0.5-cuantil, y el **primer** y **tercer cuartiles** son su 0.25-cuantil y su 0.75-cuantil.



## Variables aleatorias normales


Una variable aleatoria continua $X$ es **normal** de parámetros
$\mu$ y $\sigma$, y lo denotaremos escribiendo 
$X\sim N(\mu,\sigma)$}, cuando su función de densidad es
$$
f_{X}(x)=\frac{1}{\sqrt{2\pi}\sigma} e^{{-(x-\mu)^2}/({2\sigma^{2}})} \text{
para todo } x\in \mathbb{R}
$$

Si $X\sim N(\mu,\sigma)$, entonces
$$
E(X)=\mu,\quad Var(X)=\sigma^2,\quad \sigma(X)=\sigma
$$

Una variable aleatoria normal es **típica} (o **estándar}) cuando tiene $\mu=0$ y $\sigma=1$; la denotaremos usualmente por $Z$

En particular, si $Z\sim N(0,1)$, $E(Z)=0$ y $\sigma(Z)=1$.




La gráfica de la densidad de una variable aleatoria normal es la conocida **campana de Gauss}\vspace*{-1ex}

\begin{center}
\includegraphics[width=0.8\linewidth]{dnorm01}
\end{center}




 
\frametitle{Distribución normal}

La distribución normal 
\begin{itemize}
* es una distribución teórica, no la encontraréis exacta en la práctica
* no es más <<normal>> que otras
* aproxima bien muchas distribuciones reales porque


\begin{quote}
Muchas variables aleatorias que consisten en tomar $n$ observaciones independientes de una o varias variables aleatorias y sumarlas tienen distribución aproximadamente normal cuando $n$ es grande, aunque las variables aleatorias de partida no lo sean
\end{quote}
\end{itemize}





\frametitle{Binomial aprox. normal}
\vspace{-1ex}

Si $X\sim B(n,p)$, con $n$ grande y $p$ lejos de 0 y 1 (digamos, $n\geq 50$, $10\leq pn\leq n-10$, pero cuanto más centrada sea $p$, menor puede ser $n$),  $X\approx N(np,\sqrt{np(1-p)})$
\vspace{-1ex}

\begin{center}
\includegraphics[width=0.7\linewidth]{normvsbin}
\end{center}









\frametitle{Poisson aprox. normal}
\vspace{-1ex}

Si $X\sim Po(\lambda)$ y $\lambda$ es grande (digamos, $\lambda\geq 50$), entonces $X\approx N(\lambda,\sqrt{\lambda})$
\vspace{-1ex}

\begin{center}
\includegraphics[width=0.7\linewidth]{poisvsnormal}
\end{center}







%
%
% 
%\frametitle{Con R}\vspace*{-1ex}
%
%Para calcular probabilidades de una $N(\mu,\sigma)$,  hay que calcular las integrales a mano \includegraphics[width=0.7cm]{emorisa}\ o podéis usar \texttt{R}, para el que la normal es \blue{\texttt{norm}}
%%
%%
%%Si $X\sim N(\mu,\sigma)$,
%%\begin{itemize}
%%*  **\texttt{dnorm(}$x$\texttt{,}$\mu$\texttt{,}$\sigma$\texttt{)}} da el valor de la densidad $f_X(x)$
%%
%%*  **\texttt{pnorm(}$x$\texttt{,}$\mu$\texttt{,}$\sigma$\texttt{)}} da el valor de la distribución $F_X(x)$
%%
%%
%%* **\texttt{qnorm(}$q$\texttt{,}$\mu$\texttt{,}$\sigma$\texttt{)}} da el $q$-cuantil 
%%
%%
%%* **\texttt{rnorm(}$N$\texttt{,}$\mu$\texttt{,}$\sigma$\texttt{)}} da una lista de $N$ números aleatorios generados con esta distribución
%%\end{itemize}
%%
%%
%%En la normal estándar no es necesario entrar $\mu$ y $\sigma$
%





%
%
%
%[fragile] 
%\frametitle{Con R}\vspace*{-1ex}
%
%\blue{Si $X\sim N(3,0.5)$, ¿qué vale $P(X\leq 2)$?}\vspace*{-1ex}
%
%\begin{verbatim}
%> pnorm(2,3,0.5)
%[1] 0.02275013
%\end{verbatim}
%
%\blue{Si $X\sim N(0,1)$, ¿qué vale $P(-1\leq X\leq 1)$?}
%
%Como $P(-1\leq X\leq 1)=P(X\leq 1)-P(X\leq -1)$,\vspace*{-1ex}
%
%\begin{verbatim}
%> pnorm(1)-pnorm(-1)
%[1] 0.6826895
%\end{verbatim}
%
%
%\blue{¿Qué vale el primer cuartil de $X\sim N(3,0.5)$?}\vspace*{-1ex}
%
%\begin{verbatim}
%> qnorm(0.25,3,0.5)
%[1] 2.662755
%\end{verbatim}
%
%
%



\frametitle{Propiedades} 
\vspace{-1ex}

Si $X\sim N(\mu,\sigma)$, su densidad $f_X$ es simétrica respecto de $x=\mu$,
$$
f_{X}(\mu-x)=f_{X}(\mu+x),
$$
y tiene el máximo en $x=\mu$


En particular, si $Z\sim N(0,1)$, entonces
$f_{Z}(-x)=f_{Z}(x)$, y $f_Z$ tiene el máximo en $x=0$
\vspace{-5ex}

\begin{center}
\includegraphics[width=\linewidth]{simn}
\end{center}




\frametitle{Propiedades} 

Recordemos que $P(X\leq x)=F_X(x)$ es el área comprendida entre la densidad $y=f_X(x)$ y el eje de abscisas a la izquierda de $x$
\begin{center}
\includegraphics[width=\linewidth]{undernorm}
\end{center}




\frametitle{Propiedades}\vspace*{-2ex}

La simetría de $f_X$ hace que las áreas a la izquierda de $\mu-x$ y a la derecha de $\mu+x$ sean iguales\vspace*{-2ex}

\begin{center}
\includegraphics[width=0.9\linewidth]{simnorm1}
\end{center}
\vspace{-1.5cm}

$$
P(X\leq \mu-x)**=}P(X\geq \mu+x)=1-P(X\leq \mu+x)
$$

En particular (tomando $x=0$)
$$
P(X\leq \mu)=1-P(X\leq \mu)\Rightarrow P(X\leq \mu)=0.5
$$
** $\mu$ es también la {mediana} de $X$}




\frametitle{Propiedades}\vspace*{-2ex}

En particular, si $Z\sim N(0,1)$, las áreas a la izquierda de $-z$ y a la derecha de $z$ son iguales\vspace*{-0.4cm}

\begin{center}
\includegraphics[width=\linewidth]{simnorm2}
\end{center}
\vspace{-1cm}

$$
P(Z\leq -z)=P(Z\geq z)=1-P(Z\leq z)
$$

Y en particular la mediana de $Z$ es 0





\frametitle{Propiedades} 

Aumentar la $\mu$ desplaza a la derecha el máximo, y con él toda la curva
\begin{center}
\includegraphics[width=\linewidth]{mu1mu2}

$\mu_1<\mu_2$
\end{center}






\frametitle{Propiedades} 

Aumentar la $\sigma$ achata la curva: al aumentar la desviación típica, los valores se alejan más del valor medio
\begin{center}
\includegraphics[width=\linewidth]{sigma1sigma2}

$\sigma_1<\sigma_2$
\end{center}






\frametitle{Propiedades} 

El efecto combinado
\begin{center}
\includegraphics[width=\linewidth]{musigma}

$\mu_1<\mu_2,\ \sigma_1<\sigma_2$
\end{center}






\frametitle{Propiedades} 

Algunos números que se espera que recordéis:

\begin{itemize}
* Si $X\sim N(\mu,\sigma)$,
$$
\begin{array}{l}
P(\mu-\sigma\leq X\leq \mu+\sigma)\approx 0.68\\
P(\mu-2\sigma\leq X\leq \mu+2\sigma)\approx 0.95\\
P(\mu-3\sigma\leq X\leq \mu+3\sigma)\approx 0.997
\end{array}
$$

* Algunos cuantiles relevantes para $Z\sim N(0,1)$
$$
\begin{array}{l}
\text{$0.95$-cuantil}:\ z_{0.95}=1.64\\
\text{$0.975$-cuantil}:\ z_{0.975}=1.96\ (\approx \text{el 2 de arriba})\\
\text{$0.995$-cuantil}:\ z_{0.995}=2.58
\end{array}
$$
\end{itemize}









\frametitle{Combinaciones lineales}\vspace*{-2ex}


\begin{teorema}
Si $X\sim N(\mu,\sigma)$ y $a,b\in \mathbb{R}$, entonces
$aX+b$ es $N(a\mu+b,|a|\cdot\sigma)$

En particular, si $X\sim N(\mu,\sigma)$, entonces su **tipificada} (o **estandarizada})
$Z=\dfrac{X-\mu}{\sigma}$ es $N(0,1)$
\end{teorema}

Más en general:

\begin{teorema}
Si $X_1,\ldots,X_n$ son variables aleatorias normales independientes y $a_1,\ldots,a_n,b\in \mathbb{R}$, entonces
$a_1X_1+\cdots +a_nX_n+b$ es normal con $\mu$ y $\sigma$ lo que toque:
$$
\mu=a_1\mu_1+\cdots +a_n\mu_n+b,\ 
\sigma=\sqrt{a_1^2\sigma^2_1+\cdots +a_n^2\sigma^2_n}
$$
\end{teorema}










\frametitle{Reducción a la tipificada}

Las probabilidades de la normal tipificada determinan las de la normal original  $X\sim N(\mu,\sigma)$:
$$
\begin{array}{l}
**P(X\leq a)}\displaystyle   =P\Big(\frac{X-\mu}{\sigma}\leq \frac{a-\mu}{\sigma}\Big)**=P\Big(Z\leq \frac{a-\mu}{\sigma}\Big)}\\[5ex]
**P(a\leq X\leq b)}\displaystyle   =P\Big( \frac{a-\mu}{\sigma}\leq \frac{X-\mu}{\sigma}\leq \frac{b-\mu}{\sigma}\Big)\\[2ex] \hphantom{**P(a\leq X\leq b)}}\ \displaystyle**=P\Big(\frac{a-\mu}{\sigma}\leq Z\leq \frac{b-\mu}{\sigma}\Big)}
\end{array}
$$
Sirve para deducir fórmulas, y vuestros padres las usaban para cálculos (con tablas); ahora es más cómodo usar un programa










\frametitle{Intervalos de normalidad}\vspace*{-1ex}

Si $X\sim N(\mu,\sigma)$, ¿cómo calcular un intervalo centrado en $\mu$ tal que la probabilidad de que $X$ pertenezca a este intervalo sea un valor $q$ fijo?


\begin{teorema}
Si $X\sim N(\mu,\sigma)$, 
$$
P\big(\mu- z_{\frac{1+q}{2}}\cdot \sigma<X<\mu+ z_{\frac{1+q}{2}}\cdot \sigma\big)=q
$$
(donde $z_{\frac{1+q}{2}}$ denota el $\dfrac{1+q}{2}$-cuantil de $Z\sim N(0,1)$)
\end{teorema}




\frametitle{Intervalos de normalidad}\vspace*{-1ex}
\begin{center}
\includegraphics[width=0.15\linewidth]{cp}
\end{center}
\blue{\bf Demostración}:
$$
\begin{array}{l}
P(\mu-x\leq X\leq \mu+x)=q\\
\qquad \Longleftrightarrow \displaystyle P\Big(\frac{\mu-x-\mu}{\sigma}\leq \frac{X-\mu}{\sigma}\leq \frac{\mu+x-\mu}{\sigma}\Big)=q\\
\qquad \Longleftrightarrow \displaystyle P(-x/{\sigma}\leq Z\leq  {x}/{\sigma})=q\\
\qquad \Longleftrightarrow \displaystyle P(Z\leq  {x}/{\sigma})-P(Z\leq  -{x}/{\sigma})=q\\
\qquad \Longleftrightarrow \displaystyle P(Z\leq  {x}/{\sigma})-(1-P(Z\leq  {x}/{\sigma}))=q\\
\qquad \text{(por la simetría de $f_Z$ alrededor de 0)}\\
\qquad \Longleftrightarrow \displaystyle 2F_Z(x/\sigma)=2P(Z\leq  {x}/{\sigma})=q+1\\
\qquad \Longleftrightarrow F_Z(x/\sigma)=(1+q)/2\\
\qquad \Longleftrightarrow x/\sigma=**z_{(1+q)/2}}\\
\qquad \Longleftrightarrow x=z_{(1+q)/2}\cdot \sigma
\end{array}
$$









\frametitle{Intervalos de normalidad}

\blue{Según la OMS, las alturas de las mujeres europeas de 18 años siguen una ley $N(163.1,18.53)$. \only<1>{¿Intervalo de alturas centrado en la media que contenga a la mitad las europeas de 18 años?}\only<2,3>{¿Intervalo de alturas centrado en la media tal que la probabilidad de que la altura de una europea de 18 años escogida al azar pertenezca a este intervalo sea  $1/2$?}}

\only<3>{$X$: altura de una mujer europea de 18 años

$X\sim N(163.1,18.53)$

$$\begin{array}{rl}
q=0.5 & \Rightarrow (1+q)/2=0.75\\ &
 \Rightarrow z_{0.75}=\text{\texttt{qnorm(0.75)}}=0.6745
 \end{array}
 $$

Por lo tanto, es el intervalo $163.1\pm 0.6745\cdot 18.53$, es decir $(150.6, 175.6)$}












\frametitle{Intervalos de referencia}

Si $X\sim N(\mu,\sigma)$, el **intervalo de referencia} para $X$ es el intervalo $(\mu-x,\mu+x)$ tal que
$$
P(\mu-x< X< \mu+x)=0.95,
$$
es decir (recordando $q=0.95\Rightarrow (1+q)/2=0.975$ y $z_{0.975}=1.96\approx 2$)
$$
\mu\pm 1.96\sigma
$$
(o $\mu\pm 2\sigma$, para simplificar)

\begin{center}
\includegraphics[width=\linewidth]{analit}
\end{center}






\frametitle{z-\textsl{score}}\vspace*{-1ex}

El **z-score} (**valor}, **puntuación}, **puntaje}, **z}) de un valor $x_0$ respecto de una distribución $N(\mu,\sigma)$ es
$$
\frac{x_0-\mu}{\sigma}
$$
Cuanto mayor en valor absoluto, más <<raro>> es $x_0$; el signo, si está por encima o por debajo del valor esperado

\blue{Ejemplo}: Según la OMS, las alturas de las mujeres europeas de 18 años siguen una ley $N(163.1,18.53)$. ¿Cuál sería el z-score de la jugadora de baloncesto Alba Torrens, que mide 191 cm?

$$ 
\frac{191-163.1}{18.53}=1.5
$$










\frametitle{z-\textsl{score}}\vspace*{-1ex}

\begin{center}
\includegraphics[width=\linewidth]{zscore1}\\[2ex]
\includegraphics[width=\linewidth]{zscore2}
\end{center}
\vspace*{3cm}



{\tiny E. Delgado \textsl{et al}.  ``Asociación entre peso al nacer y factores de riesgo cardiometabólicos en niños de Bucaramanga, Colombia."  Nutrición Hospitalaria 34 (2017), 1105--1111.

}




\frametitle{Criterios diagnósticos}\vspace*{-2ex}\small

Supongamos que la concentración de un cierto metabolito es una variable aleatoria $X_S\sim N(\mu_S, \sigma_S)$ sobre  personas sanas y una variable aleatoria $X_E\sim N(\mu_E, \sigma_E)$ sobre personas enfermas. Supongamos $\mu_E>\mu_S$.

Podemos usar como test diagnóstico de la enfermedad la concentración del metabolito: positivo si mayor que un cierto valor de referencia  $x_0$, negativo si menor.

Sensibilidad:
$$
P(+|E)  =P(X_E\geq x_0)=1-P(X_E< x_0)=1-F_{X_E}(x_0)
$$

Especificidad
$$
P(-|S)=P(X_S< x_0)=F_{X_S}(x_0)
$$

Dibujamos la curva ROC (teórica) y escogemos $x_0$






\frametitle{Criterios diagnósticos}\vspace*{-2ex}


\begin{center}
\includegraphics[width=0.8\linewidth]{rocnormal}
\end{center}

{\tiny P. Martínez-Camblor, ``Comparación de pruebas diagnósticas desde la curva ROC."
Rev. Colomb. Estadística 30 (2007), 163--176

}






\frametitle{Transformaciones}

Algunas variables interesantes son muy diferentes de normales, pero admiten transformaciones en variables parecidas a normales.

\only<1>{\blue{Ejemplo}: 280 mediciones de triglicéridos en sangre de cordón umbilical
\begin{center}
\includegraphics[width=0.6\linewidth]{Trigl1}
\end{center}}
\only<2>{\blue{Ejemplo}: 280 **logaritmos} de mediciones de triglicéridos en sangre de cordón umbilical
\begin{center}
\includegraphics[width=0.56\linewidth]{Trigl2}
\end{center}}







\frametitle{Transformaciones}

A menudo el logaritmo de una variable asimétrica a la derecha  es aproximadamente normal

Diremos que $X$ es una variable **lognormal} cuando $\ln(X)$ es normal. En este caso, se suelen dar
\begin{itemize}
* La **media geométrica} de $X$: $\mu^*(X)=e^{\mu(\ln(X))}$
\end{itemize}






\frametitle{Media geométrica}\vspace*{-1ex}
\begin{center}
\includegraphics[width=0.15\linewidth]{cp}
\end{center}

La **media geométrica} de $X=\{x_1,\ldots,x_n\}$ es 
$$
\sqrt[n]{x_1\cdots x_n}
$$


La media aritmética de sus logaritmos es
$$
\overline{\ln(X)}=\frac{\ln(x_1)+\cdots+\ln(x_n)}{n}
$$
Entonces
$$
\begin{array}{rl}
e^{\overline{\ln(X)}} & =e^{(\ln(x_1)+\cdots+\ln(x_n))/n}\\
& =\sqrt[n]{e^{\ln(x_1)}\cdots e^{\ln(x_1)}}\\
& = \sqrt[n]{x_1\cdots x_n}
\end{array}
$$









\frametitle{Transformaciones}

A menudo el logaritmo de una variable asimétrica a la derecha es aproximadamente normal

Diremos que $X$ es una variable **lognormal} cuando $\ln(X)$ es normal. En este caso, se suelen dar
\begin{itemize}
* La **media geométrica} de $X$: $\mu^*(X)=e^{\mu(\ln(X))}$
* La **desviación típica geométrica} de $X$: $\sigma^*(X)=e^{\sigma(\ln(X))}$
\end{itemize}
o directamente sus logaritmos neperianos $\mu(\ln(X))$ y $\sigma(\ln(X))$

La transformación logarítmica de ida y vuelta permite calcular intervalo de referencia








\frametitle{Transformaciones}
\blue{La concentración  $X$ de glucosa en suero en diabéticos bajo un determinado tratamiento sigue aproximadamente una distribución lognormal, con $\mu^*=144.51$ (mg/dL) y $\sigma^*=1.46$. ¿Cuál es el intervalo de referencia para sus valores?}

$\mu_{\ln(X)}=\ln(\mu^*_X)=\ln(144.51)=4.9733$

$\sigma_{\ln(X)}=\ln(\sigma^*_X)=\ln(1.46)=0.38$

El intervalo de referencia para $\ln(X)$ es
$$
(4.9733-1.96\cdot 0.38, 4.9733+1.96\cdot 0.38)=(4.2285,5.7181)
$$


Como
$$
\hspace*{-1ex}P(a< \ln(X)< b)=P(e^a< e^{\ln(X)}< e^b)=P(e^a< X< e^b)
$$
el intervalo de referencia para $X$ es
$$
\big(e^{4.2285},e^{5.7181}\big)=(68.6142, 304.3261)
$$






\frametitle{Ajuste}

Muchos métodos estadísticos que explicaremos requieren que la muestra provenga de una distribución normal. ¿Cómo lo podemos saber?\pause

No podemos (el azar, ya sabéis), pero podemos mirar si es razonable suponer que proviene de una distribución normal

Más adelante explicaremos métodos estadísticos, por ahora métodos gráficos:

\begin{itemize}
* Un histograma y, superpuesta, la densidad de la normal con media y desv. típ. las de la muestra

* Un **q-q-plot}
\end{itemize}




\frametitle{Ajuste}

\blue{Ejemplo}: 280 mediciones de triglicéridos en sangre de cordón umbilical

Su histograma y la densidad de la normal con $\mu$ su media y $\sigma$ su desv. típ. es:

\begin{center}
\includegraphics[width=0.7\linewidth]{Trigl1}
\end{center}





\frametitle{Ajuste}

\blue{Ejemplo}: 280 **logaritmos} de mediciones de triglicéridos en sangre de cordón umbilical

Su histograma y la densidad de la normal con $\mu$ su media y $\sigma$ su desv. típ. es:

\begin{center}
\includegraphics[width=0.6\linewidth]{Trigl2}
\end{center}






\frametitle{Q-q-plot}

Un **q-q-plot} de una muestra y una distribución teórica es el gráfico de los puntos
\begin{center}
%\hspace*{-1ex}\big($q$-cuantil de la distribución, $q$-cuantil de la muestra\big)
\hspace*{-1ex}\big($q$-cuantil de la muestra, $q$-cuantil de la distribución\big)
\end{center}

Si la muestra proviene de la distribución, es de esperar que
\begin{center}
$q$-cuantil de la muestra $\approx$ $q$-cuantil de la distribución
\end{center}
y estos puntos estarán cerca de la diagonal principal $x=y$

Cuando la distribución a comparar es una normal es un **normal(-q)-plot}







\frametitle{Q-q-plot}\vspace*{-3ex}

\begin{center}
\includegraphics[width=0.75\linewidth]{qqtrigl1}\\
\textsl{Normal-plot} de las medidas de triglicéridos 
\end{center}





\frametitle{Q-q-plot}\vspace*{-3ex}

\begin{center}
\includegraphics[width=0.7\linewidth]{Trigl1}
\end{center}





\frametitle{Q-q-plot}\vspace*{-3ex}

\begin{center}
\includegraphics[width=0.75\linewidth]{qqtrigl2}\\
\textsl{Normal-plot} de los logaritmos de las medidas de triglicéridos 
\end{center}







\frametitle{Q-q-plot}\vspace*{-4ex}

\begin{center}
\includegraphics[width=0.6\linewidth]{glucose}\\
\footnotesize\textsl{Normal-plot} de unas medidas de niveles glucosa (mmol/l)
\end{center}
\pause
\begin{quote}\footnotesize\sf
2.2, 2.9, 3.3, 3.3, 3.3, 3.4, 3.4, 3.4, 3.6, 3.6, 3.6, 3.6, 3.7, 3.7, 3.8, 3.8, 3.8, 3.9, 4.0, 4.0, 4.0, 4.1, 4.1,
4.1, 4.2, 4.3, 4.4, 4.4, 4.4, 4.5, 4.6, 4.7, 4.7, 4.7, 4.8, 4.9, 4.9, 5.0, 5.1, 6.0
\end{quote}




\frametitle{Q-q-plot}\vspace*{-2ex}

\begin{center}
\includegraphics[width=0.8\linewidth]{bmi}\\
\footnotesize\textsl{Normal-plot} de unos IMC
\end{center}





\frametitle{Q-q-plot}\vspace*{-2ex}

\begin{center}
\includegraphics[width=0.8\linewidth]{histbmi}\\
\footnotesize Histograma de los IMC
\end{center}





\frametitle{Ejercicio}\vspace*{-1ex}

Se acepta que la presión sistólica se distribuye como una variable normal con valor medio y 
desviación típica que dependen de la edad. Para la franja de edad 16--24 años, estos valores son:
\begin{itemize}
* Para hombres, $\mu=124$ y $\sigma=13.7$
* Para mujeres, $\mu=117$ y $\sigma=13.7$
\end{itemize}
El modelo de hipertensión-hipotensión aceptado es el siguiente:
\begin{center}
\begin{picture}(250,60)(-15,-20)
 \thicklines
\put(0,20){\line(1,0){100}}
\put(120,20){\line(1,0){100}}
\put(110,20){\makebox(0,0)[t]{$\ldots$}}
\put(30,20){\line(0,1){10}}
\put(60,20){\line(0,1){10}}
\put(200,20){\line(0,1){10}}
\put(170,20){\line(0,1){10}}

\put(30,40){\makebox(0,0)[t]{$\scriptstyle 5\%$}}
\put(60,40){\makebox(0,0)[t]{$\scriptstyle 10\%$}}
\put(170,40){\makebox(0,0)[t]{$\scriptstyle 90\%$}}
\put(200,40){\makebox(0,0)[t]{$\scriptstyle 95\%$}}

\put(10,10){\makebox(0,0)[b]{\scriptsize Hipotenso}}
\put(45,10){\makebox(0,0)[b]{\scriptsize Riesgo}}
\put(110,10){\makebox(0,0)[b]{\scriptsize Normal}}
\put(185,10){\makebox(0,0)[b]{\scriptsize Riesgo}}
\put(220,10){\makebox(0,0)[b]{\scriptsize Hipertenso}}
\end{picture}
\end{center}\vspace*{-1cm}

Calculad los límites de cada clase para cada sexo en este grupo de edad.







\frametitle{Ejercicio}\vspace*{-2ex}

\begin{itemize}
* El límite superior del grupo de hipotensión es el valor que deja a la izquierda un 5\% de las tensiones: el 0.05-cuantil de la distribución
* El límite superior del grupo de riesgo de hipotensión es el valor que deja a la izquierda un 10\% de las tensiones: el 0.1-cuantil de la distribución
* El límite inferior del grupo de riesgo de hipertensión es el valor que deja a la izquierda un 90\% de las tensiones: el 0.9-cuantil de la distribución
* El límite inferior del grupo de hipertensión es el valor que deja a la izquierda un 95\% de las tensiones: el 0.95-cuantil de la distribución
\end{itemize}
En los hombres, la tensión sistólica es una variable aleatoria $N(124,13.7)$






[fragile]
\frametitle{Ejercicio}\vspace*{-2ex}

\begin{lstlisting}
> qnorm(0.05,124,13.7)
[1] 101.4655
> qnorm(0.1,124,13.7)
[1] 106.4427
> qnorm(0.9,124,13.7)
[1] 141.5573
> qnorm(0.95,124,13.7)
[1] 146.5345
\end{lstlisting}

Hemos trabajado más de lo necesario: por la simetría, el 0.95-cuantil (o el 0.9-cuantil) ha de estar a la misma distancia de $\mu$ que el 0.05-cuantil (que el 0.1-cuantil), pero a la derecha
$$
124-101.4655=22.5345\Longrightarrow  124+22.5345=126.5345
$$




\frametitle{Ejercicio}

Entre los hombres de 16 a 24 años:

\begin{center}
\begin{tabular}{|ll|}
\hline
Grupo & Intervalo\\ \hline
Hipotensión & $<101.5$\\
Riesgo de hipot. & 101.5 a 106.4\\
Normal & 106.4 a 141.6\\
Riesgo de hipert. & 141.6 a 141.5\\
Hipertensión & $> 141.5$\\ \hline
\end{tabular}
\end{center}

Calculad los límites para las mujeres

