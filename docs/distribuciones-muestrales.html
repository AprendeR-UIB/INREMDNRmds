<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Lección 1 Distribuciones muestrales | INREMDN.knit</title>
  <meta name="description" content="" />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="Lección 1 Distribuciones muestrales | INREMDN.knit" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Lección 1 Distribuciones muestrales | INREMDN.knit" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="chap-IC.html"/>
<script src="libs/header-attrs-2.9/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">INREMDN</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Presentación</a></li>
<li class="chapter" data-level="1" data-path="distribuciones-muestrales.html"><a href="distribuciones-muestrales.html"><i class="fa fa-check"></i><b>1</b> Distribuciones muestrales</a>
<ul>
<li class="chapter" data-level="1.1" data-path="distribuciones-muestrales.html"><a href="distribuciones-muestrales.html#estimadores"><i class="fa fa-check"></i><b>1.1</b> Estimadores</a></li>
<li class="chapter" data-level="1.2" data-path="distribuciones-muestrales.html"><a href="distribuciones-muestrales.html#la-media-muestral"><i class="fa fa-check"></i><b>1.2</b> La media muestral</a></li>
<li class="chapter" data-level="1.3" data-path="distribuciones-muestrales.html"><a href="distribuciones-muestrales.html#la-proporción-muestral"><i class="fa fa-check"></i><b>1.3</b> La proporción muestral</a></li>
<li class="chapter" data-level="1.4" data-path="distribuciones-muestrales.html"><a href="distribuciones-muestrales.html#la-varianza-muestral"><i class="fa fa-check"></i><b>1.4</b> La varianza muestral</a></li>
<li class="chapter" data-level="1.5" data-path="distribuciones-muestrales.html"><a href="distribuciones-muestrales.html#la-distribución-t-de-student"><i class="fa fa-check"></i><b>1.5</b> La distribución t de Student</a></li>
<li class="chapter" data-level="1.6" data-path="distribuciones-muestrales.html"><a href="distribuciones-muestrales.html#test"><i class="fa fa-check"></i><b>1.6</b> Test</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chap-IC.html"><a href="chap-IC.html"><i class="fa fa-check"></i><b>2</b> Intervalos de confianza</a>
<ul>
<li class="chapter" data-level="2.1" data-path="chap-IC.html"><a href="chap-IC.html#definiciones-básicas"><i class="fa fa-check"></i><b>2.1</b> Definiciones básicas</a></li>
<li class="chapter" data-level="2.2" data-path="chap-IC.html"><a href="chap-IC.html#un-ejemplo-ic-95-para-la-media-de-una-variable-aleatoria-normal"><i class="fa fa-check"></i><b>2.2</b> Un ejemplo: IC-95% para la media de una variable aleatoria normal</a></li>
<li class="chapter" data-level="2.3" data-path="chap-IC.html"><a href="chap-IC.html#intervalo-de-confianza-para-la-media-basado-en-la-t-de-student"><i class="fa fa-check"></i><b>2.3</b> Intervalo de confianza para la media basado en la t de Student</a></li>
<li class="chapter" data-level="2.4" data-path="chap-IC.html"><a href="chap-IC.html#intervalos-de-confianza-para-proporciones"><i class="fa fa-check"></i><b>2.4</b> Intervalos de confianza para proporciones</a></li>
<li class="chapter" data-level="2.5" data-path="chap-IC.html"><a href="chap-IC.html#un-intervalo-de-confianza-para-la-diferencia-de-proporciones"><i class="fa fa-check"></i><b>2.5</b> Un intervalo de confianza para la diferencia de proporciones</a></li>
<li class="chapter" data-level="2.6" data-path="chap-IC.html"><a href="chap-IC.html#intervalos-de-confianza-para-diferencias-de-medias"><i class="fa fa-check"></i><b>2.6</b> Intervalos de confianza para diferencias de medias</a></li>
<li class="chapter" data-level="2.7" data-path="chap-IC.html"><a href="chap-IC.html#test-1"><i class="fa fa-check"></i><b>2.7</b> Test</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="contrastes-de-hipótesis.html"><a href="contrastes-de-hipótesis.html"><i class="fa fa-check"></i><b>3</b> Contrastes de hipótesis</a>
<ul>
<li class="chapter" data-level="3.1" data-path="contrastes-de-hipótesis.html"><a href="contrastes-de-hipótesis.html#hipótesis-nula-y-alternativa"><i class="fa fa-check"></i><b>3.1</b> Hipótesis nula y alternativa</a></li>
<li class="chapter" data-level="3.2" data-path="contrastes-de-hipótesis.html"><a href="contrastes-de-hipótesis.html#sec:moneda"><i class="fa fa-check"></i><b>3.2</b> Un ejemplo</a></li>
<li class="chapter" data-level="3.3" data-path="contrastes-de-hipótesis.html"><a href="contrastes-de-hipótesis.html#sec:pval"><i class="fa fa-check"></i><b>3.3</b> El p-valor</a></li>
<li class="chapter" data-level="3.4" data-path="contrastes-de-hipótesis.html"><a href="contrastes-de-hipótesis.html#tipo-de-errores"><i class="fa fa-check"></i><b>3.4</b> Tipo de errores</a></li>
<li class="chapter" data-level="3.5" data-path="contrastes-de-hipótesis.html"><a href="contrastes-de-hipótesis.html#sec:exttest"><i class="fa fa-check"></i><b>3.5</b> Ejemplo: El test t</a></li>
<li class="chapter" data-level="3.6" data-path="contrastes-de-hipótesis.html"><a href="contrastes-de-hipótesis.html#recapitulación"><i class="fa fa-check"></i><b>3.6</b> Recapitulación</a>
<ul>
<li class="chapter" data-level="" data-path="contrastes-de-hipótesis.html"><a href="contrastes-de-hipótesis.html#intervalo-de-confianza-de-un-contraste"><i class="fa fa-check"></i>Intervalo de confianza de un contraste</a></li>
<li class="chapter" data-level="" data-path="contrastes-de-hipótesis.html"><a href="contrastes-de-hipótesis.html#la-potencia"><i class="fa fa-check"></i>La potencia</a></li>
<li class="chapter" data-level="" data-path="contrastes-de-hipótesis.html"><a href="contrastes-de-hipótesis.html#el-riesgo-de-falso-positivo"><i class="fa fa-check"></i>El riesgo de falso positivo</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="contrastes-de-hipótesis.html"><a href="contrastes-de-hipótesis.html#test-2"><i class="fa fa-check"></i><b>3.7</b> Test</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="contrastes-de-hipótesis-uni-y-biparamétricos.html"><a href="contrastes-de-hipótesis-uni-y-biparamétricos.html"><i class="fa fa-check"></i><b>4</b> Contrastes de hipótesis uni- y biparamétricos</a>
<ul>
<li class="chapter" data-level="4.1" data-path="contrastes-de-hipótesis-uni-y-biparamétricos.html"><a href="contrastes-de-hipótesis-uni-y-biparamétricos.html#contrastes-para-medias"><i class="fa fa-check"></i><b>4.1</b> Contrastes para medias</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="contrastes-de-hipótesis-uni-y-biparamétricos.html"><a href="contrastes-de-hipótesis-uni-y-biparamétricos.html#test-t-para-una-media"><i class="fa fa-check"></i><b>4.1.1</b> Test t para una media</a></li>
<li class="chapter" data-level="4.1.2" data-path="contrastes-de-hipótesis-uni-y-biparamétricos.html"><a href="contrastes-de-hipótesis-uni-y-biparamétricos.html#tests-t-para-dos-medias"><i class="fa fa-check"></i><b>4.1.2</b> Tests t para dos medias</a></li>
<li class="chapter" data-level="4.1.3" data-path="contrastes-de-hipótesis-uni-y-biparamétricos.html"><a href="contrastes-de-hipótesis-uni-y-biparamétricos.html#tests-no-paramétricos"><i class="fa fa-check"></i><b>4.1.3</b> Tests no paramétricos</a></li>
<li class="chapter" data-level="4.1.4" data-path="contrastes-de-hipótesis-uni-y-biparamétricos.html"><a href="contrastes-de-hipótesis-uni-y-biparamétricos.html#ejemplos"><i class="fa fa-check"></i><b>4.1.4</b> Ejemplos</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="contrastes-de-hipótesis-uni-y-biparamétricos.html"><a href="contrastes-de-hipótesis-uni-y-biparamétricos.html#contrastes-de-varianzas"><i class="fa fa-check"></i><b>4.2</b> Contrastes de varianzas</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="distribuciones-muestrales" class="section level1" number="1">
<h1><span class="header-section-number">Lección 1</span> Distribuciones muestrales</h1>
<div id="estimadores" class="section level2" number="1.1">
<h2><span class="header-section-number">1.1</span> Estimadores</h2>
<p>En un problema típico de <strong>estadística inferencial</strong>:</p>
<ul>
<li><p>Queremos conocer el valor de una característica en el total de una población, pero no podemos medir esta característica en <strong>todos</strong> los individuos de la población.</p></li>
<li><p>Entonces, extraemos una muestra de la población, medimos la característica en los individuos de esta muestra, calculamos algo con los datos obtenidos e <strong>inferimos</strong> el valor de la característica en el global de la población.</p></li>
</ul>
<p>Inmediatamente surgen varias preguntas, que responderemos entre esta lección y la próxima:</p>
<ul>
<li>¿Cómo tiene que ser la muestra?</li>
<li>¿Qué tenemos que calcular?</li>
<li>¿Con qué precisión podemos inferir la característica de la población?</li>
</ul>
<p>¿Qué tipo de muestra tenemos que tomar? Vamos a suponer de ahora en adelante que tomamos <strong>muestras aleatorias simples</strong>. Esto incluye las muestras aleatorias sin reposición si la población es mucho más grande que la muestra, ya que entonces no hay diferencia práctica entre permitir y prohibir las repeticiones. En algunos casos muy concretos permitiremos muestras aleatorias sin reposición en general.</p>

<div class="rmdromans">
Sí, ya sabemos que en la práctica casi nunca tomamos muestras aleatorias, sino oportunistas. En este caso, recordad lo que os explicábamos en la Sección <a href="#sec:oport"><strong>??</strong></a>. Lo que hay que hacer es describir en detalle las características de la muestra para justificar que, pese a no ser aleatoria, es razonablemente representativa de la población y podría pasar por aleatoria.
</div>
<p>¿Qué calculamos? Pues un <strong>estimador</strong>: alguna función adecuada aplicada a los valores de la muestra, y que dependerá de lo que queramos estimar. Por ejemplo:</p>
<ul>
<li><p>Si queremos estimar la altura media de los estudiantes de la UIB, tomaremos una muestra aleatoria de estudiantes de la UIB, mediremos sus alturas y calcularemos su <strong>media aritmética</strong>.</p></li>
<li><p>Si queremos estimar la proporción de estudiantes de la UIB que han pasado la COVID-19, tomaremos una muestra aleatoria de estudiantes de la UIB, les haremos un test de anticuerpos y calcularemos la <strong>proporción muestral</strong> de positivos en la muestra.</p></li>
<li><p>Si queremos estimar el riesgo relativo para un estudiante de la UIB de suspender alguna asignatura si es fumador, tomaremos una muestra aleatoria de estudiantes de la UIB, anotaremos si fuman o no y si han suspendido alguna asignatura o no, y calcularemos el <strong>cociente entre las proporciones muestrales</strong> de suspensos entre los fumadores y los no fumadores de la muestra.</p></li>
</ul>

<div class="rmdimportant">
<strong>Un estimador es una variable aleatoria</strong>, definida sobre la población formada por las muestras de la población de partida. Por lo tanto, tiene función de densidad, función de distribución (que genéricamente llamaremos <strong>distribución muestral</strong>, para indicar que mide la probabilidad de que le pase algo al valor del <strong>estimador sobre una muestra</strong>), esperanza, desviación típica, etc.
</div>
</div>
<div id="la-media-muestral" class="section level2" number="1.2">
<h2><span class="header-section-number">1.2</span> La media muestral</h2>
<p>Cuando queremos estimar el valor medio de una variable sobre una población, tomamos una muestra de valores y calculamos su media aritmética, ¿verdad? Pues eso es la media muestral.</p>
<p>Dada una variable aleatoria <span class="math inline">\(X\)</span>, llamamos <strong>media muestral de</strong> (muestras de) <strong>tamaño <span class="math inline">\(n\)</span></strong> a la variable aleatoria <span class="math inline">\(\overline{X}\)</span> “Tomamos una <em>muestra aleatoria simple</em> de tamaño <span class="math inline">\(n\)</span> de <span class="math inline">\(X\)</span> y calculamos la media aritmética de sus valores”.</p>

<div class="rmdcaution">
Fijaos en que definimos la media muestral solo para muestras aleatorias simples. Naturalmente, tiene sentido definirla para muestras cualesquiera, pero entonces su distribución muestral dejaría de cumplir las propiedades que damos en esta sección. La misma advertencia vale a los estimadores que definimos en las próximas secciones.
</div>
<p>Veamos algunas propiedades de la distribución muestral de <span class="math inline">\(\overline{X}\)</span>:</p>

<div class="theorem">
<p><span id="thm:mitjmostgral" class="theorem"><strong>Teorema 1.1  </strong></span>Sea <span class="math inline">\(X\)</span> una variable aleatoria cualquiera de media <span class="math inline">\(\mu_X\)</span> y desviación típica <span class="math inline">\(\sigma_X\)</span>, y sea <span class="math inline">\(\overline{X}\)</span> la media muestral de tamaño <span class="math inline">\(n\)</span> de <span class="math inline">\(X\)</span>. Entonces:</p>
<ul>
<li><p><span class="math inline">\(E(\overline{X})=\mu_X\)</span></p></li>
<li><p><span class="math inline">\(\sigma(\overline{X})=\dfrac{\sigma_X}{\sqrt{n}}\)</span></p></li>
</ul>
</div>

<div class="rmdcorbes">
<p>Formalmente, la media muestral de tamaño <span class="math inline">\(n\)</span> de una variable aleatoria <span class="math inline">\(X\)</span> se define como la variable aleatoria
<span class="math display">\[
\overline{X}=\frac{X_1+\cdots+X_n}{n}
\]</span>
donde <span class="math inline">\(X_1,\ldots,X_n\)</span> son <span class="math inline">\(n\)</span> copias independientes de la variable <span class="math inline">\(X\)</span>.</p>
<p>Entonces, por la linealidad de la esperanza
<span class="math display">\[
  E(\overline{X})=\frac{E(X_1)+\cdots+E(X_n)}{n}=\frac{n\cdot \mu_X}{n}=\mu_X
\]</span>
porque, como <span class="math inline">\(X_1,\ldots,X_n\)</span> son copias de <span class="math inline">\(X\)</span>,
<span class="math inline">\(E(X_1)=\cdots=E(X_n)=\mu_X\)</span>.</p>
<p>Y por la linealidad de la varianza de la suma de variables <strong>independientes</strong>
<span class="math display">\[
\sigma(\overline{X})^2=\frac{\sigma(X_1)^2+\cdots+\sigma(X_n)^2}{n^2}=\frac{n\cdot \sigma_X^2}{n^2}=\frac{\sigma_X^2}{n}
\]</span>
porque, de nuevo, como <span class="math inline">\(X_1,\ldots,X_n\)</span> son copias de <span class="math inline">\(X\)</span>,
<span class="math inline">\(\sigma(X_1)^2=\cdots=\sigma(X_n)^2=\sigma_X^2\)</span>.</p>
</div>
<p>Que <span class="math inline">\(E(\overline{X})\)</span> sea <span class="math inline">\(\mu_X\)</span> nos indica que <span class="math inline">\(\overline{X}\)</span> sirve para estimar <span class="math inline">\(\mu_X\)</span>, porque <strong>su valor esperado es <span class="math inline">\(\mu_X\)</span></strong>:</p>
<blockquote>
<p>Si calculáramos muchas medias de muestras aleatorias de <span class="math inline">\(X\)</span>, es muy probable que, de media, obtuviéramos un valor muy cercano a <span class="math inline">\(\mu_X\)</span>.</p>
</blockquote>
<p>Cuando el valor esperado de un estimador es precisamente el parámetro poblacional que se quiere estimar, se dice que el estimador es <strong>insesgado</strong>. Así, el primer punto del teorema anterior nos dice que la media muestral <span class="math inline">\(\overline{X}\)</span> es un estimador insesgado de la media poblacional <span class="math inline">\(\mu_X\)</span>.</p>
<p>Que <span class="math inline">\(\sigma(\overline{X})\)</span> sea <span class="math inline">\(\sigma_X/\sqrt{n}\)</span> implica que la variabilidad de las medias muestrales crece con la variabilidad de <span class="math inline">\(X\)</span> y decrece si tomamos muestras de mayor tamaño. Esto último es razonable. Aunque la variabilidad de <span class="math inline">\(X\)</span> sea grande, si tomamos muestras grandes, es probable que los valores extremos se compensen al calcular sus medias y éstas tengan menos variabilidad que la variable <span class="math inline">\(X\)</span> original.</p>
<p>A <span class="math inline">\(\sigma_X/\sqrt{n}\)</span> se le llama el <strong>error típico de la media muestral</strong> (para la variable aleatoria <span class="math inline">\(X\)</span> y muestras de tamaño <span class="math inline">\(n\)</span>).</p>

<div class="example">
<p><span id="exm:simulacion1" class="example"><strong>Ejemplo 1.1  </strong></span>Vamos a realizar un experimento. Vamos a tomar una población de 10<sup>6</sup> sujetos y una variable aleatoria <span class="math inline">\(X\)</span> que sobre cada sujeto toma un valor real entre 0 y 1, todos estos valores con la misma probabilidad. Llamaremos <code>X</code> al vector con los 10<sup>6</sup> valores de esta variable aleatoria, y dibujaremos un histograma de este vector de números para que veáis que los valores salen muy dispersos. Mostramos el código de R para que podáis repetir el experimento por vuestra cuenta; como es una simulación, cada vez que lo ejecutéis dará resultados diferentes, pero el mismo efecto global.</p>
</div>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="distribuciones-muestrales.html#cb1-1" aria-hidden="true" tabindex="-1"></a>X<span class="ot">=</span><span class="fu">runif</span>(<span class="dv">10</span><span class="sc">^</span><span class="dv">6</span>)</span>
<span id="cb1-2"><a href="distribuciones-muestrales.html#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(X,<span class="at">freq=</span><span class="cn">FALSE</span>,<span class="at">main=</span><span class="st">&quot;Histograma de X&quot;</span>,<span class="at">xlab=</span><span class="st">&quot;&quot;</span>,<span class="at">ylab=</span><span class="st">&quot;Densidad&quot;</span>,<span class="at">col=</span><span class="st">&quot;light blue&quot;</span>)</span></code></pre></div>
<p><img src="INREMDN_files/figure-html/unnamed-chunk-16-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>La desviación típica <span class="math inline">\(\sigma_X\)</span> de la variable <span class="math inline">\(X\)</span> sobre nuestra población es</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="distribuciones-muestrales.html#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sd</span>(X)<span class="sc">*</span><span class="fu">sqrt</span>((<span class="dv">10</span><span class="sc">^</span><span class="dv">6-1</span>)<span class="sc">/</span><span class="dv">10</span><span class="sc">^</span><span class="dv">6</span>)</span></code></pre></div>
<pre><code>## [1] 0.2884943</code></pre>

<div class="rmdnote">
La función <code>sd</code> calcula la desviación típica muestral, con denominador <span class="math inline">\(\sqrt{n-1}\)</span>. Para calcular la desviación típica de verdad, con denominador <span class="math inline">\(\sqrt{n}\)</span> hay que multiplicarla por <span class="math inline">\(\sqrt{n-1}\)</span> y dividirla por <span class="math inline">\(\sqrt{n}\)</span>.
</div>
<p>Ahora vamos a tomar 1000 medias muestrales de tamaño 100 de esta población, las organizaremos en un vector que llamaremos <code>Medias</code> y dibujaremos un histograma de este vector de medias.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="distribuciones-muestrales.html#cb4-1" aria-hidden="true" tabindex="-1"></a>Medias<span class="ot">=</span><span class="fu">replicate</span>(<span class="dv">1000</span>,<span class="fu">mean</span>(<span class="fu">sample</span>(X,<span class="dv">100</span>,<span class="at">replace=</span><span class="cn">TRUE</span>)))</span>
<span id="cb4-2"><a href="distribuciones-muestrales.html#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(Medias, <span class="at">breaks=</span><span class="dv">15</span>,<span class="at">freq=</span><span class="cn">FALSE</span>,<span class="at">main=</span><span class="st">&quot;Histograma de las medias muestrales&quot;</span>,<span class="at">xlab=</span><span class="st">&quot;&quot;</span>,<span class="at">ylab=</span><span class="st">&quot;Densidad&quot;</span>,<span class="at">col=</span><span class="st">&quot;light blue&quot;</span>,<span class="at">xlim=</span><span class="fu">c</span>(<span class="fl">0.4</span>,<span class="fl">0.6</span>))</span></code></pre></div>
<p><img src="INREMDN_files/figure-html/unnamed-chunk-19-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>Podéis observar cómo los valores de estas medias se concentran alrededor de 0.5. Veamos su desviación típica:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="distribuciones-muestrales.html#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sd</span>(Medias)<span class="sc">*</span><span class="fu">sqrt</span>((<span class="dv">1000-1</span>)<span class="sc">/</span><span class="dv">1000</span>)</span></code></pre></div>
<pre><code>## [1] 0.02921408</code></pre>
<p>Fijaos cómo se acerca mucho al valor <span class="math inline">\(\sigma_X/\sqrt{100}=0.0288494\)</span> predicho por el teorema anterior. No coinciden exactamente, porque <span class="math inline">\(\sigma_X/\sqrt{100}\)</span> es el valor de la desviación típica <strong>poblacional</strong> de <span class="math inline">\(\overline{X}\)</span>, es decir, para toda la “población” de medias muestrales de muestras aleatorias simples de tamaño 100 de nuestra población de partida, y nosotros hemos tomado una muestra de “solo” 1000 medias de estas.</p>
<p>La media muestral <span class="math inline">\(\overline{X}\)</span> de tamaño <span class="math inline">\(n\)</span> de una variable aleatoria <span class="math inline">\(X\)</span> se interpreta formalmente como la variable aleatoria obtenida tomando <span class="math inline">\(n\)</span> copias independientes <span class="math inline">\(X_1,\ldots,X_n\)</span> de <span class="math inline">\(X\)</span> y calculando
<span class="math display">\[
\overline{X}=\frac{X_1+\cdots+X_n}{n}.
\]</span></p>
<p>Por lo tanto, es una combinación lineal de <span class="math inline">\(n\)</span> copias independientes de <span class="math inline">\(X\)</span>.
Recordando que una combinación de variables aleatorias normales independientes es normal, tenemos el resultado siguiente:</p>

<div class="theorem">
<p><span id="thm:unnamed-chunk-21" class="theorem"><strong>Teorema 1.2  </strong></span>Si <span class="math inline">\(X\)</span> es <span class="math inline">\(N(\mu_X,\sigma_X)\)</span>, entonces <span class="math inline">\(\overline{X}\)</span> es <span class="math inline">\(N(\mu_X,\sigma_X/\sqrt{n})\)</span>,
y por lo tanto
<span class="math display">\[
Z=\frac{\overline{X}-\mu_X}{\sigma_X/\sqrt{n}}
\]</span>
es <span class="math inline">\(N(0,1)\)</span>.</p>
</div>
<p>Si <span class="math inline">\(X\)</span> no es normal, la tesis del teorema anterior sigue siendo cierta “aproximadamente” siempre y cuando <span class="math inline">\(n\)</span> sea grande. Este resultado, llamado el <strong>Teorema Central del Límite</strong> es, como su nombre indica, uno de los más importantes en estadística.</p>

<div class="theorem">
<p><span id="thm:unnamed-chunk-22" class="theorem"><strong>Teorema 1.3  </strong></span>Sea <span class="math inline">\(X\)</span> una variable aleatoria <strong>cualquiera</strong> de esperanza <span class="math inline">\(\mu_X\)</span> y desviación típica <span class="math inline">\(\sigma_X\)</span>. Si <span class="math inline">\(n\)</span> es <strong>suficientemente grande</strong>,
<span class="math inline">\(\overline{X}\)</span> es <strong>aproximadamente</strong> <span class="math inline">\(N(\mu_X, {\sigma_X}/{\sqrt{n}})\)</span>
y por lo tanto
<span class="math display">\[
Z=\frac{\overline{X}-\mu_X}{{\sigma_X}/{\sqrt{n}}}
\]</span>
es aproximadamente <span class="math inline">\(N(0,1)\)</span>.</p>
</div>
<p>Dos observaciones:</p>
<ul>
<li><p>¿Cuándo una muestra es lo suficientemente grande como para poder invocar el Teorema Central del Límite? En realidad, depende de la <span class="math inline">\(X\)</span>. Cuánto más se parezca <span class="math inline">\(X\)</span> a una variable normal, más pequeñas pueden ser la muestras. Por fijar un valor, aceptaremos que “suficientemente grande” es <span class="math inline">\(n\geqslant 40\)</span>.</p></li>
<li><p>¿Qué quiere decir que una variable aleatoria sea “aproximadamente” normal? Pues que su función de distribución <span class="math inline">\(F_X\)</span> toma valores muy cercanos a la función de distribución de una normal. Recordad cómo una <span class="math inline">\(B(n,p)\)</span> con <span class="math inline">\(n\)</span> grande era “aproximadamente normal” en la lección anterior.</p></li>
</ul>

<div class="rmdnote">
Si miráis el histograma de las 1000 medias muestrales del Ejemplo <a href="distribuciones-muestrales.html#exm:simulacion1">1.1</a>, veréis que se parece al de una muestra de una variable normal. Es que <span class="math inline">\(\overline{X}\)</span> es aproximadamente normal, por el Teorema Central del Límite, aunque la variable <span class="math inline">\(X\)</span> sea muy diferente de una normal. Para verlo, en la figura que sigue superponemos al histograma de las medias la gráfica de la densidad de una variable normal de media y desviación típica las predichas por el Teorema Central del Límite.
</div>
<p><img src="INREMDN_files/figure-html/unnamed-chunk-24-1.png" width="480" style="display: block; margin: auto;" /></p>

<div class="rmdrecordau">
<p>En resumen:</p>
<ul>
<li><p>Si <span class="math inline">\(X\)</span> es normal, <span class="math inline">\(\overline{X}\)</span> es <span class="math inline">\(N(\mu_X,{\sigma_X}/{\sqrt{n}})\)</span>.</p></li>
<li><p>Si <span class="math inline">\(X\)</span> no es normal pero <span class="math inline">\(n\)</span> es grande (pongamos <span class="math inline">\(n\geqslant 40\)</span>, aunque puede ser menor si <span class="math inline">\(X\)</span> se parece a una normal y seguramente tendrá que ser mayor si <span class="math inline">\(X\)</span> es muy diferente de una normal), <span class="math inline">\(\overline{X}\)</span> es <em>aproximadamente</em> <span class="math inline">\(N(\mu_X,{\sigma_X}/{\sqrt{n}})\)</span>.</p></li>
</ul>
</div>

<div class="rmdcaution">
Las afirmaciones del bloque anterior son verdaderas para medias muestrales de muestras aleatorias simples. Si la muestra que usemos no podemos suponer que sea aleatoria simple, ninguno de los dos resultados es válido.
</div>
</div>
<div id="la-proporción-muestral" class="section level2" number="1.3">
<h2><span class="header-section-number">1.3</span> La proporción muestral</h2>
<p>Cuando queremos estimar la proporción de sujetos de una población que tienen una determinada característica, tomamos una muestra y calculamos la proporción de sujetos de la muestra con esta característica. Esta será la <strong>proporción muestral</strong> de sujetos con esta característica en nuestra muestra.</p>
<p>Dada una variable aleatoria <span class="math inline">\(X\)</span> de Bernoulli <span class="math inline">\(Be(p_X)\)</span>, la <strong>proporción muestral de</strong> (muestras de) <strong>tamaño <span class="math inline">\(n\)</span></strong>, <span class="math inline">\(\widehat{p}_X\)</span>, es la variable aleatoria consistente en tomar una muestra aleatoria de tamaño <span class="math inline">\(n\)</span> de <span class="math inline">\(X\)</span> y calcular la proporción de éxitos en la muestra: es decir, contar el número total de éxitos y dividir el resultado por <span class="math inline">\(n\)</span>.</p>
<p>Fijaos en que <span class="math inline">\(\widehat{p}_X\)</span> es un caso particular de media muestral <span class="math inline">\(\overline{X}\)</span>: estamos calculando medias muestrales de muestras aleatorias simples de la variable de Bernoulli <span class="math inline">\(X\)</span>. Por lo tanto, todo lo que hemos dicho para medias muestrales vale también para proporciones muestrales:</p>

<div class="theorem">
<p><span id="thm:unnamed-chunk-27" class="theorem"><strong>Teorema 1.4  </strong></span>Si <span class="math inline">\(X\)</span> es una variable aleatoria de Bernoulli con probabilidad poblacional de éxito <span class="math inline">\(p_X\)</span> y <span class="math inline">\(\widehat{p}_X\)</span> es la proporción muestral de tamaño <span class="math inline">\(n\)</span>:</p>
<ul>
<li><p><span class="math inline">\(E(\widehat{p}_X)=p_X\)</span></p></li>
<li><p><span class="math inline">\(\sigma({\widehat{p}_X})=\sqrt{\dfrac{p_X(1-p_X)}{n}}\)</span></p></li>
</ul>
</div>
<p><span class="math inline">\(E(\widehat{p}_X)=p_X\)</span> nos dice que <span class="math inline">\(\widehat{p}_X\)</span> es un estimador insesgado de <span class="math inline">\(p_X\)</span>. Si calculáramos muchas proporciones muestrales de muestras aleatorias de <span class="math inline">\(X\)</span>, es muy probable que, de media, obtuviéramos un valor muy cercano a la proporción poblacional de éxitos <span class="math inline">\(p_X\)</span>.</p>
<p><span class="math inline">\(\sigma({\widehat{p}_X})=\sqrt{\dfrac{p_X(1-p_X)}{n}}\)</span> nos dice que, fijada la variable <span class="math inline">\(X\)</span>, si tomamos muestras de tamaño mayor, la variabilidad de los resultados de <span class="math inline">\(\widehat{p}_X\)</span> disminuye.</p>

<div class="rmdimportant">
En el caso de la proporción muestral, a veces vamos a permitir tomar <strong>muestras aleatorias sin reposición</strong>. En este caso, seguimos teniendo que <span class="math inline">\(E(\widehat{p}_X)=p_X\)</span>, pero ahora,
si <span class="math inline">\(N\)</span> es el tamaño de la población,
<span class="math display">\[
\sigma({\widehat{p}_X})=\sqrt{\frac{p_X(1-p_X)}{n}}\cdot
\sqrt{\frac{\vphantom{(p_X}N-n}{N-1}}.
\]</span>
El factor
<span class="math display">\[
\sqrt{\frac{N-n}{N-1}}
\]</span>
que transforma <span class="math inline">\(\sigma({\widehat{p}_X})\)</span> para muestras aleatorias simples en la desviación típica de <span class="math inline">\({\widehat{p}_X}\)</span> para muestras aleatorias sin reposición es el <strong>factor de población finita</strong> que transformaba la desviación típica de una variable binomial (que cuenta éxitos en muestras aleatorias simples) en la desviación típica de una variable hipergeométrica (que cuenta éxitos en muestras aleatorias sin reposición).
</div>

<div class="rmdrecordau">
Y recordad que si el tamaño de la población <span class="math inline">\(N\)</span> es muy grande comparado con <span class="math inline">\(n\)</span>, podemos suponer que una muestra aleatoria sin reposición es simple.
</div>
<p>Si tomamos muestras aleatorias simples de tamaño <span class="math inline">\(n\)</span> de una variable aleatoria Bernoulli <span class="math inline">\(X\)</span>:</p>
<ul>
<li><p><span class="math inline">\(\sqrt{\dfrac{p_X(1-p_X)}{n}}\)</span> es el <strong>error típico</strong> de la variable aleatoria <span class="math inline">\(\widehat{p}_X\)</span>: su desviación típica.</p></li>
<li><p>Para cada muestra, <span class="math inline">\(\sqrt{\dfrac{\widehat{p}_X(1-\widehat{p}_X)}{n}}\)</span> es el <strong>error típico</strong> de la muestra, que estima el error típico de <span class="math inline">\(\widehat{p}_X\)</span>.</p></li>
</ul>
<p>Y como la proporción muestral es un caso particular de media muestral, por el Teorema Central del Límite tenemos el resultado siguiente:</p>

<div class="theorem">
<p><span id="thm:unnamed-chunk-30" class="theorem"><strong>Teorema 1.5  </strong></span>Si <span class="math inline">\(n\)</span> es grande y las muestras aleatorias son simples,
<span class="math inline">\(\widehat{p}_X\)</span> es aproximadamente <span class="math inline">\(N\big (p_X,\sqrt{{p_X(1-p_X)}/{n}}\big)\)</span>
y por lo tanto
<span class="math display">\[
\frac{\widehat{p}_X-p_X}{\sqrt{\frac{{p}_X(1-{p}_X)}{n}}}
\]</span>
es aproximadamente <span class="math inline">\(N(0,1)\)</span>.</p>
</div>
</div>
<div id="la-varianza-muestral" class="section level2" number="1.4">
<h2><span class="header-section-number">1.4</span> La varianza muestral</h2>
<p>Dada una variable aleatoria <span class="math inline">\(X\)</span>, llamamos:</p>
<ul>
<li><p><strong>Varianza muestral de</strong> (muestras de) <strong>tamaño <span class="math inline">\(n\)</span></strong>, <span class="math inline">\(\widetilde{S}_{X}^2\)</span>, a la variable aleatoria consistente en tomar una muestra aleatoria simple de tamaño <span class="math inline">\(n\)</span> de <span class="math inline">\(X\)</span> y calcular la varianza muestral de sus valores.</p></li>
<li><p><strong>Desviación típica muestral de</strong> (muestras de) <strong>tamaño <span class="math inline">\(n\)</span></strong>, <span class="math inline">\(\widetilde{S}_{X}\)</span>, a la variable aleatoria consistente en tomar una muestra aleatoria simple de tamaño <span class="math inline">\(n\)</span> de <span class="math inline">\(X\)</span> y calcular la desviación típica muestral de sus valores.</p></li>
</ul>
<p>Formalmente, estas variables se definen tomando <span class="math inline">\(n\)</span> copias independientes <span class="math inline">\(X_1,\ldots,X_n\)</span> de <span class="math inline">\(X\)</span> y calculando
<span class="math display">\[
\widetilde{S}_{X}^2=\frac{\sum_{i=1}^n (X_{i}-\overline{X})^2}{n-1},\quad 
\widetilde{S}_{X}=+\sqrt{\widetilde{S}_{X}^2}
\]</span></p>
<p>Tenemos los dos resultados siguientes. El primero nos dice que <span class="math inline">\(\widetilde{S}_{X}^2\)</span> es un estimador insesgado de la varianza poblacional <span class="math inline">\(\sigma_{X}^2\)</span>.</p>

<div class="theorem">
<p><span id="thm:unnamed-chunk-31" class="theorem"><strong>Teorema 1.6  </strong></span><span class="math inline">\(E(\widetilde{S}_{X}^2)=\sigma_{X}^2\)</span>.</p>
</div>
<p>Por lo tanto, <strong>esperamos</strong> que la varianza muestral de una muestra aleatoria simple de <span class="math inline">\(X\)</span> valga <span class="math inline">\(\sigma_{X}^2\)</span>, en el sentido usual de que si tomamos muestras aleatorias simples de <span class="math inline">\(X\)</span> de tamaño <span class="math inline">\(n\)</span> grande y calculamos sus varianzas muestrales, muy probablemente obtengamos de media un valor muy cercano a <span class="math inline">\(\sigma_{X}^2\)</span>.</p>

<div class="rmdcaution">
Y por lo tanto <strong>no esperamos</strong> que la varianza “a secas” de una muestra aleatoria simple valga <span class="math inline">\(\sigma_{X}^2\)</span>, porque la varianza muestral y la varianza “a secas” dan valores diferentes (tienen el mismo numerador y denominadores diferentes).
</div>
<p>El segundo resultado nos dice que si la variable <span class="math inline">\(X\)</span> es <strong>normal</strong>, un múltiplo adecuado de <span class="math inline">\(\widetilde{S}_{X}^2\)</span> tiene distribución muestral conocida, lo que nos permitirá calcular probabilidades de sucesos relativos a <span class="math inline">\(\widetilde{S}_{X}^2\)</span>.</p>

<div class="theorem">
<p><span id="thm:khi2" class="theorem"><strong>Teorema 1.7  </strong></span>Si <span class="math inline">\(X\)</span> es <span class="math inline">\(N(\mu_X,\sigma_X)\)</span> y tomamos muestras de tamaño <span class="math inline">\(n\)</span>, la variable aleatoria
<span class="math display">\[
\chi^2=  \dfrac{(n-1)\widetilde{S}_{X}^2}{\sigma_{X}^2}
\]</span>
tiene una distribución conocida, llamada <strong>ji cuadrado con <span class="math inline">\(n-1\)</span> grados de libertad</strong>, <span class="math inline">\(\chi_{n-1}^2\)</span>.</p>
</div>

<div class="rmdrecordau">
La letra griega <span class="math inline">\(\chi\)</span> en castellano se lee <strong>ji</strong>; en catalán, <strong>khi</strong>; en inglés, <strong>chi</strong>, pronunciado “xai”.
</div>
<p><img src="INREMDN_files/figure-html/chihogar.png" width="40%" style="display: block; margin: auto;" /></p>
<p>La distribución <span class="math inline">\(\chi_\nu^2\)</span>, donde <span class="math inline">\(\nu\)</span> es un parámetro llamado sus <strong>grados de libertad</strong>, es la distribución de probabilidad de la suma de los cuadrados de <span class="math inline">\(\nu\)</span> variables aleatorias normales estándar independientes. Para R es <code>chisq</code>. Os puede interesar recordar que una variable <span class="math inline">\(\chi_\nu^2\)</span> de tipo ji cuadrado con <span class="math inline">\(\nu\)</span> grados de libertad:</p>
<ul>
<li><p>Tiene valor esperado <span class="math inline">\(E(\chi_\nu^2)=\nu\)</span> y varianza <span class="math inline">\(\sigma(\chi_\nu^2)^2=2 \nu\)</span>.</p></li>
<li><p>Su función de distribución es estrictamente creciente.</p></li>
<li><p>Tiene una distribución asimétrica a la derecha, como muestra el gráfico siguiente:</p></li>
</ul>
<p><img src="INREMDN_files/figure-html/unnamed-chunk-35-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>A medida que el número de grados de libertad <span class="math inline">\(\nu\)</span> crece, la asimetría tiende a desaparecer y, por el Teorema Central del Límite, si <span class="math inline">\(\nu\)</span> es lo bastante grande, la distribución <span class="math inline">\(\chi_\nu^2\)</span> se aproxima a la de una variable normal <span class="math inline">\(N(\nu,\sqrt{2\nu})\)</span>.</p>
<p><img src="INREMDN_files/figure-html/unnamed-chunk-36-1.png" width="480" style="display: block; margin: auto;" /></p>

<div class="rmdcaution">
<p>Tened cuidado:</p>
<ul>
<li><p>Si la variable poblacional <span class="math inline">\(X\)</span> no es normal, la conclusión del Teorema <a href="distribuciones-muestrales.html#thm:khi2">1.7</a> no es verdadera.</p></li>
<li><p>Aunque <span class="math inline">\(X\)</span> sea normal, <span class="math inline">\(E(\widetilde{S}_{X})\neq \sigma_{X}\)</span>. La desviación típica muestral es un estimador <strong>sesgado</strong> de <span class="math inline">\(\sigma_{X}\)</span> (pero tiene otras buenas propiedades que hacen que la usemos igualmente).</p></li>
<li><p>Ya lo hemos comentado antes. Si <span class="math inline">\(S^2_{X}\)</span> es la varianza “a secas” (dividiendo por <span class="math inline">\(n\)</span> en vez de por <span class="math inline">\(n-1\)</span>), <span class="math inline">\(E(S^2_{X})\neq \sigma^2_{X}\)</span>. Esto lo podéis comprobar fácilmente, porque <span class="math inline">\(S_X^2\)</span> se obtiene a partir de <span class="math inline">\(\widetilde{S}_{X}^2\)</span> cambiando el denominador,
<span class="math display">\[
S_X^2=\frac{n-1}{n} \widetilde{S}_{X}^2
\]</span>
y por lo tanto
<span class="math display">\[
E(S_X^2)=\frac{n-1}{n}E(\widetilde{S}_{X}^2)=\frac{n-1}{n}\sigma^2_{X}
\]</span></p></li>
</ul>
</div>
</div>
<div id="la-distribución-t-de-student" class="section level2" number="1.5">
<h2><span class="header-section-number">1.5</span> La distribución t de Student</h2>
<p>Recordad que si la variable poblacional <span class="math inline">\(X\)</span> es <span class="math inline">\(N(\mu_X,\sigma_X)\)</span> y tomamos muestras aleatorias simples de tamaño <span class="math inline">\(n\)</span>, entonces la variable
<span class="math display">\[
\frac{\overline{X}-\mu_X}{\sigma_{X}/\sqrt{n}}
\]</span>
es normal estándar. Desde el punto de vista teórico, para obtener fórmulas, esto será útil, pero normalmente no nos sirve para calcular la probabilidad de que a <span class="math inline">\(\overline{X}\)</span> le pase algo, porque casi nunca sabemos la desviación típica poblacional <span class="math inline">\(\sigma_{X}\)</span>. ¿Qué pasa si la estimamos por medio de <span class="math inline">\(\widetilde{S}_{X}\)</span> con la misma muestra con la que calculamos <span class="math inline">\(\overline{X}\)</span>? Pues que el resultado siguiente nos salva el día, porque la variable que resulta tiene distribución conocida.</p>

<div class="theorem">
<span id="thm:unnamed-chunk-38" class="theorem"><strong>Teorema 1.8  </strong></span>Sea <span class="math inline">\(X\)</span> una variable <span class="math inline">\(N(\mu_X,\sigma_X)\)</span>. Si tomamos muestras aleatorias simples de tamaño <span class="math inline">\(n\)</span>, la variable aleatoria
<span class="math display">\[
T=\frac{\overline{X}-\mu_X}{\widetilde{S}_{X}/\sqrt{n}}
\]</span>
tiene una distribución conocida, llamada <strong>t de Student con <span class="math inline">\(n-1\)</span> grados de libertad</strong>, <span class="math inline">\(t_{n-1}\)</span>.
</div>
<p>Al denominador <span class="math inline">\(\widetilde{S}_{X}/\sqrt{n}\)</span> de la <span class="math inline">\(T\)</span> del teorema anterior se le llama el <strong>error típico</strong> de la muestra, y estima el error típico <span class="math inline">\(\sigma_X/\sqrt{n}\)</span> de la media muestral <span class="math inline">\(\overline{X}\)</span>.</p>

<div class="rmdimportant">
Fijaos en que el teorema anterior es solo para variables poblacionales <span class="math inline">\(X\)</span> <strong>normales</strong>. Por el Teorema Central del Límite, si <span class="math inline">\(n\)</span> es grande, <span class="math inline">\(T\)</span> es aproximadanente <span class="math inline">\(t_{n-1}\)</span> aunque <span class="math inline">\(X\)</span> no sea normal.
</div>
<p>Para R, la distribución t de Student es <code>t</code>, a secas. Algunas propiedades que conviene que recordéis de las variables <span class="math inline">\(T_\nu\)</span> que tienen distribución <span class="math inline">\(t\)</span> de Student con <span class="math inline">\(\nu\)</span> grados de libertad, <span class="math inline">\(t_\nu\)</span>:</p>
<ul>
<li><p>Su valor esperado es <span class="math inline">\(E(T_\nu)=0\)</span> y su varianza es <span class="math inline">\(\sigma(T_\nu)=\dfrac{\nu}{\nu-2}\)</span> (en realidad esto solo es verdad si <span class="math inline">\(\nu\geqslant 3\)</span>, pero no hace falta recordarlo).</p></li>
<li><p>Su función de distribución es estrictamente creciente.</p></li>
<li><p>Su función de distribución es simétrica respecto de 0 (como la de una <span class="math inline">\(N(0,1)\)</span>):
<span class="math display">\[
P(T_\nu\leqslant-x)=P(T_\nu\geqslant x)=1-P(T_\nu\leqslant x)
\]</span></p></li>
<li><p>Si <span class="math inline">\(\nu\)</span> es grande (digamos, de nuevo, <span class="math inline">\(\nu\geqslant 40\)</span>), <span class="math inline">\(T_\nu\)</span> es aproximadamente una <span class="math inline">\(N(0,1)\)</span> (pero con un poco más de varianza, porque <span class="math inline">\(\nu/(\nu-2)&gt;1\)</span>, y por lo tanto un poco más achatada). Esto es consecuencia del Teorema Central del Límite.</p></li>
</ul>
<p><img src="INREMDN_files/figure-html/unnamed-chunk-40-1.png" width="480" style="display: block; margin: auto;" /></p>
<p><img src="INREMDN_files/figure-html/unnamed-chunk-41-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>Denotaremos por <span class="math inline">\(t_{\nu,q}\)</span> el <span class="math inline">\(q\)</span>-cuantil de una variable aleatoria <span class="math inline">\(T_{\nu}\)</span> con distribución <span class="math inline">\(t_\nu\)</span>. Es decir, <span class="math inline">\(t_{\nu,q}\)</span> es el valor tal que
<span class="math display">\[
P(T_{\nu}\leqslant t_{\nu,q})=q
\]</span>
Entonces:</p>
<ul>
<li><p>Por la simetría de la distribución <span class="math inline">\(t_\nu\)</span>,
<span class="math display">\[
t_{\nu,q}=-t_{\nu,1-q}.
\]</span>
Exactamente lo mismo que pasaba con la normal estándar</p></li>
<li><p>Si <span class="math inline">\(\nu\)</span> es grande, <span class="math inline">\(T_\nu\)</span> será aproximadamente una <span class="math inline">\(N(0,1)\)</span> y por lo tanto <span class="math inline">\(t_{\nu,q}\)</span> es aproximadamente igual a <span class="math inline">\(z_q\)</span>.</p></li>
</ul>

<div class="rmdcaution">
<p>No confundáis:</p>
<ul>
<li><p><strong>Desviación típica de una variable aleatoria</strong>: El parámetro poblacional, normalmente desconocido. Es <span class="math inline">\(\sigma_X\)</span>.</p></li>
<li><p><strong>Desviación típica</strong> (muestral o no) <strong>de una muestra</strong>: El estadístico que calculamos sobre la muestra. Es <span class="math inline">\(\widetilde{S}_X\)</span> (la muestral) o <span class="math inline">\({S}_X\)</span> (la “a secas”).</p></li>
<li><p><strong>Error típico de la media muestral</strong>: La desviación típica de la variable media muestral. Es <span class="math inline">\(\sigma_X/\sqrt{n}\)</span>, con <span class="math inline">\(n\)</span> el tamaño de las muestras.</p></li>
<li><p><strong>Error típico de una muestra</strong>: Estimación del error típico del estimador a partir de la muestra. Es <span class="math inline">\(\widetilde{S}_X/\sqrt{n}\)</span>, con <span class="math inline">\(n\)</span> el tamaño de la muestra.</p></li>
</ul>
<p>Fijaos en que el denominador <span class="math inline">\(\sqrt{n}\)</span> hace que, en general, los errores típicos sean mucho más pequeños que las desviaciones típicas. Id con cuidado, porque esto se usa a menudo en artículos para enmascarar los resultados. Si una muestra ha salido con una dispersión muy grande, se da su error típico en vez de su desviación típica y parece que ha salido más concentrada.</p>
</div>
</div>
<div id="test" class="section level2" number="1.6">
<h2><span class="header-section-number">1.6</span> Test</h2>
<p><strong>(1)</strong> Si el tamaño de una muestra aleatoria simple aumenta (marca todas las afirmaciones correctas):</p>
<ol style="list-style-type: decimal">
<li>La media muestral siempre disminuye.</li>
<li>El error típico de la media muestral siempre disminuye.</li>
<li>El error típico de la muestra siempre disminuye.<br />
</li>
<li>La varianza muestral siempre aumenta.</li>
<li>El número de grados de libertad del estimador <span class="math inline">\(\chi^2\)</span> asociado a la varianza muestral siempre aumenta.</li>
<li>Ninguna de las otras afirmaciones es correcta</li>
</ol>
<p><strong>(2)</strong> Si queremos disminuir a la mitad el error típico de la media muestral:</p>
<ol style="list-style-type: decimal">
<li>Tenemos que aumentar en un 50% el tamaño de las muestras.</li>
<li>Tenemos que doblar el tamaño de las muestras.</li>
<li>Tenemos que cuadruplicar el tamaño de las muestras.<br />
</li>
<li>Tenemos que dividir por 2 el tamaño de las muestras.</li>
<li>Tenemos que dividir por 4 el tamaño de las muestras.</li>
<li>Ninguna de las otras respuestas es correcta.</li>
</ol>
<p><strong>(3)</strong> La prevalencia de una afección en una población es del 10%. Si estimamos dicha prevalencia repetidamente mediante las proporciones muestrales de muestras aleatorias simples de tamaño 1000, estas estimaciones siguen una distribución que (marca todas las afirmaciones correctas):</p>
<ol style="list-style-type: decimal">
<li>Es una distribución muestral.</li>
<li>Es aproximadamente normal.<br />
</li>
<li>Es binomial.</li>
<li>Tiene media 0.1.<br />
</li>
<li>Tiene media 900.</li>
<li>Ninguna de las otras afirmaciones es correcta</li>
</ol>
<p><strong>(4)</strong> Sobre una muestra de 100 mujeres se obtuvo una concentración media de la hemoglobina de 10 con una desviación típica de 2. ¿Qué vale el error típico de la muestra (para la media muestral, se entiende)?</p>
<ol style="list-style-type: decimal">
<li>0.02</li>
<li>0.04</li>
<li>0.2</li>
<li>0.4</li>
<li>1</li>
<li>Ninguno de los anteriores</li>
</ol>
<p><strong>(5)</strong> ¿Cuáles de las afirmaciones siguientes sobre la media muestral son verdaderas? Marca todas las respuestas correctas.</p>
<ol style="list-style-type: decimal">
<li>Si la distribución poblacional es normal, siempre coincide con la media de la distribución poblacional.</li>
<li>Si la distribución poblacional es normal, siempre coincide con la mediana de la distribución poblacional.</li>
<li>Siempre sirve para estimar la media poblacional, aunque la distribución poblacional no sea normal.<br />
</li>
<li>Si la distribución poblacional es normal, sirve para estimar la mediana poblacional.</li>
<li>Se calcula sumando todos los valores de la muestra y dividiendo por <span class="math inline">\(n-1\)</span>, donde <span class="math inline">\(n\)</span> indica el tamaño de la muestra.</li>
<li>Ninguna de las otras respuestas es correcta.</li>
</ol>
<p><strong>(6)</strong> La concentración de un cierto metabolito en sangre tiene un valor medio <span class="math inline">\(\mu\)</span>. Si tomamos muestras aleatorias simples de 20 individuos, calculamos su media muestral <span class="math inline">\(\overline{X}\)</span> y su desviación típica muestral <span class="math inline">\(\widetilde{S}_X\)</span> (marca la continuación más correcta):</p>
<ol style="list-style-type: decimal">
<li>El estadístico <span class="math inline">\(\frac{\overline{X}-\mu}{\widetilde{S}_X/\sqrt{n}}\)</span> tiene siempre distribución normal.</li>
<li>El estadístico <span class="math inline">\(\frac{\overline{X}-\mu}{\widetilde{S}_X/\sqrt{n}}\)</span> tiene siempre distribución t de Student.</li>
<li>El estadístico <span class="math inline">\(\frac{\overline{X}-\mu}{\widetilde{S}_X/\sqrt{n}}\)</span> tiene distribución normal si la concentración sigue una ley normal.</li>
<li>El estadístico <span class="math inline">\(\frac{\overline{X}-\mu}{\widetilde{S}_X/\sqrt{n}}\)</span> tiene distribución t de Student si la concentración tiene distribución normal.<br />
</li>
<li>El estadístico <span class="math inline">\(\frac{\overline{X}-\mu}{\widetilde{S}_X/\sqrt{n}}\)</span> no tiene nunca ni distribución normal ni distribución t de Student, porque las muestras no son lo suficientemente grandes.</li>
</ol>
<p><strong>(7)</strong> Tenemos una variable aleatoria <span class="math inline">\(X\)</span> normal de media <span class="math inline">\(\mu\)</span> y desviación típica <span class="math inline">\(\sigma\)</span>. Tomamos muestras aleatorias simples de tamaño <span class="math inline">\(n\)</span>, y denotamos por <span class="math inline">\(\widetilde{S}_X\)</span> su desviación típica muestral. ¿Cuáles de las afirmaciones siguientes son verdaderas? Marca todas las respuestas verdaderas:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(E(\widetilde{S}_X^2)=\sigma^2\)</span>.</li>
<li><span class="math inline">\(E(\widetilde{S}_X)=\sigma\)</span>.</li>
<li><span class="math inline">\(\widetilde{S}_X^2\)</span> sigue una distribución ji cuadrado con <span class="math inline">\(n-1\)</span> grados de libertad.</li>
<li><span class="math inline">\((n-1)\widetilde{S}_X^2/\sigma^2\)</span> sigue una distribución ji cuadrado con <span class="math inline">\(n-1\)</span> grados de libertad.</li>
<li><span class="math inline">\((n-1)\widetilde{S}_X/\sigma\)</span> sigue una distribución ji cuadrado con <span class="math inline">\(n-1\)</span> grados de libertad.</li>
<li>Todas las otras respuestas son falsas.</li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="chap-IC.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection",
"download": ["pdf", "epub"]
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
