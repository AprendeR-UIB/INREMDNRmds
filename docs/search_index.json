[["index.html", "Presentación", " Presentación Esto es una edición en línea de los apuntes de Introducción a la Investigación en Salud y Bioestadística del grado de Medicina de la UIB. Este trabajo se publica bajo licencia Atribución-No Comercial-SinDerivados 4.0 Estos apuntes están permanentemente en construcción. En la lista siguiente iremos anunciando las actualizaciones: 2021-09-15: Subidas las lecciones 1 a 12. El libro está escrito en R Markdown, usando RStudio como editor de textos y el paquete bookdown para convertir los ficheros markdown en un libro. Significado de algunas cajas: Material muy importante. ¡Cuidado! Ejercicio. Detalles matemáticos que os pueden interesar, pero que podéis obviar sin ningún problema. Comentario que queremos enfatizar. Comentario que queremos que recordéis Cuestión en la que queremos que caigáis en la cuenta. Acabamos de matar un gatito "],["variables-aleatorias-continuas.html", "Lección 1 Variables aleatorias continuas 1.1 Densidad y distribución 1.2 Esperanza, varianza, cuantiles… 1.3 Variables aleatorias normales 1.4 Test", " Lección 1 Variables aleatorias continuas Recordad que una variable aleatoria continua toma valores continuos. Por ejemplo: Peso de una persona Nivel de colesterol en sangre Diámetro de un tumor En este curso vamos a restringirnos a variables aleatorias continuas \\(X: \\Omega\\to \\mathbb{R}\\) que satisfacen la siguiente propiedad extra: su función de distribución \\[ \\begin{array}{rcl} F_X: \\mathbb{R} &amp; \\to &amp; [0,1]\\\\ x &amp;\\mapsto &amp;P(X\\leqslant x) \\end{array} \\] es continua. Todas las variables aleatorias continuas que os puedan interesar en algún momento van a cumplir esta propiedad, así que no perdemos nada imponiéndola. ¿Y qué ganamos? Pues que podemos usar todas las técnicas matemáticas aplicables a funciones continuas para estudiar \\(F_X\\). Por ejemplo, nuestras variables continuas verifican la propiedad siguientes: Teorema 1.1 Si \\(X\\) es una variable aleatoria continua, la probabilidad de que tome cada valor concreto es 0: \\[ P(X=a)=0 \\text{ para todo $a\\in \\mathbb{R}$}. \\] Por si pasa por aquí alguien que necesite una demostración: \\[ \\begin{array}{l} \\displaystyle P(X=a) = P(X\\leqslant a)-P(X&lt;a)=P(X\\leqslant a)-P\\Big(\\bigcup_{n\\geqslant 1} \\Big(X\\leqslant a-\\frac{1}{n}\\Big)\\Big)\\\\ \\displaystyle \\qquad= P(X\\leqslant a)-\\lim_{n\\geqslant 1}P\\Big(X\\leqslant a-\\frac{1}{n}\\Big)\\\\ \\displaystyle \\qquad= F_X(a)-\\lim_{n\\geqslant 1}F_X\\Big(a-\\frac{1}{n}\\Big)=0 \\end{array} \\] porque \\(F_X\\) es continua. En particular, para una variable aleatoria continua: Probabilidad 0 no significa imposible. Cada valor de \\(X\\) tiene probabilidad 0, pero cuando tomamos un sujeto, tendrá algún valor de \\(X\\), ¿no?. Por lo tanto, su valor de \\(X\\) es posible, aunque tenga probabilidad 0. De \\(P(X=a)=0\\) se deduce que la probabilidad de un suceso definido con una desigualdad es exactamente la misma que la del suceso correspondiente definido con una desigualdad estricta. En particular, contrariamente a lo que pasaba en las variables aleatorias discretas, para una variable aleatoria continua siempre tenemos que \\[ P(X\\leqslant a)=P(X&lt;a) \\] porque \\[ P(X\\leqslant a)=P(X&lt;a)+P(X=a)=P(X&lt;a)+0=P(X&lt;a). \\] De manera similar: \\(P(X\\geqslant a)=P(X&gt; a)+P(X=a)=P(X&gt; a)\\) \\(P(a\\leqslant X\\leqslant b)=P(a&lt;X&lt;b)+P(X=a)+P(X=b)\\) \\(=P(a&lt;X&lt;b)\\) 1.1 Densidad y distribución Sea \\(X\\) una variable aleatoria continua. Como ya hemos dicho, su función de distribución \\(F_X\\) se sigue definiendo como \\[ x\\mapsto F_X(x)=P(X\\leqslant x) \\] Pero puesto que tenemos que \\(P(X=x)=0\\), ahora no podemos definir la función de densidad de \\(X\\) como \\(f_X(x)=P(X=x)\\). ¿Qué podemos hacer? Recordad que, en las variables aleatorias discretas \\[ F_X(a)=\\sum_{x\\leqslant a} f_X(x) \\] En el contexto de matemáticas “continuas”, la suma \\(\\sum\\) se traduce en la integral \\(\\int\\). Se define entonces la función de densidad de una variable aleatoria continua \\(X\\) como la función \\(f_X:\\mathbb{R}\\to \\mathbb{R}\\) tal que: \\(f_X(x)\\geqslant 0\\), para todo \\(x\\in \\mathbb{R}\\) \\(\\displaystyle F_X(a)=\\int_{-\\infty}^a f_{X}(x)\\, dx\\) para todo \\(a\\in \\mathbb{R}\\). Recordad (o aprended por primera vez) que la integral tiene una interpretación sencilla en términos de áreas. En concreto, dados \\(a\\in \\mathbb{R}\\) y una función \\(f(x)\\), la integral \\[ \\int_{-\\infty}^a f(x)\\, dx \\] es igual al área de la región a la izquierda de la recta vertical \\(x=a\\) comprendida entre la curva \\(y=f(x)\\) y el eje de abscisas \\(y=0\\). Por lo tanto, la función de densidad \\(f_X\\) de \\(X\\) es la función positiva tal que para todo \\(a\\in \\mathbb{R}\\), \\(F_X(a)\\) es igual al área bajo la curva \\(y=f_X(x)\\) (entre esta curva y el eje de abscisas) a la izquierda de \\(x=a\\). ¿Cuál es la idea intuitiva que hay detrás de esta definición de densidad? Suponed que dibujamos histogramas de frecuencias relativas de los valores de \\(X\\) sobre toda la población. Como estamos hablando de toda la población, la frecuencia relativa de cada clase es la proporción de individuos de la población cuyo valor de \\(X\\) pertenece a esta clase: es decir, la probabilidad de que \\(X\\) caiga dentro de la clase. Recordad que, en un histograma de frecuencias relativas: La frecuencia relativa (ahora, la probabilidad) de cada clase es el área de su barra, es decir, el ancho de la clase por la altura de la barra. Llamamos a la altura de una barra la densidad de la clase. Si \\(a\\) es un extremo de una clase, la frecuencia relativa acumulada hasta \\(a\\) (la probabilidad de que \\(X\\leqslant a\\)) es la suma de las áreas de las barras a la izquierda de \\(a\\). Si dibujamos los histogramas de \\(X\\) tomando clases cada vez más estrechas, sus polígonos de frecuencias (en rojo) tienden a dibujar una curva: Cuando el ancho de las clases tiende a 0, obtenemos una curva que es el límite de estos polígonos de frecuencias: En el límite, la probabilidad de que \\(X\\leqslant a\\) será el límite de las sumas de las áreas de las barras a la izquierda de \\(a\\), y por tanto el área a la izquierda de \\(a\\) bajo esta curva límite. Esto nos dice que esta curva es precisamente la función de densidad \\(y=f_X(x)\\). La función de densidad \\(f_X\\) de una variable aleatoria continua \\(X\\) es la función límite de los polígonos de frecuencias de histogramas de \\(X\\) cuando el ancho de las clases tiende a 0. Veamos algunas propiedades que se deducen de que \\(F_X(a)=P(X\\leqslant a)\\) sea igual al área bajo la curva \\(y=f_X(x)\\) a la izquierda de \\(x=a\\): Como \\(P(X&lt;\\infty)=P(\\Omega)=1\\), el área total bajo la curva \\(y=f_X(x)\\) es 1. \\(P(a\\leqslant X\\leqslant b)=P(X\\leqslant b)-P(X&lt;a)\\) es el área bajo la curva \\(y=f_X(x)\\) a la izquierda de \\(x=b\\) menos el área bajo la curva \\(y=f_X(x)\\) a la izquierda de \\(x=a\\), es decir, \\(P(a\\leqslant X\\leqslant b)\\) es igual al área bajo la curva \\(y=f_X(x)\\) entre \\(x=a\\) y \\(x=b\\). Si \\(\\varepsilon&gt;0\\) es muy, muy pequeño, el área bajo \\(y=f_X(x)\\) entre \\(a-\\varepsilon\\) y \\(a+\\varepsilon\\) es aproximadamente igual a la del rectángulo de base el intervalo \\([a-\\varepsilon,a+\\varepsilon]\\) y altura \\(f_X(a)\\), que vale \\(2\\varepsilon\\cdot f_X(a)\\) (ved la Figura 1.1). Es decir, \\[ P(a-\\varepsilon\\leqslant X\\leqslant a+\\varepsilon)\\approx 2\\varepsilon\\cdot f_X(a). \\] Figura 1.1: El área bajo la curva alrededor de \\(a\\) es aproximadamente igual a la del rectángulo de altura \\(f_X(a)\\) Por lo tanto \\(f_X(a)\\) nos da una indicación de la probabilidad de que \\(X\\) valga aproximadamente \\(a\\) (pero no es \\(P(X=a)\\), que vale 0). Es decir, por ejemplo, si \\(f_X(a)=0.1\\) y \\(f_X(b)=0.5\\), la probabilidad de que \\(X\\) tome un valor muy cercano a \\(b\\) es 5 veces mayor que la probabilidad de que tome un valor muy cercano a \\(a\\). Pero \\(P(X=a)=P(X=b)=0\\), así que, por favor, evitad decir que “la probabilidad de que \\(X\\) valga \\(b\\) es 5 veces mayor que la probabilidad de que valga \\(a\\)”. Sí, ya sabemos que \\(5\\cdot 0=0\\), pero la frase es engañosa. Unas consideraciones finales: Lo hemos dicho en la definición, y lo hemos usado implícitamente en toda la sección, pero lo volvemos a repetir: \\(f_X(x)\\geqslant 0\\) para todo \\(x\\in \\mathbb{R}\\). En realidad, que \\(f_X(x)\\) sea \\(\\geqslant 0\\) para todo \\(x\\in \\mathbb{R}\\) es consecuencia de que \\(F_X\\) sea positiva y creciente (recordad que las funciones de distribución son siempre crecientes, porque si \\(x&lt;y\\), \\(F_X(x)=P(X\\leqslant x)\\leqslant P(X\\leqslant y)=F_X(y)\\)) y coincida con \\(\\int_{-\\infty}^x f_X(x)\\,dx\\). \\(f_X(x)\\) no es una probabilidad, y por lo tanto puede ser mayor que 1. Por ejemplo, el gráfico siguiente muestra la densidad de una variable normal \\(N(0,0.01)\\) (véase la Sección 1.3), que llega a valer casi 40. La función de densidad \\(f_X\\) no tiene por qué ser continua, aunque la función de distribución \\(F_X\\) lo sea. 1.2 Esperanza, varianza, cuantiles… La esperanza y la varianza de una variable aleatoria continua \\(X\\), con función de densidad \\(f_X\\), se definen como en el caso discreto, substituyendo la suma \\(\\sum_{x\\in D_x}\\) por una integral. La media, o esperanza (valor medio, valor esperado…), de \\(X\\) es \\[ E(X)=\\int_{-\\infty}^{\\infty}x \\cdot f_{X}(x)\\, dx \\] Es decir, es el área comprendida entre el eje de abscisas y la curva \\(y=xf_X(x)\\). Como en el caso discreto, también la denotaremos a veces por \\(\\mu_X\\). Este valor tiene la misma interpretación que en el caso discreto: Representa el valor medio de \\(X\\) sobre el total de la población. Es (con probabilidad 1) el límite de la media aritmética de los valores de \\(X\\) sobre muestras aleatorias simples de tamaño \\(n\\), cuando \\(n\\to \\infty\\). Si \\(g:\\mathbb{R}\\to \\mathbb{R}\\) es una función continua, la esperanza de \\(g(X)\\) es \\[ E(g(X))=\\int_{-\\infty}^{+\\infty} g(x) f_X(x)dx \\] La varianza de \\(X\\) es \\[ \\sigma(X)^2=E((X-\\mu_X)^2) \\] y se puede demostrar que es igual a \\[ \\sigma(X)^2=E(X^2)-\\mu_X^2 \\] También se escribe \\(\\sigma_X^2\\). La desviación típica de \\(X\\) es \\[ \\sigma(X)=+\\sqrt{\\sigma(X)^2} \\] y también se escribe \\(\\sigma_X\\). Como en el caso discreto, la varianza y la desviación típica miden la variabilidad de los resultados de \\(X\\) respecto de su valor medio. Estos parámetros de \\(X\\) tienen las mismas propiedades en el caso continuo que en el discreto. Las recordamos: Si \\(b\\) es una variable aleatoria constante, \\(E(b)=b\\) y \\(\\sigma(b)^2=0\\). Si \\(\\sigma(X)^2=0\\), \\(X\\) es constante. Y por supuesto, si \\(X\\) solo puede tomar un valor, ya no es continua, sino discreta. Por lo tanto, por convenio, de ahora en adelante supondremos que nuestras variables aleatorias continuas siempre tienen varianza no nula. Si \\(X_1,\\ldots,X_n\\) son variables aleatorias y \\(a_1,\\ldots,a_n,b\\in \\mathbb{R}\\), \\[ E(a_1X_1+\\cdots+a_nX_n+b)=a_1E(X_1)+\\cdots+a_nE(X_n)+b \\] En particular: \\(E(a X+b)=a E(X)+b\\). \\(E(X+Y)=E(X)+E(Y)\\). Si \\(X\\leqslant Y\\), entonces \\(E(X)\\leqslant E(Y)\\). Si \\(a,b\\in \\mathbb{R}\\), \\(\\sigma(aX+b)^2=a^2 \\sigma(X)^2\\) y \\(\\sigma(aX+b)=|a|\\cdot \\sigma(X)\\). Si \\(X,Y\\) son independientes, \\(\\sigma(X+Y)^2=\\sigma(X)^2+\\sigma(Y)^2\\). Si no, en principio no. Si \\(X_1,\\ldots,X_n\\) son variables aleatorias independientes y \\(a_1,\\ldots,a_n,b\\in \\mathbb{R}\\), \\[ \\begin{array}{l} \\sigma(a_1X_1+\\cdots+a_nX_n+b)^2=a_1^2\\cdot\\sigma(X_1)^2+\\cdots+a_n^2\\cdot\\sigma(X_n)^2\\\\ \\sigma(a_1X_1+\\cdots+a_nX_n+b)=\\sqrt{a_1^2\\cdot\\sigma(X_1)^2+\\cdots+a_n^2\\cdot\\sigma(X_n)^2} \\end{array} \\] Si no son independientes, estas igualdades pueden ser falsas. Dado \\(p\\) entre 0 y 1, el cuantil de orden \\(p\\) (o \\(p\\)-cuantil) de una variable aleatoria continua \\(X\\) es el menor valor \\(x_p\\in \\mathbb{R}\\) tal que \\[ F_X(x_p)=P(X\\leqslant x_p)=p \\] Fijaos en que como \\(F_X(x)\\) tiende a 0 (la probabilidad del conjunto vacío) cuando \\(x\\to -\\infty\\) y tiende a 1 (la probabilidad de todo \\(\\mathbb{R}\\)) cuando \\(x\\to +\\infty\\) y es continua, por el Teorema del Valor medio de las funciones continuas (que dice, básicamente, que las funciones continuas no dan saltos) toma todos los valores entre 0 y 1 y por lo tanto dado cualquier \\(p\\) entre 0 y 1 existe algún \\(x\\) tal que \\(F_X(x)=p\\). La mediana de \\(X\\) es su 0.5-cuantil, los primer y tercer cuartiles son su 0.25-cuantil y su 0.75-cuantil, etc. 1.3 Variables aleatorias normales Una variable aleatoria continua \\(X\\) es normal (o tiene distribución normal) de parámetros \\(\\mu\\) y \\(\\sigma\\) (es \\(N(\\mu,\\sigma)\\), para abreviar) cuando su función de densidad es \\[ f_{X}(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma} e^{{-(x-\\mu)^2}/{2\\sigma^{2}}} \\mbox{ para todo } x\\in \\mathbb{R} \\] Naturalmente, no os tenéis que saber esta fórmula. Pero sí que tenéis que saber que: Una variable aleatoria normal \\(X\\) es continua, y por lo tanto \\(P(X=x)=0\\), \\(P(X\\leqslant x)=P(X&lt;x)\\) etc. Si \\(X\\) es normal \\(N(\\mu,\\sigma)\\), su valor esperado es \\(E(X)=\\mu\\) y su desviación típica es \\(\\sigma_X=\\sigma\\). Si \\(X\\) es normal, su función de distribución \\(F_X\\) es inyectiva y creciente: si \\(x&lt;y\\), \\(F_X(x)&lt;F_X(y)\\). Una variable aleatoria normal es típica (o estándar) cuando es \\(N(0,1)\\). Usaremos normalmente \\(Z\\) para denotar una variable normal estándar. Si \\(Z\\) es una normal estándar, \\(E(Z)=0\\) y \\(\\sigma(Z)=1\\). La gráfica de la densidad de una variable aleatoria normal es la famosa campana de Gauss: La distribución normal es una distribución teórica, no la encontraréis exacta en la vida real. Y pese a su nombre, no es más “normal” que otras distribuciones continuas. Pero es muy importante, debido a que muchas distribuciones de la vida real son aproximadamente normales porque: Toda variable aleatoria que consista en tomar \\(n\\) medidas independientes de una o varias variables aleatorias y sumarlas, tiene distribución aproximadamente normal cuando \\(n\\) es muy grande, aunque las variables aleatorias de partida no sean normales. Ejemplo 1.1 Una variable binomial \\(B(n,p)\\) se obtiene tomando \\(n\\) medidas independientes de una variable Bernoulli \\(Be(p)\\) y sumando los resultados. Por lo tanto, por la “regla” anterior, una \\(B(n,p)\\) tendría que ser aproximadamente normal si \\(n\\) es grande. Pues sí, si \\(n\\) es grande (pongamos mayor que 40, aunque si \\(p\\) está muy cerca de 0 o 1 el tamaño de las muestras tiene que ser mayor), la distribución de una variable \\(X\\) binomial \\(B(n,p)\\) se acerca mucho a la de una normal \\(N(np,\\sqrt{np(1-p)})\\), donde, recordad que si \\(X\\) es \\(B(n,p)\\), entonces \\(\\mu_X=np\\) y \\(\\sigma_X=\\sqrt{np(1-p)}\\). Por ejemplo, el gráfico siguiente compara las funciones de distribución de una binomial \\(B(40,0.3)\\) y una normal \\(N(40\\cdot 0.3,\\sqrt{40\\cdot 0.3\\cdot 0.7})\\). En los próximos temas utilizaremos a menudo que una variable \\(B(n,p)\\) con \\(n\\) es grande es aproximadamente \\(N(np,\\sqrt{np(1-p)})\\). Para calcular probabilidades de una \\(N(\\mu,\\sigma)\\), hay que calcular las integrales a mano. O podéis usar R o alguna aplicación para móvil o tablet. Para R, la normal es norm. Así, por ejemplo, si \\(X\\) es \\(N(1,2)\\) \\(P(X\\leqslant 1.5)\\) es pnorm(1.5,1,2) ## [1] 0.5987063 El 0.4-cuantil de \\(X\\), es decir, el valor \\(q\\) tal que \\(P(X\\leqslant q)=0.4\\) es qnorm(0.4,1,2) ## [1] 0.4933058 \\(P(X=1.5)\\) es dnorm(1.5,1,2) ## [1] 0.1933341 ¡No! Como \\(X\\) es continua, \\(P(X=1.5)=0\\). Lo que os da dnorm(1.5,1,2) es el valor de la función de densidad de \\(X\\) en 1.5. Si la normal es estándar, no hace falta entrar la \\(\\mu=0\\) y la \\(\\sigma=1\\). Así, si \\(Z\\) es \\(N(0,1)\\) \\(P(Z\\leqslant 1.5)\\) es pnorm(1.5) ## [1] 0.9331928 pnorm(1.5,0,1) ## [1] 0.9331928 Su 0.95-cuantil es qnorm(0.95) ## [1] 1.644854 Ejemplo 1.2 La presión sistólica, medida en mm Hg, se distribuye como una variable normal con valor medio \\(\\mu\\) y desviación típica \\(\\sigma\\) que dependen del sexo y la edad. Para la franja de edad 16-24 años, estos valores son: Para hombres, \\(\\mu=124\\) y \\(\\sigma=13.7\\) Para mujeres, \\(\\mu=117\\) y \\(\\sigma=13.7\\) El modelo de hipertensión-hipotensión aceptado es el descrito en la Figura 1.2. Queremos calcular los límites de cada clase para cada sexo en este grupo de edad. Figura 1.2: Modelo de hipertensión-hipotensión. Veamos: El límite superior del grupo de hipotensión será el valor que deja a la izquierda un 5% de las tensiones: el 0.05-cuantil de la distribución. El límite superior del grupo de riesgo de hipotensión será el valor que deja a la izquierda un 10% de las tensiones: el 0.1-cuantil de la distribución. El límite inferior del grupo de riesgo de hipertensión será el valor que deja a la izquierda un 90% de las tensiones: el 0.9-cuantil de la distribución. El límite inferior del grupo de hipertensión será el valor que deja a la izquierda un 95% de las tensiones: el 0.95-cuantil de la distribución. En los hombres, la tensión sistólica es una variable aleatoria \\(N(124,13.7)\\). Podemos usar R o una aplicación para calcular estos cuantiles. Con R: El 0.05-cuantil es qnorm(0.05,124,13.7) ## [1] 101.4655 El 0.1-cuantil es qnorm(0.1,124,13.7) ## [1] 106.4427 El 0.9-cuantil es qnorm(0.9,124,13.7) ## [1] 141.5573 El 0.95-cuantil es qnorm(0.95,124,13.7) ## [1] 146.5345 En resumen, para los hombres de 16 a 24 años: \\[ \\begin{array}{|ll|} \\hline \\text{Grupo} &amp; \\text{Intervalo}\\\\ \\hline \\text{Hipotenso} &amp; &lt;101.5\\\\ \\text{Prehipotenso} &amp; 101.5\\text{ a }106.4\\\\ \\text{Normotenso} &amp; 106.4\\text{ a }141.6\\\\ \\text{Prehipertenso} &amp; 141.6\\text{ a }146.5\\\\ \\text{Hipertenso} &amp; &gt; 146.5\\\\ \\hline \\end{array} \\] Calculad estos límites para las mujeres de 16 a 24 años. 1.3.1 Propiedades básicas Una de las propiedades clave de la distribución normal es su simetría: Si \\(X\\) es \\(N(\\mu,\\sigma)\\), su densidad \\(f_X\\) es simétrica respecto de \\(\\mu\\), es decir, \\[ f_{X}(\\mu-x)=f_{X}(\\mu+x), \\] y tiene el máximo en \\(x=\\mu\\). Decimos entonces que \\(\\mu\\) es la moda de \\(X\\). Recordad que no tiene sentido definir la moda de una variable continua \\(X\\) como el valor \\(x_0\\) tal que \\(P(X=x_0)\\) sea máximo, porque \\(P(X=x)=0\\) para todo \\(x\\in \\mathbb{R}\\). Se define entonces la moda de una variable continua \\(X\\) como el valor (o los valores) \\(x_0\\) tal que \\(f_X(x_0)\\) es máximo. Por lo tanto, como \\(f_X(x_0)\\) mide la probabilidad de que \\(X\\) valga aproximadamente \\(x_0\\), tenemos que la moda de \\(X\\) es el valor cerca del cual es más probable que caiga el valor de \\(X\\). En particular, si \\(Z\\) es \\(N(0,1)\\), entonces \\(f_Z\\) es simétrica alrededor de 0, es decir, \\(f_{Z}(-x)=f_{Z}(x)\\), y la moda de \\(Z\\) es \\(x=0\\). Recordad que la función de distribución de una variable aleatoria continua \\(X\\), \\[ F_X(x)=P(X\\leqslant x) \\] es el área comprendida entre la densidad \\(y=f_X(x)\\) y el eje de abscisas a la izquierda de \\(x\\). Entonces, la simetría de \\(f_X\\) hace que, para todo \\(x\\geqslant 0\\), las áreas a la izquierda de \\(\\mu-x\\) y a la derecha de \\(\\mu+x\\) sean iguales. Es decir, \\[ P(X\\leqslant \\mu-x)=P(X\\geqslant \\mu+x)=1-P(X\\leqslant \\mu+x) \\] En particular (tomando \\(x=0\\)) \\[ P(X\\leqslant \\mu)=1-P(X\\leqslant \\mu)\\Rightarrow P(X\\leqslant \\mu)=0.5 \\] y por lo tanto, \\(\\mu\\) es también la mediana de \\(X\\). Si \\(X\\) es \\(N(\\mu,\\sigma)\\), \\(\\mu\\) es la media, la mediana y la moda de \\(X\\). En el caso concreto de la normal estándar \\(Z\\), para cualquier \\(z\\geqslant 0\\) se tiene que las áreas a la izquierda de \\(-z\\) y a la derecha de \\(z\\) son iguales \\[ P(Z\\leqslant -z)=P(Z\\geqslant z)=1-P(Z\\leqslant z) \\] y la mediana de \\(Z\\) es 0. Ahora que sabemos más cosas de la normal, en el Ejemplo 1.2 nos hubiéramos podido ahorrar la mitad del trabajo. Llamemos \\(X\\) a la variable aleatoria que nos da la presión arterial, en mm Hg, de un hombre de entre 16 y 24 años. Nos dicen que \\(X\\) es \\(N(124,13.7)\\). Por la simetría de \\(X\\) alrededor de \\(\\mu=124\\), si escribimos el 0.05-cuantil como \\(124-x\\), entonces \\(P(X\\geqslant 124+x)=P(X\\leqslant 124-x)=0.05\\) y por lo tanto \\(P(X\\leqslant 124+x)=1-P(X\\geqslant 124+x)=0.95\\), es decir, \\(124+x\\) será el 0.95-cuantil de \\(X\\). El 0.05-cuantil ha sido 101.5. Escribiendo \\(101.5=124-x\\), obtenemos \\(x=22.5\\). Por lo tanto, el 0.95-cuantil tiene que ser \\(124+22.5=146.5\\). Lo mismo pasa con el 0.9-cuantil y el 0.1-cuantil, razonadlo y comprobadlo. El argumento que hemos desarrollado en la nota anterior muestra en general que si \\(X\\) es \\(N(\\mu,\\sigma)\\) y su \\(q\\)-cuantil es \\(\\mu-x\\), entonces su \\((1-q)\\)-cuantil es \\(\\mu+x\\). Figura 1.3: Quantils gratis! Si \\(\\mu\\) crece, desplaza a la derecha el máximo de la densidad, y con él toda la curva. Si \\(\\sigma\\) crece, la curva se aplana: al aumentar la desviación típica, los valores se dispersan y se alejan más del valor medio. El gráfico siguiente muestra el efecto combinado: Denotaremos por \\(z_q\\) el \\(q\\)-cuantil de una variable normal estándar \\(Z\\). Es decir, \\(z_q\\) es el valor tal que \\(P(Z\\leqslant z_q)=q\\). Aparte de que \\(z_{0.5}=0\\) (la mediana de \\(Z\\) es 0), hay dos cuantiles más de la normal estándar \\(Z\\) que os conviene recordar: \\(z_{0.95}=1.64\\); es decir, \\(P(Z\\leqslant 1.64)=0.95\\) y por lo tanto \\(P(Z\\leqslant -1.64)=P(Z\\geqslant 1.64)=0.05\\) y \\[ P(-1.64\\leqslant Z\\leqslant 1.64)=0.9. \\] \\(z_{0.975}=1.96\\); es decir, \\(P(Z\\leqslant 1.96)=0.975\\) y por lo tanto \\(P(Z\\leqslant -1.96)=P(Z\\geqslant 1.96)=0.025\\) y \\[ P(-1.96\\leqslant Z\\leqslant 1.96)=0.95. \\] Muy a menudo el valor 1.96 de \\(z_{0.975}\\) se aproxima por 2. Tenéis permiso para hacerlo cuando no dispongáis de medios (R, aplis de móvil) para calcular cuantiles o cuando tengáis que hacer algún cálculo “a ojo” que involucre este cuantil. Ejemplo 1.3 Supongamos que la concentración de un cierto metabolito es una variable aleatoria de distribución normal, pero cuyos parámetros \\(\\mu\\) y \\(\\sigma\\) dependen de si la medimos en personas sanas o en personas con una cierta enfermedad. Sean: \\(X_E\\) la variable aleatoria “Tomo una persona enferma y mido su concentración de este metabolito”, y supongamos que es \\(N(\\mu_E, \\sigma_E)\\). \\(X_S\\) la variable aleatoria “Tomo una persona sana y mido su concentración de este metabolito”, y supongamos que es \\(N(\\mu_S, \\sigma_S)\\). Supongamos, para fijar ideas, que \\(\\mu_E&gt;\\mu_S\\): la concentración media de este metabolito en los enfermos es más alta que en las personas sanas. Podríamos usar como prueba diagnóstica de la enfermedad la concentración del metabolito. Para cada valor de referencia \\(x_0\\), nuestra prueba dará: Positivo, si la concentración es mayor o igual que \\(x_0\\). Negativo, si la concentración es menor que \\(x_0\\). Entonces: La sensibilidad de esta prueba es \\[ P(+|E) =P(X_E\\geqslant x_0)=1-P(X_E&lt; x_0)=1-F_{X_E}(x_0) \\] Su especificidad es \\[ P(-|S)=P(X_S&lt; x_0)=F_{X_S}(x_0) \\] Su tasa de falsos positivos es \\[ P(+|S)=P(X_S\\geqslant x_0)=1-F_{X_S}(x_0) \\] Al variar \\(x_0\\), tenemos valores diferentes de la sensibilidad y la tasa de falsos positivos. Entonces, podemos dibujar su curva ROC y escoger el umbral con algún criterio o valorar su capacidad diagnóstica global con su AUC. Por ejemplo, imaginad que la densidad de \\(X_E\\) es la línea discontinua del gráfico de la izquierda de la figura siguiente y la de \\(X_S\\) la línea continua. Ambas son normales y \\(\\mu_E&gt;\\mu_S\\), porque el pico de la densidad de \\(X_E\\) está a la derecha del de \\(X_S\\). Si para cada \\(x\\) dibujamos los puntos \\((1-F_{X_S}(x),1-F_{X_E}(x))\\), obtenemos la curva ROC de la derecha de dicha figura. Una de las propiedades de la distribución normal que nos facilitan mucho la vida es que toda combinación lineal de variables aleatorias normales independientes es normal. En concreto, tenemos los resultados siguientes: Teorema 1.2 Sea \\(X\\) una variable \\(N(\\mu,\\sigma)\\). Para todos \\(a,b\\in \\mathbb{R}\\), \\(aX+b\\) es \\(N(a\\mu+b,|a|\\cdot\\sigma)\\). En particular, la tipificada de \\(X\\) \\[ Z=\\dfrac{X-\\mu}{\\sigma} \\] es \\(N(0,1)\\). Más en general: Teorema 1.3 Si \\(X_1,\\ldots,X_n\\) son variables aleatorias normales independientes, cada \\(X_i\\) de tipo \\(N(\\mu_i,\\sigma_i)\\), y \\(a_1,\\ldots,a_n,b\\in \\mathbb{R}\\), entonces \\(a_1X_1+\\cdots +a_nX_n+b\\) es \\(N(\\mu,\\sigma)\\) con \\[ \\mu=a_1\\mu_1+\\cdots +a_n\\mu_n+b,\\ \\sigma=\\sqrt{a_1^2\\sigma^2_1+\\cdots +a_n^2\\sigma^2_n} \\] Que toda combinación lineal de variables normales vuelva a ser del mismo tipo, es decir, normal, es una propiedad muy útil de las variables normales que pocas familias de distribuciones comparten. Por ejemplo, si \\(X\\) es una variable binomial \\(B(n,p)\\) con \\(p\\neq 0\\), la variable \\(2X\\) no es binomial, porque solo toma valores pares, mientras que una variable binomial \\(B(m,q)\\) ha de poder tomar todos los valores entre 0 y \\(m\\). Las probabilidades de la normal tipificada determinan las de la normal original, porque si \\(X\\) es \\(N(\\mu,\\sigma)\\): \\[ \\begin{array}{rl} P(a\\leqslant X\\leqslant b)\\!\\!\\!\\!\\! &amp; \\displaystyle =P\\Big( \\frac{a-\\mu}{\\sigma}\\leqslant \\frac{X-\\mu}{\\sigma}\\leqslant \\frac{b-\\mu}{\\sigma}\\Big)\\\\ &amp; \\displaystyle =P\\Big(\\frac{a-\\mu}{\\sigma}\\leqslant Z\\leqslant \\frac{b-\\mu}{\\sigma}\\Big) \\end{array} \\] Esto sirve para deducir fórmulas, y vuestros padres lo usaban para calcular probabilidades (con tablas de probabilidades de la normal estándar); ahora es más cómodo usar una aplicación del móvil. 1.3.2 Intervalos de referencia Un intervalo de referencia del \\(Q\\%\\) para una variable aleatoria \\(X\\) es un intervalo \\([a,b]\\) tal que \\[ P(a\\leqslant X\\leqslant b)=Q/100. \\] Es decir, un intervalo de referencia del \\(Q\\%\\) para \\(X\\) es un intervalo que contiene los valores de \\(X\\) del \\(Q\\%\\) de los sujetos de la población. Por ejemplo, hemos visto en la sección anterior que [-1.64,1.64] y [-1.96,1.96] son intervalos de referencia del 90% y del 95%, respectivamente, para una variable normal estándar \\(Z\\). Y en el Ejemplo 1.2 hemos visto que un intervalo de referencia del 90% para la presión sistólica de los hombres de 16 a 24 años, medida en mm Hg, es [101.5,146.5]. Los más comunes son los intervalos de referencia del 95%, que satisfacen que \\[ P(a\\leqslant X\\leqslant b)=0.95 \\] y son los, que por ejemplo, os dan como valores de referencia en las analíticas: Cuando se habla de un intervalo de referencia sin dar la probabilidad, se sobreentiende siempre que es el intervalo de referencia del 95%. Un 5% de la población cae fuera del intervalo de referencia del 95%. Cuando \\(X\\) es \\(N(\\mu,\\sigma)\\), estos intervalos de referencia se toman siempre centrados en la media \\(\\mu\\), es decir, de la forma \\([\\mu-\\text{algo},\\mu+\\text{algo}]\\). Para calcularlos se usa el resultado siguiente: Teorema 1.4 Si \\(X\\) es \\(N(\\mu,\\sigma)\\), un intervalo de referencia del \\(Q\\%\\) para \\(X\\) es \\[ [\\mu- z_{(1+q)/2}\\cdot \\sigma, \\mu+ z_{(1+q)/2}\\cdot \\sigma] \\] donde \\(q=Q/100\\) y \\(z_{(1+q)/2}\\) denota el \\((1+q)/2\\)-cuantil de la normal estándar \\(Z\\). Se suele escribir \\[ \\mu\\pm z_{(1+q)/2}\\cdot \\sigma. \\] En efecto: \\[ \\begin{array}{l} P(\\mu-x\\leqslant X\\leqslant \\mu+x)=q\\\\ \\qquad \\Longleftrightarrow \\displaystyle P\\Big(\\frac{\\mu-x-\\mu}{\\sigma}\\leqslant \\frac{X-\\mu}{\\sigma}\\leqslant \\frac{\\mu+x-\\mu}{\\sigma}\\Big)=q\\\\ \\qquad \\Longleftrightarrow \\displaystyle P(-x/{\\sigma}\\leqslant Z\\leqslant {x}/{\\sigma})=q\\\\ \\qquad \\Longleftrightarrow \\displaystyle P(Z\\leqslant {x}/{\\sigma})-P(Z\\leqslant -{x}/{\\sigma})=q\\\\ \\qquad \\Longleftrightarrow \\displaystyle P(Z\\leqslant {x}/{\\sigma})-P(Z\\geqslant {x}/{\\sigma})=q\\\\ \\qquad \\text{(por la simetría de $f_Z$ alrededor de 0)}\\\\ \\qquad \\Longleftrightarrow \\displaystyle P(Z\\leqslant {x}/{\\sigma})-(1-P(Z\\leqslant {x}/{\\sigma}))=q\\\\ \\qquad \\Longleftrightarrow \\displaystyle 2P(Z\\leqslant {x}/{\\sigma})=q+1\\\\ \\qquad \\Longleftrightarrow P(Z\\leqslant {x}/{\\sigma})=(1+q)/2\\\\ \\qquad \\Longleftrightarrow x/\\sigma= z_{(1+q)/2}\\\\ \\qquad \\Longleftrightarrow x=z_{(1+q)/2}\\cdot \\sigma \\end{array} \\] Si \\(q=0.95\\), entonces \\((1+q)/2=0.975\\) y \\(z_{0.975}=1.96\\). Por lo tanto, el intervalo de referencia del 95% para una variable \\(X\\) normal \\(N(\\mu,\\sigma)\\) es \\[ \\mu\\pm 1.96\\sigma. \\] Y como este 1.96 a menudo se aproxima por 2, el intervalo de referencia del 95% se simplifica a \\[ \\mu\\pm 2\\sigma. \\] Esto dice, básicamente, que si una población sigue una distribución normal \\(N(\\mu,\\sigma)\\), un 95% de sus individuos tienen su valor de \\(X\\) a distancia como máximo \\(2\\sigma\\) (“a dos sigmas”) de \\(\\mu\\). Ejemplo 1.4 Según la OMS, las alturas (en cm) de las mujeres europeas de 18 años siguen una ley \\(N(163.1,18.53)\\). ¿Cuál es el intervalo de alturas centrado en la media que contiene a la mitad las europeas de 18 años? Fijaos en que, si llamamos \\(X\\) a la variable aleatoria “Tomo una mujer europea de 18 años y mido su altura en cm”, lo que queremos saber es el intervalo centrado en su media, 163.1, tal que la probabilidad de que la altura de una europea de 18 años escogida al azar pertenezca a este intervalo sea 0.5. Es decir, el intervalo de referencia del 50% para \\(X\\). Nos dicen que \\(X\\) es \\(N(163.1,18.53)\\). Si \\(q=0.5\\), entonces \\((1+q)/2=0.75\\) y podemos calcular con R o una aplicación el 0.75-cuantil de una normal estándar. Da \\(z_{0.75}=0.6745\\). Por lo tanto, es el intervalo \\(163.1\\pm 0.6745\\cdot 18.53\\). Redondeando a mm, \\([150.6, 175.6]\\). Esto nos dice que la mitad de las mujeres europeas de 18 años miden entre 150.6 y 175.6. El z-score (z-valor, z-puntuación, z-puntaje…) de un valor \\(x_0\\in \\mathbb{R}\\) respecto de una distribución \\(N(\\mu,\\sigma)\\) es \\[ \\frac{x_0-\\mu}{\\sigma} \\] Es decir, el z-score de \\(x_0\\) es el resultado de “tipificar” \\(x_0\\) en el sentido del Teorema 1.2.2. Si la variable poblacional es normal, cuanto mayor es el valor absoluto del z-score de \\(x_0\\), más “raro” es \\(x_0\\); el signo nos dice si es más grande o más pequeño que el valor esperado \\(\\mu\\). Ejemplo 1.5 Recordad que, según la OMS, las alturas de las mujeres europeas de 18 años siguen una ley \\(N(163.1,18.53)\\). ¿Cuál sería el z-score de una jugadora de baloncesto de 18 años que midiera 191 cm? Sería \\[ \\frac{191-163.1}{18.53}=1.5 \\] Esto se suele leer diciendo que la altura de esta jugadora está 1.5 sigmas por encima de la media. 1.4 Test (1) Sea \\(X\\) una variable aleatoria continua de función de densidad: \\[ f_X(x)=\\left\\{\\begin{array}{ll} 0 &amp; \\mbox{si $x&lt;0$}\\\\ \\frac{2\\sqrt{2}}{\\sqrt{\\pi}} e^{-2x^2} &amp; \\mbox{si $x\\geqslant 0$} \\end{array} \\right. \\] ¿Es cierto que \\(P(X=1)=2\\sqrt{2}e^{-2}/\\sqrt{\\pi}\\)? Sí No: en realidad \\(P(X=1)=\\int_{-\\infty}^1 \\frac{2\\sqrt{2}}{\\sqrt{\\pi}} e^{-2x^2}\\,dx\\) pero no sé calcular esta integral, o sí sé calcularla, pero me da pereza hacerlo. Esto no es la función de densidad de una variable aleatoria continua, porque no es una función continua (en el 0 salta de 0 a \\(2\\sqrt{2}/\\sqrt{\\pi}\\)) Todas las otras respuestas son incorrectas (2) \\(X\\) una variable aleatoria continua de media \\(\\mu\\). ¿Qué vale \\(P(X=\\mu)\\)? 0.5 \\(\\mu\\) 0 Depende de la variable aleatoria Todas las otras respuestas son falsas (3) \\(X\\) una variable aleatoria continua de moda \\(M\\). ¿Qué vale \\(P(X=M)\\)? 1 0.5 0 Depende de la variable aleatoria, pero es estrictamente mayor que cualquier otro valor de \\(P(X=x)\\) Depende de la variable aleatoria, pero es el valor máximo de la función de densidad de \\(X\\). Todas las otras respuestas son falsas (4) En una variable aleatoria discreta, su función de densidad (marca una única respuesta): Es tal que su integral desde \\(-\\infty\\) es la función de distribución. Mide lo denso que es su dominio. Aplicada a un par de números reales, nos da la probabilidad de obtener valores dentro del intervalo definido por dichos números. Aplicada a un número real, nos da da la probabilidad de obtener dicho número. Aplicada a un número real, nos da la probabilidad de obtener un valor menor o igual que dicho número. (5) Sea \\(Z\\) una variable aleatoria normal estándar. Marca las afirmaciones verdaderas. Es asimétrica a la izquierda. Su media es 1. Su desviación típica es 0. Su varianza es 1. Su mediana es 0. (6) Sea \\(X\\) una variable aleatoria \\(N(\\mu,\\sigma)\\) y \\(f_X\\) su función de densidad. ¿Qué vale el área entre la curva \\(y=f_X(x)\\) y el eje de abscisas? 0 \\(\\mu\\) \\(\\sigma\\) 1 Todas las otras respuestas son falsas (7) Sea \\(X\\) una variable aleatoria \\(N(\\mu,\\sigma)\\) y \\(f_X\\) su función de densidad. ¿Cuál de las afirmaciones siguientes es correcta? \\(\\mu\\) es la media de \\(X\\), pero no su mediana \\(\\mu\\) es la media y la mediana de \\(X\\), pero no su moda \\(\\mu\\) es la media, la mediana y la moda de \\(X\\), pero no es verdad que \\(P(X=\\mu)&gt;P(X=a)\\) para todo \\(a\\neq \\mu\\) \\(\\mu\\) es la media, la mediana y la moda de \\(X\\) y \\(P(X=\\mu)&gt;P(X=a)\\) para todo \\(a\\neq \\mu\\) (8) ¿Qué distribución es la más adecuada para modelar el número anual de fallecimientos entre enfermos de cáncer tratados con una determinada quimioterapia? Marca una única respuesta. Normal Binomial Poisson Uniforme acotada (todos los números de fallecimientos entre 0 y el número \\(N\\) de enfermos de cáncer tratados con esta quimioterapia tienen la misma probabilidad) (9) El FME (Flujo Máximo de Espiración) de las chicas de 11 años sigue una distribución aproximadamente normal de media 300 l/min y desviación típica 20 l/min. Marca las afirmaciones verdaderas: Aproximadamente la mitad de las chicas de 11 años tienen un FME entre 280 l/min y 320 l/min. Alrededor del 95% de las chicas de 11 años tienen un FME entre 280 l/min y 320 l/min. Alrededor del 95% de las chicas de 11 años tienen un FME entre 260 l/min y 340 l/min. Alrededor del 5% de las chicas de 11 años tienen un FME inferior a 260 l/min. Ninguna chica de 11 años tiene FME superior a 360 l/min. (10) En una muestra aleatoria extraída de población sana se encuentra que una variable bioquímica tiene media 90 y desviación típica 10. Si tomamos una muestra de individuos sanos ¿es razonable esperar que aproximadamente el 95% de ellos tengan un valor de esa variable comprendido entre 70 y 110? (marca todas las respuestas correctas): Sí, siempre. No, nunca. Si la variable tiene distribución normal, entonces sí. Si la muestra es muy grande, entonces sí. Si la variable tiene distribución normal y la muestra es muy grande, entonces sí. "],["estimadores.html", "Lección 2 Estimadores 2.1 La media muestral 2.2 La proporción muestral 2.3 La varianza muestral 2.4 La distribución t de Student 2.5 Test", " Lección 2 Estimadores En un problema típico de estadística inferencial: Queremos conocer el valor de una característica en el total de una población, pero no podemos medir esta característica en todos los individuos de la población. Entonces, extraemos una muestra de la población, medimos la característica en los individuos de esta muestra, calculamos algo con los datos obtenidos e inferimos el valor de la característica en el global de la población. Inmediatamente surgen varias preguntas, que responderemos entre esta lección y la próxima: ¿Cómo tiene que ser la muestra? ¿Qué tenemos que calcular? ¿Con qué precisión podemos inferir la característica de la población? ¿Qué tipo de muestra tenemos que tomar? Vamos a suponer de ahora en adelante que tomamos muestras aleatorias simples. Esto incluye las muestras aleatorias sin reposición si la población es mucho más grande que la muestra, ya que entonces no hay diferencia práctica entre permitir y prohibir las repeticiones. En algunos casos muy concretos permitiremos muestras aleatorias sin reposición en general. Sí, ya sabemos que en la práctica casi nunca tomamos muestras aleatorias, sino oportunistas. En este caso, recordad lo que os explicábamos en la Sección ??. Lo que hay que hacer es describir en detalle las características de la muestra para justificar que, pese a no ser aleatoria, es razonablemente representativa de la población y podría pasar por aleatoria. ¿Qué calculamos? Pues un estimador: alguna función adecuada aplicada a los valores de la muestra, y que dependerá de lo que queramos estimar. Por ejemplo: Si queremos estimar la altura media de los estudiantes de la UIB, tomaremos una muestra aleatoria de estudiantes de la UIB, mediremos sus alturas y calcularemos su media aritmética. Si queremos estimar la proporción de estudiantes de la UIB que han pasado la COVID-19, tomaremos una muestra aleatoria de estudiantes de la UIB, les haremos un test de anticuerpos y calcularemos la proporción muestral de positivos en la muestra. Si queremos estimar el riesgo relativo para un estudiante de la UIB de suspender alguna asignatura si es fumador, tomaremos una muestra aleatoria de estudiantes de la UIB, anotaremos si fuman o no y si han suspendido alguna asignatura o no, y calcularemos el cociente entre las proporciones muestrales de suspensos entre los fumadores y los no fumadores de la muestra. Un estimador es una variable aleatoria, definida sobre la población formada por las muestras de la población de partida. Por lo tanto, tiene función de densidad, función de distribución, esperanza, desviación típica, etc. 2.1 La media muestral Cuando queremos estimar el valor medio de una variable sobre una población, tomamos una muestra de valores y calculamos su media aritmética, ¿verdad? Pues eso es la media muestral. Dada una variable aleatoria \\(X\\), llamamos media muestral de (muestras de) tamaño \\(n\\) a la variable aleatoria \\(\\overline{X}\\) “Tomamos una muestra aleatoria simple de tamaño \\(n\\) de \\(X\\) y calculamos la media aritmética de sus valores”. Fijaos en que definimos la media muestral solo para muestras aleatorias simples. Naturalmente, tiene sentido definirla para muestras cualesquiera, pero entonces su distribución no cumple las propiedades que damos en esta sección. La misma advertencia vale para los estimadores que definimos en las próximas secciones. Veamos algunas propiedades de la distribución de \\(\\overline{X}\\): Teorema 2.1 Sea \\(X\\) una variable aleatoria cualquiera de media \\(\\mu_X\\) y desviación típica \\(\\sigma_X\\), y sea \\(\\overline{X}\\) la media muestral de tamaño \\(n\\) de \\(X\\). Entonces: \\(E(\\overline{X})=\\mu_X\\) \\(\\sigma(\\overline{X})=\\dfrac{\\sigma_X}{\\sqrt{n}}\\) Formalmente, la media muestral de tamaño \\(n\\) de una variable aleatoria \\(X\\) se define como la variable aleatoria \\[ \\overline{X}=\\frac{X_1+\\cdots+X_n}{n} \\] donde \\(X_1,\\ldots,X_n\\) son \\(n\\) copias independientes de la variable \\(X\\). Entonces, por la linealidad de la esperanza \\[ E(\\overline{X})=\\frac{E(X_1)+\\cdots+E(X_n)}{n}=\\frac{n\\cdot \\mu_X}{n}=\\mu_X \\] porque, como \\(X_1,\\ldots,X_n\\) son copias de \\(X\\), \\(E(X_1)=\\cdots=E(X_n)=\\mu_X\\). Y por la “linealidad” de la varianza de la suma de variables independientes \\[ \\sigma(\\overline{X})^2=\\frac{\\sigma(X_1)^2+\\cdots+\\sigma(X_n)^2}{n^2}=\\frac{n\\cdot \\sigma_X^2}{n^2}=\\frac{\\sigma_X^2}{n} \\] porque, de nuevo, como \\(X_1,\\ldots,X_n\\) son copias de \\(X\\), \\(\\sigma(X_1)^2=\\cdots=\\sigma(X_n)^2=\\sigma_X^2\\). Que \\(E(\\overline{X})\\) sea \\(\\mu_X\\) nos indica que \\(\\overline{X}\\) sirve para estimar \\(\\mu_X\\), porque su valor esperado es \\(\\mu_X\\): Si calculáramos muchas medias de muestras aleatorias de \\(X\\), es muy probable que, de media, obtuviéramos un valor muy cercano a \\(\\mu_X\\). Cuando el valor esperado de un estimador es precisamente el parámetro poblacional que se quiere estimar, se dice que el estimador es insesgado. Así, el primer punto del teorema anterior dice que la media muestral \\(\\overline{X}\\) es un estimador insesgado de la media poblacional \\(\\mu_X\\). Que \\(\\sigma(\\overline{X})\\) sea \\(\\sigma_X/\\sqrt{n}\\) implica que la variabilidad de las medias muestrales crece con la variabilidad de \\(X\\) y decrece si tomamos muestras de mayor tamaño. Esto último es razonable. Aunque la variabilidad de \\(X\\) sea grande, si tomamos muestras grandes, es de esperar que los valores extremos se compensen al calcular sus medias y que estas últimas tengan por lo tanto menos variabilidad que la variable \\(X\\) original. A \\(\\sigma_X/\\sqrt{n}\\) se le llama el error típico de la media muestral (para la variable aleatoria \\(X\\) y muestras de tamaño \\(n\\)). Ejemplo 2.1 Vamos a realizar un experimento. Vamos a tomar una población de 106 sujetos y una variable aleatoria \\(X\\) que sobre cada sujeto toma un valor real entre 0 y 1, todos estos valores con la misma probabilidad. Llamaremos X al vector con los 106 valores de esta variable aleatoria, y dibujaremos un histograma de este vector de números para que veáis que los valores salen muy dispersos. Mostramos el código de R para que podáis repetir el experimento por vuestra cuenta; como es una simulación, cada vez que lo ejecutéis dará resultados diferentes, pero el mismo efecto global. X=runif(10^6) hist(X,freq=FALSE,main=&quot;Histograma de X&quot;,xlab=&quot;&quot;,ylab=&quot;Densidad&quot;,col=&quot;light blue&quot;) La desviación típica \\(\\sigma_X\\) de la variable \\(X\\) sobre nuestra población es sd(X)*sqrt((10^6-1)/10^6) ## [1] 0.2884943 La función sd calcula la desviación típica muestral, con denominador \\(\\sqrt{n-1}\\). Para calcular la desviación típica de verdad, con denominador \\(\\sqrt{n}\\) hay que multiplicarla por \\(\\sqrt{n-1}\\) y dividirla por \\(\\sqrt{n}\\). Ahora vamos a tomar 1000 medias muestrales de tamaño 100 de esta población, las organizaremos en un vector que llamaremos Medias y dibujaremos un histograma de este vector de medias. Medias=replicate(1000,mean(sample(X,100,replace=TRUE))) hist(Medias, breaks=15,freq=FALSE,main=&quot;Histograma de las medias muestrales&quot;,xlab=&quot;&quot;,ylab=&quot;Densidad&quot;,col=&quot;light blue&quot;,xlim=c(0.4,0.6)) Podéis observar cómo los valores de estas medias se concentran alrededor de 0.5. Veamos su desviación típica: sd(Medias)*sqrt((1000-1)/1000) ## [1] 0.02921408 Fijaos cómo se acerca mucho al valor \\(\\sigma_X/\\sqrt{100}=0.0288494\\) predicho por el teorema anterior. No coinciden exactamente, porque \\(\\sigma_X/\\sqrt{100}\\) es el valor de la desviación típica poblacional de \\(\\overline{X}\\), es decir, para toda la “población” de medias muestrales de muestras aleatorias simples de tamaño 100 de nuestra población de partida, y nosotros hemos tomado una muestra de “solo” 1000 medias de estas. La media muestral \\(\\overline{X}\\) de tamaño \\(n\\) de una variable aleatoria \\(X\\) se interpreta formalmente como la variable aleatoria obtenida tomando \\(n\\) copias independientes \\(X_1,\\ldots,X_n\\) de \\(X\\) y calculando \\[ \\overline{X}=\\frac{X_1+\\cdots+X_n}{n}. \\] Por lo tanto, es una combinación lineal de \\(n\\) copias independientes de \\(X\\). Recordando que una combinación de variables aleatorias normales independientes es normal, tenemos el resultado siguiente: Teorema 2.2 Si \\(X\\) es \\(N(\\mu_X,\\sigma_X)\\), entonces \\(\\overline{X}\\) es \\(N(\\mu_X,\\sigma_X/\\sqrt{n})\\). Si \\(X\\) no es normal, la tesis del teorema anterior sigue siendo cierta “aproximadamente” siempre y cuando \\(n\\) sea grande. Este resultado, llamado el Teorema Central del Límite es, como su nombre indica, uno de los más importantes en estadística y el motivo de la importancia de la distribución normal. Teorema 2.3 Sea \\(X\\) una variable aleatoria cualquiera de esperanza \\(\\mu_X\\) y desviación típica \\(\\sigma_X\\). Si \\(n\\) es muy grande, \\(\\overline{X}\\) es aproximadamente \\(N(\\mu_X, {\\sigma_X}/{\\sqrt{n}})\\). Dos observaciones: ¿Cuándo es una muestra lo bastante grande como para poder invocar el Teorema Central del Límite? En realidad, depende de la \\(X\\). Cuánto más se parezca \\(X\\) a una variable normal, más pequeñas pueden ser la muestras. Por fijar un valor, aceptaremos que “muy grande” en este teorema es \\(n\\geqslant 40\\). ¿Qué quiere decir que una variable aleatoria sea “aproximadamente” normal? Pues que su función de distribución \\(F_X\\) toma valores muy cercanos a la función de distribución de una normal. Recordad cómo una \\(B(n,p)\\) con \\(n\\) grande era “aproximadamente normal” en la lección anterior. Si miráis el histograma de las 1000 medias muestrales del Ejemplo 2.1, veréis que se parece al de una muestra de una variable normal. Es que \\(\\overline{X}\\) es aproximadamente normal, por el Teorema Central del Límite, aunque la variable \\(X\\) sea muy diferente de una normal. Para verlo, en la figura que sigue superponemos al histograma de las medias la gráfica de la densidad de una variable normal de media y desviación típica las predichas por el Teorema Central del Límite. En resumen: Si \\(X\\) es normal, \\(\\overline{X}\\) es \\(N(\\mu_X,{\\sigma_X}/{\\sqrt{n}})\\). Si \\(X\\) no es normal pero \\(n\\) es grande (pongamos \\(n\\geqslant 40\\), aunque puede ser menor si \\(X\\) se parece a una normal y seguramente tendrá que ser mayor si \\(X\\) es muy diferente de una normal), \\(\\overline{X}\\) es aproximadamente \\(N(\\mu_X,{\\sigma_X}/{\\sqrt{n}})\\). Las afirmaciones del bloque anterior son verdaderas para medias muestrales de muestras aleatorias simples. Si no podemos suponer que la muestra sea aleatoria simple, ninguno de los dos resultados es válido. 2.2 La proporción muestral Cuando queremos estimar la proporción de sujetos de una población que tienen una determinada característica, tomamos una muestra y calculamos la proporción de sujetos de la muestra con esta característica. Esta será la proporción muestral de sujetos con esta característica en nuestra muestra. Dada una variable aleatoria \\(X\\) de Bernoulli \\(Be(p_X)\\), la proporción muestral de (muestras de) tamaño \\(n\\), \\(\\widehat{p}_X\\), es la variable aleatoria consistente en tomar una muestra aleatoria simple de tamaño \\(n\\) de \\(X\\) y calcular la proporción de éxitos en la muestra: es decir, contar el número total de éxitos y dividir el resultado por \\(n\\). Fijaos en que \\(\\widehat{p}_X\\) es un caso particular de media muestral \\(\\overline{X}\\): estamos calculando medias muestrales de muestras aleatorias simples de la variable de Bernoulli \\(X\\). Por lo tanto, todo lo que hemos dicho para medias muestrales vale también para proporciones muestrales: Teorema 2.4 Si \\(X\\) es una variable aleatoria de Bernoulli con probabilidad poblacional de éxito \\(p_X\\) y \\(\\widehat{p}_X\\) es la proporción muestral de tamaño \\(n\\): \\(E(\\widehat{p}_X)=p_X\\) \\(\\sigma({\\widehat{p}_X})=\\sqrt{\\dfrac{p_X(1-p_X)}{n}}\\) No hace falta invocar que la proporción muestral sea un caso particular de media muestral para obtener el resultado anterior. Si llamamos \\(Y\\) a la variable que cuenta el número de éxitos en una muestra aleatoria simple de tamaño \\(n\\) de la variable de Bernoulli \\(X\\), entonces, por un lado, tenemos que \\(\\widehat{p}_X=Y/n\\) y, por otro, que \\(Y\\) es \\(B(n,p_X)\\). Entonces: \\(E(\\widehat{p}_X)=E\\Big(\\dfrac{1}{n}Y\\Big)=\\dfrac{1}{n}\\cdot E(Y)=\\dfrac{1}{n}\\cdot np_X=p_X\\) \\(\\sigma({\\widehat{p}_X})=\\sigma\\Big(\\dfrac{1}{n}Y\\Big)=\\dfrac{1}{n}\\cdot \\sigma(Y)=\\dfrac{1}{n} \\sqrt{np_X(1-p_X)}=\\sqrt{\\dfrac{p_X(1-p_X)}{n}}\\) \\(E(\\widehat{p}_X)=p_X\\) nos dice que \\(\\widehat{p}_X\\) es un estimador insesgado de \\(p_X\\). Si calculáramos muchas proporciones muestrales de muestras aleatorias de \\(X\\), es muy probable que, de media, obtuviéramos un valor muy cercano a la proporción poblacional de éxitos \\(p_X\\). \\(\\sigma({\\widehat{p}_X})=\\sqrt{\\dfrac{p_X(1-p_X)}{n}}\\) nos dice que, fijada la variable \\(X\\), si tomamos muestras de tamaño mayor, la variabilidad de los resultados de \\(\\widehat{p}_X\\) disminuye. Si tomamos muestras aleatorias simples de tamaño \\(n\\) de una variable aleatoria Bernoulli \\(X\\): \\(\\sqrt{\\dfrac{p_X(1-p_X)}{n}}\\) es el error típico de la variable aleatoria \\(\\widehat{p}_X\\): su desviación típica. Para cada muestra, \\(\\sqrt{\\dfrac{\\widehat{p}_X(1-\\widehat{p}_X)}{n}}\\) es el error típico de la muestra, que estima el error típico de \\(\\widehat{p}_X\\). Y como la proporción muestral es un caso particular de media muestral, por el Teorema Central del Límite tenemos el resultado siguiente: Teorema 2.5 Si \\(n\\) es grande y las muestras aleatorias son simples, \\(\\widehat{p}_X\\) es aproximadamente \\(N\\big (p_X,\\sqrt{{p_X(1-p_X)}/{n}}\\big)\\) y por lo tanto \\[ \\frac{\\widehat{p}_X-p_X}{\\sqrt{\\frac{{p}_X(1-{p}_X)}{n}}} \\] es aproximadamente \\(N(0,1)\\). En el caso de la proporción muestral, a veces vamos a permitir tomar muestras aleatorias sin reposición. En este caso, la variable \\(Y\\) que cuenta el número de éxitos en una muestra aleatoria sin reposición de tamaño \\(n\\) de la variable de Bernoulli \\(X\\), y que verifica que que \\(\\widehat{p}_X=Y/n\\), es hipergeométrica. De aquí deducimos que seguimos teniendo que \\(E(\\widehat{p}_X)=p_X\\), pero ahora, si \\(N\\) es el tamaño de la población, \\[ \\sigma({\\widehat{p}_X})=\\sqrt{\\frac{p_X(1-p_X)}{n}}\\cdot \\sqrt{\\frac{\\vphantom{(p_X}N-n}{N-1}}. \\] El factor \\[ \\sqrt{\\frac{N-n}{N-1}} \\] que transforma \\(\\sigma({\\widehat{p}_X})\\) para muestras aleatorias simples en la desviación típica de \\({\\widehat{p}_X}\\) para muestras aleatorias sin reposición es el factor de población finita que transformaba la desviación típica de una variable binomial (que cuenta éxitos en muestras aleatorias simples) en la desviación típica de una variable hipergeométrica (que cuenta éxitos en muestras aleatorias sin reposición). Y recordad que si el tamaño de la población \\(N\\) es muy grande comparado con \\(n\\), podemos suponer que una muestra aleatoria sin reposición es simple. 2.3 La varianza muestral Dada una variable aleatoria \\(X\\), llamamos: Varianza muestral de (muestras de) tamaño \\(n\\), \\(\\widetilde{S}_{X}^2\\), a la variable aleatoria consistente en tomar una muestra aleatoria simple de tamaño \\(n\\) de \\(X\\) y calcular la varianza muestral de sus valores. Desviación típica muestral de (muestras de) tamaño \\(n\\), \\(\\widetilde{S}_{X}\\), a la variable aleatoria consistente en tomar una muestra aleatoria simple de tamaño \\(n\\) de \\(X\\) y calcular la desviación típica muestral de sus valores. Formalmente, estas variables se definen tomando \\(n\\) copias independientes \\(X_1,\\ldots,X_n\\) de \\(X\\) y calculando \\[ \\widetilde{S}_{X}^2=\\frac{\\sum_{i=1}^n (X_{i}-\\overline{X})^2}{n-1},\\quad \\widetilde{S}_{X}=+\\sqrt{\\widetilde{S}_{X}^2} \\] Tenemos los dos resultados siguientes. El primero nos dice que \\(\\widetilde{S}_{X}^2\\) es un estimador insesgado de la varianza poblacional \\(\\sigma_{X}^2\\). Teorema 2.6 \\(E(\\widetilde{S}_{X}^2)=\\sigma_{X}^2\\). Por lo tanto, esperamos que la varianza muestral de una muestra aleatoria simple de \\(X\\) valga \\(\\sigma_{X}^2\\), en el sentido usual de que si tomamos muestras aleatorias simples de \\(X\\) de tamaño \\(n\\) grande y calculamos sus varianzas muestrales, muy probablemente obtengamos de media un valor muy cercano a \\(\\sigma_{X}^2\\). Y por lo tanto no esperamos que la varianza “a secas” de una muestra aleatoria simple valga \\(\\sigma_{X}^2\\), porque la varianza muestral y la varianza “a secas” dan valores diferentes (tienen el mismo numerador y denominadores diferentes). El segundo resultado nos dice que si la variable \\(X\\) es normal, un múltiplo adecuado de \\(\\widetilde{S}_{X}^2\\) tiene distribución conocida, lo que nos permitirá calcular probabilidades de sucesos relativos a \\(\\widetilde{S}_{X}^2\\). Teorema 2.7 Si \\(X\\) es \\(N(\\mu_X,\\sigma_X)\\) y tomamos muestras de tamaño \\(n\\), la variable aleatoria \\[ \\chi^2= \\dfrac{(n-1)\\widetilde{S}_{X}^2}{\\sigma_{X}^2} \\] tiene una distribución conocida, llamada ji cuadrado con \\(n-1\\) grados de libertad, \\(\\chi_{n-1}^2\\). La letra griega \\(\\chi\\) en castellano se lee ji; en catalán, khi; en inglés, chi, pronunciado “xai”. Figura 2.1: Chi es un gatito, la distribución és ji. La distribución \\(\\chi_\\nu^2\\), donde \\(\\nu\\) es un parámetro llamado sus grados de libertad, es la distribución de probabilidad de la suma de los cuadrados de \\(\\nu\\) copias independientes de una variable normal estándar. Para R es chisq. Os puede interesar recordar que una variable \\(\\chi_\\nu^2\\) de tipo ji cuadrado con \\(\\nu\\) grados de libertad: Tiene valor esperado \\(E(\\chi_\\nu^2)=\\nu\\) y varianza \\(\\sigma(\\chi_\\nu^2)^2=2 \\nu\\). Su función de distribución es estrictamente creciente. Su densidad es asimétrica a la derecha, como muestra el gráfico siguiente: A medida que el número de grados de libertad \\(\\nu\\) crece, esta asimetría tiende a desaparecer y, por el Teorema Central del Límite, si \\(\\nu\\) es lo bastante grande, la distribución \\(\\chi_\\nu^2\\) se aproxima a la de una variable normal \\(N(\\nu,\\sqrt{2\\nu})\\). Tened cuidado: Si la variable poblacional \\(X\\) no es normal, la conclusión del Teorema 2.7 no es verdadera. Aunque \\(X\\) sea normal, \\(E(\\widetilde{S}_{X})\\neq \\sigma_{X}\\). La desviación típica muestral es un estimador sesgado de \\(\\sigma_{X}\\) (pero tiene otras buenas propiedades que hacen que la usemos igualmente). Ya lo hemos comentado antes. Si \\(S^2_{X}\\) es la varianza “a secas” (dividiendo por \\(n\\) en vez de por \\(n-1\\)), \\(E(S^2_{X})\\neq \\sigma^2_{X}\\). Esto lo podéis comprobar fácilmente, porque \\(S_X^2\\) se obtiene a partir de \\(\\widetilde{S}_{X}^2\\) cambiando el denominador, \\[ S_X^2=\\frac{n-1}{n} \\widetilde{S}_{X}^2 \\] y por lo tanto \\[ E(S_X^2)=\\frac{n-1}{n}E(\\widetilde{S}_{X}^2)=\\frac{n-1}{n}\\sigma^2_{X} \\] 2.4 La distribución t de Student Recordad que si la variable poblacional \\(X\\) es \\(N(\\mu_X,\\sigma_X)\\) y tomamos muestras aleatorias simples de tamaño \\(n\\), entonces \\(\\overline{X}\\) es \\(N(\\mu_X,\\sigma_X/\\sqrt{n})\\) y por lo tanto, tipificando, la variable \\[ \\frac{\\overline{X}-\\mu_X}{\\sigma_{X}/\\sqrt{n}} \\] es normal estándar. Esto no nos sirve de mucho para calcular la probabilidad de que \\(\\overline{X}\\) se aleje mucho de \\(\\mu_X\\), porque casi nunca sabemos la desviación típica poblacional \\(\\sigma_{X}\\). ¿Qué pasa si la estimamos por medio de \\(\\widetilde{S}_{X}\\) con la misma muestra con la que calculamos \\(\\overline{X}\\)? Pues que el resultado siguiente nos salva el día, porque la variable que resulta tiene distribución conocida y por lo tanto podemos calcular probabilidades con ella. Teorema 2.8 Sea \\(X\\) una variable \\(N(\\mu_X,\\sigma_X)\\). Si tomamos muestras aleatorias simples de tamaño \\(n\\), la variable aleatoria \\[ T=\\frac{\\overline{X}-\\mu_X}{\\widetilde{S}_{X}/\\sqrt{n}} \\] tiene una distribución conocida, llamada t de Student con \\(n-1\\) grados de libertad, \\(t_{n-1}\\). Al denominador \\(\\widetilde{S}_{X}/\\sqrt{n}\\) de la \\(T\\) del teorema anterior se le llama el error típico de la muestra, y estima el error típico \\(\\sigma_X/\\sqrt{n}\\) de la media muestral \\(\\overline{X}\\). Fijaos en que el teorema anterior es solo para variables poblacionales \\(X\\) normales. Si \\(X\\) no es normal, es necesario que \\(n\\) sea muy grande para que, por el Teorema Central del Límite, \\(T\\) sea aproximadanente \\(t_{n-1}\\). Para R, la distribución t de Student es t, a secas. Algunas propiedades que conviene que recordéis de las variables \\(T_\\nu\\) que tienen distribución \\(t\\) de Student con \\(\\nu\\) grados de libertad, \\(t_\\nu\\): Su valor esperado es \\(E(T_\\nu)=0\\) y su varianza es \\(\\sigma(T_\\nu)=\\dfrac{\\nu}{\\nu-2}\\) (en realidad esto solo es verdad si \\(\\nu\\geqslant 3\\), pero no hace falta recordarlo). Su función de distribución es estrictamente creciente. Su densidad es simétrica respecto de 0 (como la de una \\(N(0,1)\\)) y por lo tanto \\[ P(T_\\nu\\leqslant -x)=P(T_\\nu\\geqslant x)=1-P(T_\\nu\\leqslant x) \\] Si \\(\\nu\\) es grande (digamos, de nuevo, \\(\\nu\\geqslant 40\\)), \\(T_\\nu\\) es aproximadamente una \\(N(0,1)\\) (pero con un poco más de varianza, porque \\(\\nu/(\\nu-2)&gt;1\\), y por consiguiente un poco más achatada). Denotaremos por \\(t_{\\nu,q}\\) el \\(q\\)-cuantil de una variable aleatoria \\(T_{\\nu}\\) con distribución \\(t_\\nu\\). Es decir, \\(t_{\\nu,q}\\) es el valor tal que \\[ P(T_{\\nu}\\leqslant t_{\\nu,q})=q \\] Entonces: Por la simetría de la densidad de \\(T_{\\nu}\\), \\[ t_{\\nu,q}=-t_{\\nu,1-q}. \\] Exactamente lo mismo que pasaba con la normal estándar Si \\(\\nu\\) es grande, \\(T_\\nu\\) será aproximadamente una \\(N(0,1)\\) y por lo tanto \\(t_{\\nu,q}\\) es aproximadamente igual a \\(z_q\\). No confundáis: Desviación típica de una variable aleatoria: El parámetro poblacional, normalmente desconocido. Es \\(\\sigma_X\\). Desviación típica (muestral o no) de una muestra: El estadístico que calculamos sobre la muestra. Es \\(\\widetilde{S}_X\\) (la muestral) o \\({S}_X\\) (la “a secas”). Error típico de la media muestral: La desviación típica de la variable \\(\\overline{X}\\). Es \\(\\sigma_X/\\sqrt{n}\\), con \\(n\\) el tamaño de las muestras. Error típico de una muestra: Estimación del error típico de \\(\\overline{X}\\) a partir de la muestra. Es \\(\\widetilde{S}_X/\\sqrt{n}\\), con \\(n\\) el tamaño de la muestra. Fijaos en que el denominador \\(\\sqrt{n}\\) hace que, en general, los errores típicos sean mucho más pequeños que las desviaciones típicas. Id con cuidado, porque esto se usa a menudo en artículos para enmascarar los resultados. Si una muestra ha salido con una dispersión muy grande, se da su error típico en vez de su desviación típica y parece que ha salido más concentrada. 2.5 Test (1) Si el tamaño de una muestra aleatoria simple aumenta (marca todas las afirmaciones correctas): La media muestral siempre disminuye. El error típico de la media muestral siempre disminuye. El error típico de la muestra siempre disminuye. La varianza muestral siempre aumenta. El número de grados de libertad de la variable ji cuadrado asociada a la varianza muestral siempre aumenta. Ninguna de las otras afirmaciones es correcta (2) Si queremos disminuir a la mitad el error típico de la media muestral: Tenemos que aumentar en un 50% el tamaño de las muestras. Tenemos que doblar el tamaño de las muestras. Tenemos que cuadruplicar el tamaño de las muestras. Tenemos que dividir por 2 el tamaño de las muestras. Tenemos que dividir por 4 el tamaño de las muestras. Ninguna de las otras respuestas es correcta. (3) La prevalencia de una afección en una población es del 10%. Si estimamos dicha prevalencia repetidamente mediante las proporciones muestrales de muestras aleatorias simples de tamaño 1000, estas estimaciones siguen una distribución que (marca todas las afirmaciones correctas): Es aproximadamente normal. Es binomial. Tiene media 0.1. Tiene media 900. Ninguna de las otras afirmaciones es correcta (4) Sobre una muestra de 100 mujeres se obtuvo una concentración media de la hemoglobina de 10 con una desviación típica de 2. ¿Qué vale el error típico de la muestra (para la media muestral, se entiende)? 0.02 0.04 0.2 0.4 1 Ninguno de los anteriores (5) ¿Cuáles de las afirmaciones siguientes sobre la media muestral son verdaderas? Marca todas las respuestas correctas. Si la distribución poblacional es normal, siempre coincide con la media de la distribución poblacional. Si la distribución poblacional es normal, siempre coincide con la mediana de la distribución poblacional. Siempre sirve para estimar la media poblacional, aunque la distribución poblacional no sea normal. Si la distribución poblacional es normal, sirve para estimar la mediana poblacional. Se calcula sumando todos los valores de la muestra y dividiendo por \\(n-1\\), donde \\(n\\) indica el tamaño de la muestra. Ninguna de las otras respuestas es correcta. (6) La concentración de un determinado metabolito en sangre tiene un valor medio \\(\\mu\\). Si tomamos muestras aleatorias simples de \\(n=20\\) individuos, calculamos su media muestral \\(\\overline{X}\\) y su desviación típica muestral \\(\\widetilde{S}_X\\) (marca la continuación más correcta): El estadístico \\(\\frac{\\overline{X}-\\mu}{\\widetilde{S}_X/\\sqrt{20}}\\) tiene siempre distribución normal. El estadístico \\(\\frac{\\overline{X}-\\mu}{\\widetilde{S}_X/\\sqrt{20}}\\) tiene siempre distribución t de Student. El estadístico \\(\\frac{\\overline{X}-\\mu}{\\widetilde{S}_X/\\sqrt{20}}\\) tiene distribución normal si la concentración sigue una ley normal. El estadístico \\(\\frac{\\overline{X}-\\mu}{\\widetilde{S}_X/\\sqrt{20}}\\) tiene distribución t de Student si la concentración tiene distribución normal. El estadístico \\(\\frac{\\overline{X}-\\mu}{\\widetilde{S}_X/\\sqrt{20}}\\) no tiene nunca ni distribución normal ni distribución t de Student, porque \\(n=20\\) no es lo bastante grande. (7) Tenemos una variable aleatoria \\(X\\) normal de media \\(\\mu\\) y desviación típica \\(\\sigma\\). Tomamos muestras aleatorias simples de tamaño \\(n\\), y denotamos por \\(\\widetilde{S}_X\\) su desviación típica muestral. ¿Cuáles de las afirmaciones siguientes son verdaderas? Marca todas las respuestas verdaderas: \\(E(\\widetilde{S}_X^2)=\\sigma^2\\). \\(E(\\widetilde{S}_X)=\\sigma\\). \\(\\widetilde{S}_X^2\\) sigue una distribución ji cuadrado con \\(n-1\\) grados de libertad. \\((n-1)\\widetilde{S}_X^2/\\sigma^2\\) sigue una distribución ji cuadrado con \\(n-1\\) grados de libertad. \\((n-1)\\widetilde{S}_X/\\sigma\\) sigue una distribución ji cuadrado con \\(n-1\\) grados de libertad. Todas las otras respuestas son falsas. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
