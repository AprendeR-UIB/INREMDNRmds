[
["index.html", "Bioestadística (Medicina UIB) Presentación", " Bioestadística (Medicina UIB) 2020-11-18 Presentación Esto es una edición en línea de los apuntes de Introducción a la Investigación en Salud y Bioestadística del grado de Medicina de la UIB. Este trabajo se publica bajo licencia Atribución-No Comercial-SinDerivados 4.0 Estos apuntes están en construcción. En la lista siguiente iremos anunciando las actualizaciones: 2020-11-07: Corregidos errores varios en la lección 5. 2020-11-03: Corregido error en las “consideraciones” al principio de la sección 5.2. 2020-11-01: Publicadas las lecciones 6 a 9. 2020-10-24: Publicada la lección 5. 2020-10-21: Corregido el error en la Figura 2.14 (la clasificación de los estudios de cohorte y de casos y controles estaba intercambiada). 2020-10-05: Publicada la lección 4. 2020-09-30: Cambios cosméticos en la lección 2: añadidos algunos dibujos, reescrito algunas frases, añadido un ejemplo (2.10) sobre la importancia de la elección de controles. 2020-09-27: Publicadas las lecciones 1, 2 y 3. El libro está escrito en R Markdown, usando RStudio como editor de textos y el paquete bookdown para convertir los ficheros markdown en un libro. Significado de algunas cajas: Material muy importante. ¡Cuidado! Ejercicio. Detalles matemáticos que os pueden interesar, pero que podéis obviar sin ningún problema. Comentario que queremos enfatizar. Comentario que queremos que recordéis Cuestión en la que queremos que caigáis en la cuenta. Acabamos de matar un gatito "],
["distribuciones-muestrales.html", "Lección 1 Distribuciones muestrales 1.1 Estimadores 1.2 La media muestral 1.3 La proporción muestral 1.4 La varianza muestral 1.5 La distribución t de Student 1.6 Test", " Lección 1 Distribuciones muestrales 1.1 Estimadores En un problema típico de estadística inferencial: Queremos conocer el valor de una característica en el total de una población, pero no podemos medir esta característica en todos los individuos de la población. Entonces, extraemos una muestra de la población, medimos la característica en los individuos de esta muestra, calculamos algo con estas medidas e inferimos el valor de la característica en el global de la población. Inmediatamente surgen varias preguntas, que responderemos entre esta lección y la próxima: ¿Cómo tiene que ser la muestra? ¿Qué tenemos que calcular? ¿Con qué precisión podemos inferir la característica de la población? ¿Qué tipo de muestra tomamos? Vamos a suponer de ahora en adelante que tomamos muestras aleatorias simples. Esto incluye las muestras aleatorias sin reposición si la población es mucho más grande que la muestra, ya que entonces no hay diferencia práctica entre permitir y prohibir las repeticiones. En algunos casos muy concretos permitiremos muestras aleatorias sin reposición en general. Sí, ya sabemos que en la práctica casi nunca tomamos muestras aleatorias, sino oportunistas. En este caso, recordad lo que os explicábamos en la Sección ??. Lo que se suele hacer es describir en detalle las características de la muestra para justificar que, pese a no ser aleatoria, es razonablemente representativa de la población y podría pasar por aleatoria. ¿Qué calculamos? Pues un estimador: alguna función adecuada aplicada a los valores de la muestra, y que dependerá de lo que queramos estimar. Por ejemplo: Si queremos estimar la altura media de los estudiantes de la UIB, tomaremos una muestra aleatoria de estudiantes de la UIB, mediremos sus alturas y calcularemos su media aritmética. Si queremos estimar la proporción de estudiantes de la UIB que han pasado la COVID-19, tomaremos una muestra aleatoria de estudiantes de la UIB, les haremos un test de anticuerpos y calcularemos la proporción muestral de positivos en la muestra. Si queremos estimar el riesgo relativo para un estudiante de la UIB de suspender alguna asignatura si es fumador, tomaremos una muestra aleatoria de estudiantes de la UIB, anotaremos si fuman o no y si han suspendido alguna asignatura o no, y calcularemos la diferencia entre las proporciones muestrales de suspensos entre los fumadores y los no fumadores de la muestra. Fijaos que un estimador es una variable aleatoria, definida sobre la población formada por las muestras de la población de partida, y por lo tanto tiene función de distribución (que genéricamente llamaremos distribución muestral, para indicar que mide la probabilidad de que le pase algo al valor del estimador sobre una muestra), esperanza, desviación típica, etc. 1.2 La media muestral Cuando queremos estimar el valor medio de una variable sobre una población, tomamos una muestra de valores y calculamos su media aritmética, ¿verdad? Pues eso es la media muestral. Dada una variable aleatoria \\(X\\), llamamos media muestral (de tamaño \\(n\\)) a la variable aleatoria \\(\\overline{X}\\) “Tomamos una muestra aleatoria simple de tamaño \\(n\\) de \\(X\\) y calculamos la media aritmética de sus valores”. Veamos algunas propiedades de la distribución muestral de \\(\\overline{X}\\): Teorema 1.1 Sea \\(X\\) una variable aleatoria de media \\(\\mu_X\\) y desviación típica \\(\\sigma_X\\), y sea \\(\\overline{X}\\) la media muestral de tamaño \\(n\\) de \\(X\\). Entonces: \\(E(\\overline{X})=\\mu_X\\) \\(\\sigma(\\overline{X})=\\dfrac{\\sigma_X}{\\sqrt{n}}\\) Que \\(E(\\overline{X})\\) sea \\(\\mu_X\\) nos indica que \\(\\overline{X}\\) sirve para estimar \\(\\mu_X\\), porque su valor esperado es \\(\\mu_X\\): Si calculáramos muchas medias de muestras aleatorias de \\(X\\), es muy probable que, de media, obtuviéramos un valor muy cercano a \\(\\mu_X\\). Cuando el valor esperado de un estimador es precisamente el parámetro poblacional que se quiere estimar, se dice que el estimador es insesgado. Así, el primer punto del teorema anterior nos dice que la media muestral \\(\\overline{X}\\) es un estimador insesgado de la media poblacional \\(\\mu_X\\). Que \\(\\sigma(\\overline{X})\\) sea \\(\\sigma_X/\\sqrt{n}\\) implica que la variabilidad de las medias muestrales crece con la variabilidad de \\(X\\) y decrece si tomamos muestras de mayor tamaño. Esto último es razonable. Aunque la variabilidad de \\(X\\) sea grande, si tomamos muestras grandes, al calcular la media los valores extremos se compensarán y las medias resultantes tendrán menos variabilidad que \\(X\\). A \\(\\sigma_X/\\sqrt{n}\\) se le llama el error típico de la media muestral (para la variable aleatoria \\(X\\) y muestras de tamaño \\(n\\)). Ejemplo 1.1 Vamos a hacer un experimento. Vamos a tomar una población de 106 sujetos y una variable aleatoria \\(X\\) que sobre cada sujeto toma un valor entre 0 y 1, todos estos valores con la misma probabilidad, y dibujamos un histograma para que veáis que los valores salen muy dispersos Damos el código de R para que podáis repetir el experimento en casa (y como es una simulación, cada vez que lo repitáis dará valores diferentes, pero el mismo efecto global). X=runif(10^6) hist(X,freq=FALSE,main=&quot;Histograma de X&quot;,xlab=&quot;&quot;,ylab=&quot;Densidad&quot;) La desviación típica de \\(X\\), \\(\\sigma_X\\), es sd(X) ## [1] 0.2886765 Ahora vamos a tomar 1000 medias muestrales de tamaño 100 de esta población, y dibujaremos su histograma. Medias=replicate(1000,mean(sample(X,100,replace=TRUE))) hist(Medias,freq=FALSE,main=&quot;Histograma de las medias muestrales&quot;,xlab=&quot;&quot;,ylab=&quot;Densidad&quot;) Los valores de \\(\\overline{X}\\) se concentran alrededor de 0.5. Veamos su desviación típica: sd(Medias) ## [1] 0.02907574 Fijaos que se acerca mucho al valor \\(\\sigma_X/\\sqrt{100}\\) predicho por el teorema anterior (que nos da el valor de la desviación típica poblacional de \\(\\overline{X}\\), es decir, para toda la población de medias muestrales, y nosotros hemos tomado una muestra de 1000 medias). La media muestral \\(\\overline{X}\\) de tamaño \\(n\\) de una variable aleatoria \\(X\\) se interpreta formalmente como la variable aleatoria obtenida tomando \\(n\\) copias independientes \\(X_1,\\ldots,X_n\\) de \\(X\\) y calculando \\[ \\overline{X}=\\frac{X_1+\\cdots+X_n}{n}. \\] Por lo tanto, es una combinación lineal de \\(n\\) copias independientes de \\(X\\). Recordando que una combinación de variables aleatorias normales independientes es normal, tenemos el resultado siguiente: Teorema 1.2 Si \\(X\\) es \\(N(\\mu_X,\\sigma_X)\\), \\(\\overline{X}\\) es \\(N(\\mu_X,\\sigma_X/\\sqrt{n})\\), y por lo tanto \\[ Z=\\frac{\\overline{X}-\\mu_X}{\\sigma_X/\\sqrt{n}}\\text{\\ es\\ }N(0,1) \\] Si \\(X\\) no es normal, la tesis del teorema anterior sigue siendo cierta “aproximadamente” si \\(n\\) es grande. Este resultado, llamado el Teorema Central del Límite es, como su nombre indica, uno de los más importantes en estadística. Teorema 1.3 Sea \\(X\\) una variable aleatoria cualquiera de esperanza \\(\\mu_X\\) y desviación típica \\(\\sigma_X\\). Si \\(n\\) es suficientemente grande, \\[ \\overline{X}\\text{\\ es aproximadamente\\ }N(\\mu_X, {\\sigma_X}/{\\sqrt{n}}) \\] y por lo tanto \\[ Z=\\frac{\\overline{X}-\\mu_X}{{\\sigma_X}/{\\sqrt{n}}}\\text{\\ es aproximadamente\\ }N(0,1) \\] Dos observaciones: ¿Cuándo una muestra es lo suficientemente grande como para poder invocar el Teorema Central del Límite? En realidad, depende de la \\(X\\). Cuánto más se parezca \\(X\\) a una variable normal, más pequeñas pueden ser la muestras. Por fijar un valor, aceptaremos que “suficientemente grande” es \\(n\\geq 40\\). ¿Qué quiere decir que una variable aleatoria sea “aproximadamente” normal? Pues que su función de distribución \\(F_X\\) toma valores muy cercanos a la función de distribución de una normal. Recordad cómo una \\(B(n,p)\\) con \\(n\\) grande era “aproximadamente normal” en la Lección anterior. Si miráis el histograma de las 1000 medias muestrales del Ejemplo 1.1, veréis que se parece al de una muestra de una variable normal. Es que \\(\\overline{X}\\) es aproximadamente normal, por el Teorema Central del Límite. En resumen, para muestras aleatorias simples: Si \\(X\\) es normal, siempre se tiene que \\(\\overline{X}\\) es \\(N(\\mu_X,{\\sigma_X}/{\\sqrt{n}})\\) Si \\(X\\) no es normal pero \\(n\\) es grande (pongamos \\(n\\geq 30\\), aunque puede ser menor si \\(X\\) se parece a una normal y seguramente tendrá que ser mayor si \\(X\\) es muy diferente de una normal), \\(\\overline{X}\\) es aproximadamente \\(N(\\mu_X,{\\sigma_X}/{\\sqrt{n}})\\) Para muestras que no sean (prácticamente) aleatorias simples, ambos resultados son falsos, pero si no tenemos nada más… 1.3 La proporción muestral Cuando queremos estimar la proporción de sujetos de una población que tienen una determinada característica, tomamos una muestra y calculamos la proporción de sujetos de la muestra con esta característica. Dada una variable aleatoria \\(X\\) de Bernoulli \\(Be(p_X)\\), llamamos proporción muestral, \\(\\widehat{p}_X\\), a la variable aleatoria consistente en tomar una muestra aleatoria de tamaño \\(n\\) de \\(X\\) y calcular la proporción de éxitos en la muestra: es decir, contar el número total de éxitos y dividir el resultado por \\(n\\). Fijaos en que \\(\\widehat{p}_X\\) es un caso particular de media muestral \\(\\overline{X}\\): estamos calculando medias muestrales de muestras de la variable de Bernoulli \\(X\\). Por lo tanto, todo lo que hemos dicho para medias muestrales vale también para proporciones muestrales: Teorema 1.4 Si \\(X\\) es una variable aleatoria de Bernoulli con probabilidad poblacional de éxito \\(p_X\\): \\(E(\\widehat{p}_X)=p_X\\) \\(\\sigma({\\widehat{p}_X})=\\sqrt{\\dfrac{p_X(1-p_X)}{n}}\\). \\(E(\\widehat{p}_X)=p_X\\) nos dice que \\(\\widehat{p}_X\\) es un estimador insesgado de \\(p_X\\). Si calculáramos muchas proporciones muestrales de muestras aleatorias de \\(X\\), es muy probable que, de media, obtuviéramos un valor muy cercano a la proporción poblacional de éxitos \\(p_X\\). \\(\\sigma({\\widehat{p}_X})=\\sqrt{\\dfrac{p_X(1-p_X)}{n}}\\) nos dice que, fijada la variable \\(X\\), si tomamos muestras de tamaño mayor, la variabilidad de los resultados de \\(\\widehat{p}_X\\) disminuye. En el caso de la proporción muestral, nos vamos a permitir tomar muestras aleatorias sin reposición. En este caso, seguimos teniendo que \\(E(\\widehat{p}_X)=p_X\\), pero ahora, si \\(N\\) es el tamaño de la población, \\[ \\sigma({\\widehat{p}_X})=\\sqrt{\\frac{p_X(1-p_X)}{n}}\\cdot \\sqrt{\\frac{N-n}{N-1}}. \\] A este factor \\[ \\sqrt{\\frac{N-n}{N-1}} \\] que transforma \\(\\sigma({\\widehat{p}_X})\\) para muestras aleatorias simples en la desviación típica de \\({\\widehat{p}_X}\\) para muestras aleatorias sin reposición se le llama el factor de población finita, y si os fijáis, es el que transformaba la desviación típica de una variable binomial (que cuenta éxitos en muestras aleatorias simples) en la desviación típica de una variable hipergeométrica (que cuenta éxitos en muestras aleatorias sin reposición). Y recordad que si el tamaño de la población \\(N\\) es muy grande comparado con \\(n\\), podemos suponer que una muestra aleatoria sin reposición es simple. Y como antes, si $N$ es muy grande relativamente a $n$, podemos suponer en la práctica que una muestra aleatoria sin reposición es simple Si tomamos muestras aleatorias simples de tamaño \\(n\\) de una variable aleatoria Bernoulli \\(X\\): \\(\\sqrt{\\dfrac{p_X(1-p_X)}{n}}\\) es el error típico de la variable aleatoria \\(\\widehat{p}_X\\): su desviación típica. Para cada muestra, \\(\\sqrt{\\dfrac{\\widehat{p}_X(1-\\widehat{p}_X)}{n}}\\) es el error típico de la muestra, que estima el error típico de \\(\\widehat{p}_X\\). Y como la proporción muestral es un caso particular de media muestral, por el Teorema Central del Límite tenemos el resultado siguiente: Teorema 1.5 Si \\(n\\) es grande y las muestras aleatorias son simples, \\[ \\widehat{p}_X\\text{ es aproximadamente }N\\Big (p_X,\\sqrt{\\frac{p_X(1-p_X)}{n}}\\Big) \\] y por lo tanto \\[ \\frac{\\widehat{p}_X-p_X}{\\sqrt{\\frac{{p}_X(1-{p}_X)}{n}}}\\text{ es aproximadamente }N(0,1) \\] 1.4 La varianza muestral Dada una variable aleatoria \\(X\\), llamamos: Varianza muestral, \\(\\widetilde{S}_{X}^2\\), a la variable aleatoria consistente en tomar una muestra aleatoria simple de tamaño \\(n\\) de \\(X\\) y calcular la varianza muestral de sus valores. Desviación típica muestral, \\(\\widetilde{S}_{X}\\), a la variable aleatoria consistente en tomar una muestra aleatoria simple de tamaño \\(n\\) de \\(X\\) y calcular la desviación típica muestral de sus valores. Formalmente, estas variables se definen tomando \\(n\\) copias independientes \\(X_1,\\ldots,X_n\\) de \\(X\\) y calculando \\[ \\widetilde{S}_{X}^2=\\frac{\\sum_{i=1}^n (X_{i}-\\overline{X})^2}{n-1},\\quad \\widetilde{S}_{X}=+\\sqrt{\\widetilde{S}_{X}^2} \\] Tenemos los dos resultados siguientes para variables poblacionales normales. El primero nos dice que en este caso \\(\\widetilde{S}_{X}^2\\) es un estimador insesgado de la varianza poblacional \\(\\sigma_{X}^2\\). Teorema 1.6 Si \\(X\\) es \\(N(\\mu_X,\\sigma_X)\\), \\(E(\\widetilde{S}_{X}^2)=\\sigma_{X}^2\\). Por lo tanto, si \\(X\\) es normal, esperamos que la varianza muestral de una muestra aleatoria simple grande sea \\(\\sigma_{X}^2\\), en el sentido usual de que si tomamos muestras aleatorias simples de \\(X\\) de tamaño \\(n\\) grande y calculamos sus varianzas muestrales, esperamos de media obtener un valor muy cercano a \\(\\sigma_{X}^2\\). El segundo resultado nos dice que un múltiplo adecuado de \\(\\widetilde{S}_{X}^2\\) tiene distribución muestral conocida, lo que nos permite calcular probabilidades. Teorema 1.7 Si \\(X\\) es \\(N(\\mu_X,\\sigma_X)\\) y tomamos muestras de tamaño \\(n\\), la variable aleatoria \\[ \\chi^2= \\dfrac{(n-1)\\widetilde{S}_{X}^2}{\\sigma_{X}^2} \\] tiene distribución conocida llamada ji cuadrado con \\(n-1\\) grados de libertad, \\(\\chi_{n-1}^2\\). La distribución \\(\\chi_m^2\\) (la letra griega \\(\\chi\\) en castellano se lee ji; en catalán, khi; en inglés, chi, pronunciado “xai”), donde \\(m\\) es un parámetro llamado sus grados de libertad, es la distribución de probabilidad de la suma de los cuadrados de \\(m\\) variables aleatorias normales estándar independientes. Para R es chisq. Os puede interesar recordar que una variable \\(\\chi_m^2\\): Tien valor esperado \\(\\mu=m\\) y varianza \\(\\sigma^2=2 m\\). Tiene una distribución asimétrica a la derecha, como muestra el gráfico siguiente: Aunque, por el Teorema Central del Límite, si \\(m\\) es muy grande, la distribución \\(\\chi_m^2\\) se aproxima a la de una variable normal \\(N(m,\\sqrt{2m})\\). Tened cuidado: Si la variable poblacional \\(X\\) no es normal, las conclusiones de los dos teoremas anteriores no son verdaderas, ni tan solo aproximadamente. Aunque \\(X\\) sea normal, \\(E(\\widetilde{S}_{X})\\neq \\sigma_{X}\\). Aunque \\(X\\) sea normal, si \\(S^2_{X}\\) es la varianza a secas (dividiendo por \\(n\\)), \\(E(S^2_{X})\\neq \\sigma^2_{X}\\). Esto lo podéis comprobar fácilmente, porque \\(S_X^2\\) se obtiene a partir de \\(\\widetilde{S}_{X}\\) cambiando el denominador, \\[ S_X^2=\\frac{n-1}{n} \\widetilde{S}_{X} \\] y por lo tanto \\[ E(S_X^2)=\\frac{n-1}{n}E(\\widetilde{S}_{X})=\\frac{n-1}{n}\\sigma^2_{X} \\] 1.5 La distribución t de Student Recordad que si la variable poblacional \\(X\\) es \\(N(\\mu_X,\\sigma_X)\\) y tomamos muestras aleatorias simples de tamaño \\(n\\), entonces la variable \\[ \\frac{\\overline{X}-\\mu}{\\sigma_{X}/\\sqrt{n}} \\] es normal estándar. Desde el punto de vista teórico, para óbtener fórmulas, esto será útil, pero normalmente no nos sirve para calcular la probabilidad de que a \\(\\overline{X}\\) le pase algo, porque casi nunca sabemos la desviación típica poblacional \\(\\sigma_{X}\\). ¿Qué pasa si la estimamos por medio de \\(\\widetilde{S}_{X}\\) con la misma muestra con la que calculamos \\(\\overline{X}\\)? Pues que el resultado siguiente nos salva el día, porque la variable que resulta tiene distribución conocida. Teorema 1.8 Sea \\(X\\) una variable \\(N(\\mu_X,\\sigma_X)\\). Si tomamos muestras aleatorias simples de tamaño \\(n\\), la variable aleatoria \\[ T=\\frac{\\overline{X}-\\mu_X}{\\widetilde{S}_{X}/\\sqrt{n}} \\] tiene una distribución conocida, llamada t de Student con \\(n-1\\) grados de libertad, \\(t_{n-1}\\). Al denominador \\(\\widetilde{S}_{X}/\\sqrt{n}\\) de la \\(T\\) del teorema anterior se le llama el error típico de la muestra, y estima el error típico \\(\\sigma_X/\\sqrt{n}\\) de la media muestral \\(\\overline{X}\\). Algunas propiedades que conviene que recordéis de las variables \\(T_m\\) que tienen distribución \\(t\\) de Student con \\(m\\) grados de libertad, \\(t_m\\): Su valor esperado es \\(E(T_m)=0\\) y su varianza es \\(\\sigma(T_m)=\\dfrac{m}{m-2}\\) (en realidad esto solo es verdad si \\(m\\geq 3\\), pero no hace falta recordarlo). Su función de distribución es simétrica respecto de \\(0\\) (como la de una \\(N(0,1)\\)): \\[ P(T_m\\leq -x)=P(T_m\\geq x)=1-P(T_m\\leq x) \\] Si \\(m\\) es grande (digamos, de nuevo, \\(m\\geq 40\\)), \\(T_m\\) es aproximadamente una \\(N(0,1)\\) (pero con un poco más de varianza, porque \\(m/(m-2)&gt;1\\), y por lo tanto un poco más achatada). Esto es consecuencia del Teorema Central del Límite. Denotaremos por \\(t_{m,q}\\) el \\(q\\)-cuantil de una variable aleatoria \\(T_{m}\\) con distribución \\(t_m\\). Es decir, \\(t_{m,q}\\) és el valor tal que \\[ P(T_{m}\\leq t_{m,q})=q \\] Entonces: Por la simetría de la distribución \\(t_m\\), \\[ t_{m,q}=-t_{m,1-q}. \\] Exactamente lo mismo que pasaba con la normal estándar Si \\(m\\) es grande, \\(T_m\\) será aproximadamente una \\(N(0,1)\\) y por lo tanto \\(t_{m,q}\\approx z_q\\). No confundáis: Desviación típica de una variable aleatoria: El parámetro poblacional, normalmente desconocido. Es \\(\\sigma_X\\). Desviación típica (muestral o no) de una muestra: El estadístico que calculamos sobre la muestra. Es \\(\\widetilde{S}_X\\) (la muestral) o \\({S}_X\\) (la “a secas”). Error típico de la media muestral: La desviación típica de la variable media muestral. Es \\(\\sigma_X/\\sqrt{n}\\), con \\(n\\) el tamaño de las muestras. Error típico de una muestra: Estimación del error típico del estimador a partir de la muestra. Es \\(\\widetilde{S}_X/\\sqrt{n}\\), con \\(n\\) el tamaño de la muestra. Fijaos en que el denominador \\(\\sqrt{n}\\) hace que, en general, los errores típicos sean mucho más pequeños que las desviaciones típicas. 1.6 Test (1) Si el tamaño de una muestra aleatoria simple de una variable aleatoria aumenta (marcad todas las afirmaciones correctas): La media muestral siempre disminuye. El error típico de la media muestral siempre disminuye. El error típico de la muestra siempre disminuye. La varianza muestral siempre aumenta. El número de grados de libertad del estimador \\(\\chi^2\\) asociado a la varianza muestral siempre aumenta. Ninguna de las otras afirmaciones es correcta (2) Si queremos disminuir a la mitad el error típico de la media muestral: Tenemos que aumentar en un 50% el tamaño de las muestras. Tenemos que doblar el tamaño de las muestras. Tenemos que cuadruplicar el tamaño de las muestras. Tenemos que dividir por 2 el tamaño de las muestras. Tenemos que dividir por 4 el tamaño de las muestras. Ninguna de las otras respuestas es correcta. (3) La prevalencia de una afección en una población es del 10%. Si estimamos dicha prevalencia repetidamente mediante las proporciones muestrales de muestras aleatorias simples de tamaño 1000, estas estimaciones siguen una distribución que (marcad todas las afirmaciones correctas): Es una distribución muestral. Es aproximadamente normal. Es binomial. Tiene media 0.1. Tiene media 900. Ninguna de las otras afirmaciones es correcta (4) Sobre una muestra de 100 mujeres se obtuvo una concentración media de la hemoglobina de 10 con una desviación típica de 2. ¿Qué vale el error típico de la muestra (para la media muestral, se entiende)? 02 0.04 0.2 0.4 1 Ninguno de los anteriores (5) ¿Cuáles de las afirmaciones siguientes sobre la media muestral son verdaderas? Marcad todas las respuestas correctas. Si la distribución poblacional es normal, siempre coincide con la media de la distribución poblacional. Si la distribución poblacional es normal, siempre coincide con la mediana de la distribución poblacional. Siempre sirve para estimar la media poblacional. Si la distribución poblacional es normal, sirve para estimar la mediana poblacional. Se calcula sumando todos los valores de la muestra y dividiendo por \\(n-1\\), donde \\(n\\) indica la medida de la muestra. Ninguna de las otras respuestas es correcta. (6) La concentración de un cierto metabolito en sangre tiene un valor medio \\(\\mu\\). Si tomamos muestras aleatorias simples de 20 individuos, calculamos su media muestral \\(\\overline{X}\\) y su desviación típica muestral \\(\\widetilde{S}_X\\) (marcad la continuación más correcta): El estadístico \\(\\frac{\\overline{X}-\\mu}{\\widetilde{S}_X/\\sqrt{n}}\\) tiene siempre distribución normal. El estadístico \\(\\frac{\\overline{X}-\\mu}{\\widetilde{S}_X/\\sqrt{n}}\\) tiene siempre distribución t de Student. El estadístico \\(\\frac{\\overline{X}-\\mu}{\\widetilde{S}_X/\\sqrt{n}}\\) tiene distribución normal si la concentración sigue una ley normal. El estadístico \\(\\frac{\\overline{X}-\\mu}{\\widetilde{S}_X/\\sqrt{n}}\\) tiene distribución t de Student si la concentración tiene distribución normal. El estadístico \\(\\frac{\\overline{X}-\\mu}{\\widetilde{S}_X/\\sqrt{n}}\\) no tiene nunca ni distribución normal ni distribución t de Student, porque las muestras no son lo suficientemente grandes. (7) Tenemos una variable aleatoria \\(X\\) normal de media \\(\\mu\\) y desviación típica \\(\\sigma\\). Tomamos muestras aleatorias simples de tamaño \\(n\\), y denotamos por \\(\\widetilde{S}_X\\) su desviación típica muestral. ¿Cuáles de las afirmaciones siguientes son verdaderas? Marcad todas las respuestas verdaderas: \\(E(\\widetilde{S}_X^2)=\\sigma^2\\). \\(E(\\widetilde{S}_X)=\\sigma\\). \\(\\widetilde{S}_X^2\\) sigue una distribución ji cuadrado con \\(n-1\\) grados de libertad. \\((n-1)\\widetilde{S}_X^2/\\sigma^2\\) sigue una distribución ji cuadrado con \\(n-1\\) grados de libertad. Todas las otras respuestas son falsas. "]
]
