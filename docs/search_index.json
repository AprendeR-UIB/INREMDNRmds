[
["index.html", "Bioestadística (Medicina UIB) Presentación", " Bioestadística (Medicina UIB) 2020-11-16 Presentación Esto es una edición en línea de los apuntes de Introducción a la Investigación en Salud y Bioestadística del grado de Medicina de la UIB. Este trabajo se publica bajo licencia Atribución-No Comercial-SinDerivados 4.0 Estos apuntes están en construcción. En la lista siguiente iremos anunciando las actualizaciones: 2020-11-07: Corregidos errores varios en la lección 5. 2020-11-03: Corregido error en las “consideraciones” al principio de la sección 5.2. 2020-11-01: Publicadas las lecciones 6 a 9. 2020-10-24: Publicada la lección 5. 2020-10-21: Corregido el error en la Figura 2.14 (la clasificación de los estudios de cohorte y de casos y controles estaba intercambiada). 2020-10-05: Publicada la lección 4. 2020-09-30: Cambios cosméticos en la lección 2: añadidos algunos dibujos, reescrito algunas frases, añadido un ejemplo (2.10) sobre la importancia de la elección de controles. 2020-09-27: Publicadas las lecciones 1, 2 y 3. El libro está escrito en R Markdown, usando RStudio como editor de textos y el paquete bookdown para convertir los ficheros markdown en un libro. Significado de algunas cajas: Material muy importante. ¡Cuidado! Ejercicio. Detalles matemáticos que os pueden interesar, pero que podéis obviar sin ningún problema. Comentario que queremos enfatizar. Comentario que queremos que recordéis Cuestión en la que queremos que caigáis en la cuenta. Acabamos de matar un gatito "],
["variables-aleatorias-discretas.html", "Lección 1 Variables aleatorias discretas 1.1 Generalidades sobre variables aleatorias 1.2 Densidad y distribución 1.3 Esperanza 1.4 Varianza y desviación típica 1.5 Cuantiles 1.6 Familias importantes de variables aleatorias discretas 1.7 Test", " Lección 1 Variables aleatorias discretas 1.1 Generalidades sobre variables aleatorias Una variable aleatoria sobre una población \\(\\Omega\\) es una aplicación \\[ X: \\Omega\\to \\mathbb{R} \\] que asigna a cada sujeto de \\(\\Omega\\) un número real. La idea intuitiva tras esta definición es que una variable aleatoria mide una característica de los sujetos de \\(\\Omega\\) que varía al azar de un sujeto a otro. Por ejemplo: Tomamos una persona de una población y medimos su nivel de colesterol, o su altura, o su número de hijos… En este caso, \\(\\Omega\\) es la población bajo estudio, de la que tomamos la persona que medimos. Lanzamos una moneda equilibrada 3 veces y contamos las caras que obtenemos. En este caso, \\(\\Omega\\) es la población virtual de las secuencias de 3 lanzamientos de una moneda equilibrada. Procurad, al menos al principio, adquirir la disciplina de describir siempre las variables aleatorias mediante una plantilla del estilo de “Tomamos … y medimos …”, para que os quede claro cuál es la población y cuál la función. Además, añadid las unidades si es necesario. Por ejemplo: “Tomamos una persona de Mallorca y medimos su altura (en cm)”. Fijaos en que esta variable aleatoria no es la misma que “Tomamos una persona de Mallorca y medimos su altura (en m)”, porque, aunque mide lo mismo sobre los mismos sujetos, les asigna números diferentes. Y también es diferente de “Tomamos una persona de Suecia y medimos su altura (en cm)”, porque ha cambiado la población. En cambio en “Lanzamos una moneda 3 veces al aire y contamos las caras” no hay necesidad de especificar unidades, a no ser que vayáis a usar una unidad inesperada (yo qué sé, que contéis las caras en fracciones de docena). Para poder hablar genuinamente de variable aleatoria, hay que tener una probabilidad definida sobre \\(\\Omega\\). En realidad, lo que más nos interesará de una variable aleatoria son las probabilidades de los sucesos que define. ¿Y qué tipo de sucesos son los que nos interesan cuando medimos características numéricas? Pues básicamente sucesos definidos mediante igualdades y desigualdades. Por ejemplo, si \\(X\\) es la variable aleatoria “Tomamos una persona y medimos su nivel de colesterol en plasma (en mg/dl)”, nos pueden interesar sucesos del estilo de: El conjunto de las personas cuyo nivel de colesterol está entre 200 y 240. Lo denotaremos \\[ 200\\leq X\\leq 240 \\] El conjunto de las personas cuyo nivel de colesterol es menor o igual que 200: \\[ X\\leq 200 \\] El conjunto de las personas cuyo nivel de colesterol es mayor que 180: \\[ X&gt;180 \\] El conjunto de las personas cuyo nivel de colesterol es exactamente 180: \\[ X=180 \\] El conjunto de las personas cuyo nivel de colesterol es 180, 182, 184 o 200: \\[ X\\in\\{180,182,184,200\\} \\] etc. Normalmente, de estos sucesos lo que nos interesará será su probabilidad, y entonces usaremos notaciones del estilo de las siguientes: \\(P(200\\leq X\\leq 240)\\): Probabilidad de que una persona tenga el nivel de colesterol entre 200 y 240 (o, para abreviar, “probabilidad de que \\(X\\) esté entre 200 y 240”). \\(P(X\\leq 200)\\): Probabilidad de que una persona tenga el nivel de colesterol menor o igual que 200 (probabilidad de que \\(X\\) sea menor o igual que 240). \\(P(X&gt;180)\\): Probabilidad de que una persona tenga el nivel de colesterol mayor que 180 (probabilidad de que \\(X\\) sea mayor que 180). \\(P(X=180)\\): Probabilidad de que una persona tenga nivel de colesterol igual a 180 (probabilidad de que \\(X\\) valga 180). \\(P(X\\in\\{180,182,184,200\\})\\): Probabilidad de que una persona tenga nivel de colesterol 180 o 182 o 184 o 200 (probabilidad de que \\(X\\) valga 180 o 182 o 184 o 200). Recodad que nuestras probabilidades son proporciones. Por lo tanto, por ejemplo, \\(P(200\\leq X\\leq 240)\\) es la proporción de personas (de alguna población concreta) con nivel de colesterol entre 200 y 240. En este contexto, indicaremos normalmente la unión con una o y la intersección con una coma. Por ejemplo, si \\(X\\) es la variable aleatoria “Lanzamos una moneda 6 veces y contamos las caras”: \\(P(X\\leq 2\\text{ o }X\\geq 5)\\): Probabilidad de sacar como máximo 2 caras o como mínimo 5. \\(P(2\\leq X, X\\leq 5)\\): Probabilidad de sacar un número de caras que sea mayor o igual que 2 y menor o igual que 5; es decir, \\(P(2\\leq X\\leq 5)\\). Dos variables aleatorias \\(X,Y\\) son independientes cuando, para todos los pares de valores \\(a,b\\in \\mathbb{R}\\), los sucesos \\[ X\\leq a, Y\\leq b \\] son independientes, es decir, \\[ P(X\\leq a, Y\\leq b)=P(X\\leq a)\\cdot P(Y\\leq b) \\] Por ejemplo, si tomamos una persona y: \\(X\\): le pedimos que lance una moneda 3 veces y contamos las caras \\(Y\\): medimos su nivel de colesterol en plasma (en mg/dl) (seguramente) \\(X\\) e \\(Y\\) son independientes. Más en general, unas variables aleatorias \\(X_1,X_2,\\ldots,X_n\\) son independientes cuando, para todos \\(a_1,a_2,\\ldots,a_n\\in \\mathbb{R}\\), los sucesos \\[ X_1\\leq a_1, X_2\\leq a_2,\\ldots, X_n\\leq a_n \\] son independientes. Si \\(X_1,X_2,\\ldots,X_n\\) son variables aleatorias independientes, se tiene que, para todos los subconjuntos \\(A_1,\\ldots, A_n\\subseteq \\mathbb{R}\\) “razonables” (incluye todos los que os puedan interesar), los sucesos \\[ X_1\\in A_1, X_2\\in A_2,\\ldots, X_n\\in A_n \\] son también independientes, y por lo tanto en particular que \\[ P(X_1\\in A_1,\\ldots,X_n\\in A_n)=P(X_1\\in A_1)\\cdots P(X_n\\in A_n) \\] Vamos a distinguir dos tipos de variables aleatorias: Discretas: Sus posibles valores son datos cuantitativos discretos: Número de caras en 3 lanzamientos de una moneda Número de hijos Número de casos nuevos de COVID-19 en un día en una población Continuas: Sus posibles valores (teóricos) son datos cuantitativos continuos: Peso Nivel de colesterol en sangre Diámetro de un tumor 1.2 Densidad y distribución Sea \\(X: \\Omega\\to \\mathbb{R}\\) una variable aleatoria discreta. Su dominio \\(D_X\\) es el conjunto de posibles valores que puede tomar, es decir, el conjunto de los \\(x\\in \\mathbb{R}\\) tales que \\(P(X=x)&gt;0\\). Su función de densidad es la función \\(f_X:\\mathbb{R}\\to [0,1]\\) definida por \\[ f_X(x)=P(X=x) \\] Es decir, la función que asigna a cada \\(x\\in \\mathbb{R}\\) la probabilidad de que \\(X\\) valga \\(x\\) (la proporción de sujetos de la población en los que \\(X\\) vale \\(x\\)). Su función de distribución es la función \\(F_X:\\mathbb{R}\\to [0,1]\\) definida por \\[ F_X(x)=P(X\\leq x) \\] Es decir, la función que asigna a cada \\(x\\in \\mathbb{R}\\) la probabilidad de que el valor de \\(X\\) sea \\(\\leq x\\) (la proporción de sujetos de la población en los que \\(X\\) vale \\(\\leq x\\)). También se le suele llamar función de probabilidad acumulada para poner énfasis en el hecho de que \\(F_X(x)\\) mide la probabilidad “acumulada” hasta \\(x\\). Ejemplo 1.1 Sea \\(X\\) la variable aleatoria “Lanzamos 3 veces una moneda equilibrada y contamos las caras”. Entonces Su dominio es el conjunto de sus posibles valores: \\(D_X=\\{0,1,2,3\\}\\). Su función de densidad viene definida por \\(f_X(x)=P(X=x)\\): \\(f_X(0)=P(X=0)=1/8\\) (la probabilidad de sacar 0 caras) \\(f_X(1)=P(X=1)=3/8\\) (la probabilidad de sacar 1 cara) \\(f_X(2)=P(X=2)=3/8\\) (la probabilidad de sacar 2 caras) \\(f_X(3)=P(X=3)=1/8\\) (la probabilidad de sacar 3 caras) \\(f_X(x)=P(X=x)=0\\) para cualquier otro valor de \\(x\\) (la probabilidad de sacar \\(x\\) caras si \\(x\\notin\\{0,1,2,3\\}\\) es 0) Si \\(X\\) es una variable aleatoria discreta que solo puede tomar los valores de \\(D_X\\), entonces \\(P(X\\in A)=0\\) para cualquier subconjunto \\(A\\) disjunto de \\(D_X\\), precisamente porque \\(X\\) no puede tomar ningún valor de \\(A\\). Por ejemplo, ¿cuál es la probabilidad de sacar 2.5 caras al lanzar 3 veces una moneda? 0 ¿Y la de sacar \\(\\pi\\) caras? 0. En resumen, la función de densidad de \\(X\\) es \\[ f_X(x) =\\left\\{ \\begin{array}{ll} 1/8 &amp; \\text{ si $x=0$}\\\\ 3/8 &amp; \\text{ si $x=1$}\\\\ 3/8 &amp; \\text{ si $x=2$}\\\\ 1/8 &amp; \\text{ si $x=3$}\\\\ 0 &amp; \\text{ si $x\\neq 0,1,2,3$} \\end{array}\\right. \\] Figura 1.1: Función de densidad de la variable aleatoria que cuenta el número de caras en 3 lanzamientos Veamos su función de distribución \\(F_X\\). Recordad que \\(F_X(x)=P(X\\leq x)\\) y que nuestra variable solo puede tomar los valores 0, 1, 2 y 3. Si \\(x&lt;0\\), \\(F_X(x)=P(X\\leq x)=0\\) porque \\(X\\) no puede tomar ningún valor estrictamente negativo. Si \\(0\\leq x&lt;1\\), \\(F_X(x)=P(X\\leq x)=P(X=0)=f_X(0)=1/8\\), porque si \\(0\\leq x&lt;1\\), el único valor \\(\\leq x\\) que puede tomar \\(X\\) es el 0. Si \\(1\\leq x&lt;2\\), \\(F_X(x)=P(X\\leq x)=P(X=0\\text{ o }X=1)=f_X(0)+f_X(1)=4/8=1/2\\), porque si \\(1\\leq x&lt;2\\), los únicos valores \\(\\leq x\\) que puede tomar \\(X\\) son 0 y 1. Si \\(2\\leq x&lt;3\\), \\(F_X(x)=P(X\\leq x)=P(X=0\\text{ o }X=1\\text{ o }X=2)\\) \\(=f_X(0)+f_X(1)+f_X(2)=7/8\\), porque si \\(2\\leq x&lt;3\\), los únicos valores \\(\\leq x\\) que puede tomar \\(X\\) son 0, 1 y 2. Si \\(3\\leq x\\), \\(F_X(x)=P(X\\leq x)=1\\), porque si \\(3\\leq x\\), seguro que obtenemos un número de caras \\(\\leq x\\). Por lo tanto, la función \\(F_X\\) es la función \\[ F_X(x) =\\left\\{ \\begin{array}{ll} 0 &amp; \\text{ si $x&lt;0$}\\\\ 1/8 &amp; \\text{ si $0\\leq x&lt; 1$}\\\\ 4/8 &amp; \\text{ si $1\\leq x&lt; 2$}\\\\ 7/8 &amp; \\text{ si $2\\leq x&lt; 3$}\\\\ 1 &amp; \\text{ si $3\\leq x$} \\end{array}\\right. \\] Su gráfico es el siguiente: Figura 1.2: Función de distribución de la variable aleatoria que cuenta el número de caras en 3 lanzamientos Fijaos en que \\(F_X\\) es una función escalonada, con saltos en los valores del dominio, que son los únicos con probabilidad estrictamente mayor que 0 y por lo tanto los únicos que “suman” probabilidad. Además es creciente, porque si \\(x\\leq y\\), todos los sujetos de \\(X\\leq x\\) también pertenecen a \\(X\\leq y\\), y por lo tanto \\[ P(X\\leq x)\\leq P(X\\leq y). \\] Finalmente, como los valores que toma \\(F_X\\) son probabilidades, no pueden ser ni menores que 0 ni mayores que 1. El conocimiento de \\(f_X\\), más las reglas del cálculo de probabilidades, permite calcular la probabilidad de cualquier suceso relacionado con \\(X\\): \\[ P(X\\in A) =\\sum_{a\\in A} P(X=a) =\\sum_{a\\in D_X\\cap A} P(X=a) = \\sum_{a\\in D_X\\cap A} f_X(a) \\] En particular \\[ F_X(x)=P(X\\leq x)=\\sum_{a\\in D_X,\\ a\\leq x} f_X(a) \\] Dada una variable aleatoria discreta \\(X\\), ¿pueden existir dos elementos diferentes \\(x,y\\in D_X\\) tales que \\(F_X(x)=F_X(y)\\)? La moda de una variable aleatoria discreta \\(X\\) es el valor (o los valores) \\(x_0\\) tal que \\(f_X(x_0)=P(X=x_0)\\) es máximo. S trata por lo tanto del “valor más frecuente de \\(X\\)” en la población. Por ejemplo, para nuestra variable aleatoria que cuenta el número de cara en 3 lanzamientos de una moneda equilibrada, la moda son los valores 1 y 2. Hay un aspecto de las variables aleatorias discretas sobre el que queremos llamar la atención, sobre todo para compararlo luego con las variables continuas: Los valores de \\(P(X\\leq x)\\) y \\(P(X&lt;x)\\) pueden ser diferentes. Por ejemplo, con la variable \\(X\\) “Lanzamos una moneda equilibrada 3 veces y contamos las caras”: La probabilidad de sacar 2 o menos caras ya la hemos calculado, y es \\(P(X\\leq 2)=7/8\\) Pero la probabilidad de sacar menos de 2 caras es \\(P(X&lt;2)\\). En este caso, sacar menos de 2 caras es sacar 1 o menos, por lo tanto \\(P(X&lt;2)=P(X\\leq 1)=4/8\\). Considerad la variable aleatoria \\(X\\) “Lanzamos una moneda equilibrada al aire tantas veces como sea necesario hasta que salga una cara por primera vez, y contamos cuántas veces la hemos tenido que lanzar” ¿Cuál es su dominio? ¿Cuál es su función de densidad? ¿Cuál es su moda? ¿Qué significa? ¿Cuál es su función de distribución? 1.3 Esperanza Cuando tomamos una muestra de una variable aleatoria \\(X\\) definida sobre una población, podemos calcular la media y la desviación típica de sus valores para obtener una idea de cuál es su valor central y cómo son de variados sus valores. Este tipo de información también nos puede interesar para el total de la población: cuál es el “valor medio” de \\(X\\) sobre toda la población y si toma valores muy variados, o más bien concentrados alrededor de este valor medio. Lo primero lo medimos con la esperanza, o media, de \\(X\\), y lo segundo con su desviación típica. Empecemos con la primera. La esperanza, o media, (o valor esperado, valor medio, valor promedio…) de una variable aleatoria discreta \\(X\\) con densidad \\(f_X:D_X\\to [0,1]\\) es \\[ E(X)=\\sum_{x\\in D_X} x\\cdot f_X(x) \\] También se suele denotar por \\(\\mu_X\\). La interpretación de \\(E(X)\\) es que es la media de los valores de la variable \\(X\\) en el total de la población \\(\\Omega\\). En efecto, como \\(P(X=x)\\) es la proporción de los sujetos de \\(\\Omega\\) en los que \\(X\\) vale \\(x\\), entonces \\[ E(X)=\\sum_{x\\in D_X} x\\cdot P(X=x) \\] es el promedio del valor de \\(X\\) sobre todos los elementos de \\(\\Omega\\). Comparadlo con el ejemplo siguiente. Ejemplo 1.2 Si, en una clase, un 10% han sacado un 4 en un examen, un 20% un 6, un 50% un 8 y un 20% un 10, ¿cuál ha sido la nota media obtenida? Suponemos que calcularíais esta media como \\[ 4\\cdot 0.1+6\\cdot 0.2+8\\cdot 0.5+10\\cdot 0.2=7.6 \\] Pues este valor es la esperanza de la variable aleatoria “Tomo un estudiante de esta clase y miro qué nota ha sacado en este examen”: \\[ \\begin{array}{rl} E(X)\\!\\!\\!\\!\\! &amp;=4\\cdot P(X=4)+6\\cdot P(X=6)+8\\cdot P(X=8)+10\\cdot P(X=10)\\\\ &amp; = 4\\cdot 0.1+6\\cdot 0.2+8\\cdot 0.5+10\\cdot 0.2=7.6 \\end{array} \\] Aparte de su interpretación como “el promedio de \\(X\\) en el total de la población”, \\(E(X)\\) es también el valor esperado de \\(X\\), en el sentido siguiente: Suponed que tomamos una muestra aleatoria de \\(n\\) sujetos de la población, medimos \\(X\\) sobre ellos y calculamos la media aritmética de los \\(n\\) valores obtenidos. Entonces, cuando el tamaño \\(n\\) de la muestra tiende a \\(\\infty\\), esta media aritmética tiende a valer \\(E(X)\\) “casi siempre”, en el sentido de que la probabilidad de que su límite sea \\(E(X)\\) es 1. Es decir: si medimos \\(X\\) sobre muchos sujetos elegidos al azar y calculamos la media de los valores obtenidos, esperamos obtener un valor muy próximo a \\(E(X)\\). Ejemplo 1.3 Seguimos con la variable aleatoria \\(X\\) “Lanzamos una moneda equilibrada al aire 3 veces y contamos las caras”. Su valor esperado es \\[ E(X)= 0\\cdot \\frac{1}{8}+1\\cdot \\frac{3}{8}+2\\cdot \\frac{3}{8}+3\\cdot \\frac{1}{8}=1.5 \\] Esto nos dice que si repetimos muchas veces el experimento de lanzar la moneda 3 veces y contar las caras, la media de los resultados obtenidos dará, muy probablemente, un valor muy cercano a 1.5. Abreviamos esto diciendo que si lanzamos la moneda 3 veces, de media esperamos sacar 1.5 caras. Más en general, si \\(g:D_X\\to \\mathbb{R}\\) es una aplicación, \\[ E(g(X))=\\sum_{x\\in D_X} g(x)\\cdot f_X(x) \\] De nuevo, su interpretación natural es que es el promedio de \\(g(X)\\) sobre la población en la que medimos \\(X\\), y también es el valor “esperado” de \\(g(X)\\) en el sentido anterior. Ejemplo 1.4 Si lanzamos una moneda equilibrada al aire 3 veces, contamos las caras y elevamos este número de caras al cuadrado, ¿qué valor esperamos obtener, de media? Será la esperanza de \\(X^2\\), siendo \\(X\\) la variable aleatoria “Lanzamos una moneda equilibrada al aire 3 veces y contamos las caras”: \\[ E(X^2)= 0\\cdot \\frac{1}{8}+1\\cdot \\frac{3}{8}+2^2\\cdot \\frac{3}{8}+3^2\\cdot \\frac{1}{8}=3 \\] Fijaos en que \\(E(X^2) \\neq E(X)^2\\). Por ejemplo, en los dos últimos ejemplos hemos visto que si \\(X\\) es la variable aleatoria que cuenta el número de caras en 3 lanzamientos de una moneda equilibrada, \\(E(X^2)=3 \\neq E(X)^2=1.5^2=2.25\\). La esperanza de las variables aleatorias discretas tiene las propiedades siguientes, todas razonables si la interpretáis en términos del valor promedio de \\(X\\): Si indicamos por \\(b\\) una variable aleatoria constante que sobre todos los individuos de la población toma el valor \\(b\\in \\mathbb{R}\\), entonces \\(E(b)=b\\). Si en una clase todo el mundo saca un 8 de un examen, la nota media es 8, ¿no? La esperanza es lineal: Si \\(a,b\\in \\mathbb{R}\\), \\(E(aX+b)=aE(X)+b\\) Si en una clase la media de un examen ha sido un 6 y decidimos multiplicar por 1.2 todas las notas y sumarles 1 punto, la media de la nueva nota será 1.2·6+1=8.2, ¿no? Si \\(Y\\) es otra variable aleatoria, \\(E(X+Y)=E(X)+E(Y)\\). Si en una clase la media de la parte de cuestiones de un examen ha sido un 3.5 (sobre 5) y la de la parte de ejercicios ha sido un 3 (sobre 5), la nota media del examen será un 3.5+3=6.5, ¿no? La esperanza es monótona creciente: Si \\(X\\leq Y\\) (en el sentido de que el valor de \\(X\\) sobre un sujeto de la población \\(\\Omega\\) siempre es menor o igual que el valor de \\(Y\\) sobre el mismo sujeto), entonces \\(E(X)\\leq E(Y)\\). Si todos sacáis mejor nota de Anatomía que de Bioestadística, la nota media de Anatomía será mayor que la de Bioestadística, ¿no? Más en general, si \\(g,h:D_X\\to \\mathbb{R}\\) son dos funciones tales que \\(g(X)\\leq h(X)\\), entonces \\(E(g(X))\\leq E(h(X))\\) Pero atención, en general \\(E(g(X)) \\neq g(E(X))\\), como ya hemos visto. 1.4 Varianza y desviación típica La varianza de una variable aleatoria discreta \\(X\\) es \\[ \\sigma(X)^2 =E((X-\\mu_X)^2) =\\sum_{x\\in D_X} (x-\\mu_X)^2\\cdot f_X(x) \\] Es la media en la población del cuadrado de la diferencia entre \\(X\\) y su valor medio \\(\\mu_X\\). Mide la dispersión de los resultados de \\(X\\) respecto de la media. También la denotaremos \\(\\sigma_X^2\\). El resultado siguiente puede ser útil para calcularla “a mano”. Teorema 1.1 \\(\\sigma(X)^2=E(X^2)-\\mu_X^2\\). Operemos (y recorddad que \\(E(X)=\\mu_X\\)) \\[ \\begin{array}{rl} \\sigma(X)^2\\!\\!\\!\\!\\! &amp; =E((X-\\mu_X)^2)=E(X^2-2\\mu_X\\cdot X+\\mu_X^2)\\\\ &amp; = E(X^2)-2\\mu_X\\cdot E(X)+\\mu_X^2\\\\ &amp; \\text{(por la linealidad de $E$)}\\\\ &amp; = E(X^2)-2\\mu_X^2+\\mu_X^2=E(X^2)-\\mu_X^2 \\end{array} \\] La desviación típica (o desviación estándar) de una variable aleatoria discreta \\(X\\) es la raíz cuadrada positiva de su varianza: \\[ \\sigma(X)=+\\sqrt{\\sigma(X)^2} \\] También mide la dispersión de los valores de \\(X\\) respecto de la media. La denotaremos a veces por \\(\\sigma_X\\). En el contexto de las variables aleatorias, no hay “varianza” y “varianza muestral”, solo “varianza” (el mismo nombre os tendría que dar la pista que la “varianza muestral” está definida solo para muestras). El motivo para introducir la varianza y la desviación típica para medir la dispersión de los valores de \\(X\\) es la misma que en estadística descriptiva: la varianza es más fácil de manejar (no involucra raíces cuadradas) pero sus unidades son las de \\(X\\) al cuadrado, mientras que las unidades de la desviación típica son las de \\(X\\), y por lo tanto su valor es más fácil de interpretar. Ejemplo 1.5 Seguimos con la variable aleatoria \\(X\\) “Lanzamos una monea equilibrada 3 veces y contamos las caras”. Su varianza es: \\[ \\begin{array}{rl} \\sigma(X)^2 \\!\\!\\!\\!\\! &amp; \\displaystyle=(0-1.5)^2\\cdot \\frac{1}{8}+(1-1.5)^2\\cdot \\frac{3}{8}\\\\ &amp;\\displaystyle\\qquad +(2-1.5)^2\\cdot \\frac{3}{8}+(3-1.5)^2\\cdot \\frac{1}{8}=0.75 \\end{array} \\] Si recordamos que \\(\\mu_X=E(X)=1.5\\) y \\(E(X^2)=3\\), podemos ver que \\[ E(X^2)-\\mu_X^2=3-1.5^2=0.75=\\sigma(X)^2 \\] Su desviación típica es \\[ \\sigma(X) =\\sqrt{\\sigma(X)^2}=\\sqrt{0.75}= 0.866 \\] Veamos algunas propiedades de la varianza y la desviación típica: Si \\(b\\) es una variable aleatoria constante que sobre todos los individuos de la población toma el valor \\(b\\in \\mathbb{R}\\), entonces \\(\\sigma(b)^2=\\sigma(b)=0\\). Una variable aleatoria constante tiene cero dispersión, ¿no? \\(\\sigma(aX+b)^2=a^2\\cdot \\sigma(X)^2\\). En efecto \\[ \\begin{array}{l} \\sigma(aX+b)^2 =E((aX+b)^2)-E(aX+b)^2\\\\ \\quad = E(a^2X^2+2abX+b^2)-(aE(X)+b)^2\\\\ \\quad \\text{(por la linealidad de la esperanza)}\\\\ \\quad = a^2E(X^2)+2abE(X)+b^2-a^2E(X)^2-2abE(X)-b^2\\\\ \\quad \\text{(de nuevo, por la linealidad de la esperanza)}\\\\ \\quad = a^2(E(X^2)-E(X)^2)=a^2\\sigma(X)^2 \\end{array} \\] \\(\\sigma(aX+b)=|a|\\cdot \\sigma(X)\\) (recordad que la desviación típica es positiva, y \\(+\\sqrt{a^2}=|a|\\)). Si \\(X,Y\\) son variables aleatorias independientes, \\[ \\sigma(X+Y)^2=\\sigma(X)^2+\\sigma(Y)^2 \\] y por lo tanto \\[ \\sigma(X+Y)=\\sqrt{\\sigma(X)^2+\\sigma(Y)^2} \\] Si no son independientes, en general esta igualdad es falsa. Por poner un ejemplo extremo, \\[ \\sigma(X+X)^2=\\sigma(2X)^2=4\\sigma(X)^2\\neq \\sigma(X)^2+\\sigma(X)^2. \\] 1.5 Cuantiles Sea \\(p\\in [0,1]\\). El cuantil de orden \\(p\\) (o \\(p\\)-cuantil) de una variable aleatoria \\(X\\) discreta es el valor \\(x_p\\in D_X\\) tal que: \\(P(X\\leq x_p)\\geq p\\). \\(P(X&lt; x_p)&lt;p\\) Por ejemplo, que el 0.25-cuantil de una variable aleatoria discreta \\(X\\) sea, yo qué sé, 8, significa que al menos una cuarta parte de la población tiene un valor de \\(X\\) menor o igual que 8, pero menos de un 25% de la población que tiene un valor de \\(X\\) estrictamente menor que 8. Si existe algún \\(x_p\\in D_X\\) tal que \\(F_X(x_p)(=P(X\\leq x_p))=p\\), entonces el \\(p\\)-cuantil es ese \\(x_p\\). Como en estadística descriptiva, algunos cuantiles de variables aleatorias tienen nombres propios. Por ejemplo: La mediana de \\(X\\) es su 0.5-cuantil El primer y el tercer cuartiles de \\(X\\) son sus \\(0.25\\)-cuantil y \\(0.75\\)-cuantil, respectivamente. Etc. Ejemplo 1.6 Seguimos con la variable aleatoria \\(X\\) “Lanzamos una monea equilibrada 3 veces y contamos las caras”. Recordemos que su función de distribución es \\[ F_X(x)=\\left\\{ \\begin{array}{ll} 0 &amp; \\text{ si $x&lt;0$}\\\\ 0.125 &amp; \\text{ si $0\\leq x&lt;1$}\\\\ 0.5 &amp; \\text{ si $1\\leq x&lt;2$}\\\\ 0.875 &amp; \\text{ si $2\\leq x&lt;3$}\\\\ 1 &amp; \\text{ si $3\\leq x $} \\end{array} \\right. \\] Entonces, por ejemplo: Su 0.125-cuantil es 0 Su 0.25-cuantil es 1 Su mediana es 1 Su 0.75-cuantil es 2 No confundáis variable aleatoria con muestra. Aunque usamos “media”, “varianza”, “cuantiles”, etc. en ambos contextos, significan cosas diferentes. Una variable aleatoria representa una característica númerica de los sujetos de una población: “Tomamos un estudiante de medicina y medimos su altura en m.” La “media” o la “varianza” de esta variable son las de toda la población. Las llamaremos, cuando queramos recalcarlo poblacionales. Una muestra de una variable aleatoria son los valores de la misma sobre un subconjunto (relativamente pequeño) de la población. Medimos las alturas (en m) de 50 estudiantes de medicina de este curso. La “media” o la “varianza” de esta muestra son solo las de esas 50 alturas. 1.6 Familias importantes de variables aleatorias discretas En esta sección vamos a describir tres familias de variables aleatorias “distinguidas” que tenéis que conocer: Binomial Hipergeométrica Poisson Cada una de estas familias tienen un tipo específico de función de densidad. De estas familias de variables tenéis que saber: Distinguirlas: saber cuando una variable aleatoria es de una familia de estas. Su densidad, su valor esperado y su varianza. Usar algún programa o alguna aplicación para calcular cosas con ellas cuando sea necesario. 1.6.1 Variables aleatorias binomiales Un experimento de Bernoulli es una acción con solo dos posibles resultados, que identificamos con “Éxito” (\\(E\\)) y “Fracaso” (\\(F\\)), y de la que, en principio, no podemos predecir su resultado debido a la influencia del azar. Por ejemplo, lanzar un dado y mirar si ha salido un 6 (\\(E\\): sacar un 6; \\(F\\): cualquier otro resultado). La probabilidad de éxito \\(p\\) de un experimento de Bernoulli es la probabilidad de obtener \\(E\\). Es decir, \\(P(E)=p\\). Naturalmente, entonces, \\(P(F)=1-p\\). Por ejemplo: Lanzar una moneda equilibrada y mirar si da cara (\\(E\\): dar cara; \\(p=1/2\\)). Realizar un test PCR de COVID-19 a una persona y mirar si da positivo (\\(E\\): dar positivo; \\(p\\): la proporción de personas de la población de la que hemos extraído nuestro sujeto que dan positivo en el test). Una variable aleatoria de Bernoulli de parámetro \\(p\\) (abreviadamente, \\(Be(p)\\)) es una variable aleatoria \\(X\\) consistente en efectuar un experimento de Bernoulli y dar 1 si se ha obtenido un éxito y 0 si se ha obtenido un fracaso. Una variable aleatoria binomial de parámetros \\(n\\) y \\(p\\) (abreviadamente, \\(B(n,p)\\)) es una variable aleatoria \\(X\\) que cuenta el número de éxitos \\(E\\) en una secuencia de \\(n\\) repeticiones independientes de un mismo experimento de Bernoulli de probabilidad de éxito \\(p\\). Independientes significa que resultado de una no depende de los resultados de las otras. Llamaremos a \\(n\\) el tamaño de las muestras y a \\(p\\) la probabilidad (poblacional) de éxito. A veces también diremos que una variable \\(X\\) \\(B(n,p)\\) tiene distribución binomial de parámetros \\(n\\) y \\(p\\). Por ejemplo: Una variable de Bernoulli \\(Be(p)\\) es una variable binomial \\(B(1,p)\\). Lanzar una moneda equilibrada 10 veces y contar las caras es una variable binomial \\(B(10,0.5)\\) Elegir 20 personas al azar, una tras otra, permitiendo repeticiones y de manera independiente las unas de las otras, realizar sobre ellas un test PCR y contar cuántos dan positivo: es binomial \\(B(20,p)\\) con \\(p\\) la probabilidad de que el test dé positivo. El tipo más común de variables binomiales en medicina es este último: Tenemos un subconjunto \\(A\\) de una población \\(\\Omega\\) (por ejemplo, las personas que dan positivo en la PCR). Llamamos \\(p\\) a \\(P(A)\\) (la proporción poblacional de personas que dan positivo en la PCR). Tomamos muestras aleatorias simples de tamaño \\(n\\) de la población y contamos cuántos sujetos de la muestra son de \\(A\\). Esta variable aleatoria es binomial \\(B(n,p)\\). Tenemos el resultado siguiente. Teorema 1.2 Si \\(X\\) es una variable \\(B(n,p)\\): Su dominio es \\(D_X=\\{0,1,\\ldots,n\\}\\) Su función de densidad es \\[ f_X(k)=\\left\\{\\begin{array}{ll} \\displaystyle\\binom{n}{k}p^k(1-p)^{n-k} &amp; \\text{ si $k\\in D_X$}\\\\ 0 &amp; \\text{ si $k\\notin D_X$} \\end{array}\\right. \\] Su valor esperado es \\(E(X)=np\\) Su varianza es \\(\\sigma(X)^2=np(1-p)\\) Recordad que el número combinatorio \\[ \\binom{n}{k}=\\frac{\\overbrace{n\\cdot (n-1)\\cdots (n-k+1)}^k}{k\\cdot (k-1)\\cdots 2\\cdot 1}=\\frac{n!}{k!(n-k)!} \\] nos da el número de subconjuntos de \\(k\\) elementos de \\(\\{1,\\ldots,n\\}\\). El tipo de teorema anterior es el que hace que nos interese estudiar algunas familias distinguidas de variables aleatorias. Si, por ejemplo, reconocemos que una variable aleatoria es binomial y conocemos sus valores de \\(n\\) y \\(p\\) y sabemos el teorema anterior, automáticamente sabemos su función de densidad, y con ella su función de distribución, su valor esperado, su varianza etc., sin necesidad de deducir toda esta información cada vez que encontremos una variable de estas. Supongamos que efectuamos \\(n\\) repeticiones consecutivas e independientes de un experimento de Bernoulli de probabilidad de éxito \\(p\\) y contamos el número de éxitos \\(E\\); llamaremos \\(X\\) a la variable aleatoria resultante. Para seguir la demostración, si no os sentís muy cómodos con el razonamiento con \\(n\\)’s y \\(k\\)’s abstractos, vosotros id repitiéndolo tomando, por ejemplo, \\(n=4\\). Los posibles resultados son todas las palabras posibles de \\(n\\) letras formadas por \\(E\\)’s y \\(F\\)’s. Como los experimentos sucesivos son independientes, la probabilidad de cada una de estas palabras es el producto de las probabilidades de sus resultados individuales. Por lo tanto, si una palabra concreta tiene \\(k\\) letras \\(E\\) y \\(n-k\\) letras \\(F\\) (se han obtenido \\(k\\) éxitos y \\(n-k\\) fracasos), su probabilidad es \\(p^k(1-p)^{n-k}\\), independientemente del orden en el que hayamos obtenido los resultados. Para calcular la probabilidad de obtener una secuencia con \\(k\\) éxitos, sumaremos las probabilidades de obtener cada una de las secuencias de \\(k\\) letras. Como todas tienen la misma probabilidad, el resultado será la probabilidad de una palabra con \\(k\\) \\(E\\)’s y \\(n-k\\) \\(F\\)’s multiplicada por el número total de palabras diferentes con \\(k\\) \\(E\\)’s y \\(n-k\\) \\(F\\)’s. ¿Cuántas palabras hay con \\(k\\) \\(E\\)’s y \\(n-k\\) \\(F\\)’s? Cada una queda caracterizada por las posiciones de las \\(k\\) \\(E\\)’s, por lo tanto es el número de posibles elecciones de conjuntos de \\(k\\) posiciones para las \\(E\\)’s. Este es el número de posibles subconjuntos de \\(k\\) elementos (las posiciones donde habrá las \\(E\\)’s) de \\(\\{1,\\ldots,n\\}\\), que es el número combinatorio \\(\\binom{n}{k}\\). Por lo tanto ya tenemos \\[ P(X=k)=\\binom{n}{k}p^k(1-p)^{n-k}. \\] A partir de aquí, el cálculo del valor esperado y la varianza es sumar \\[ \\begin{array}{l} \\displaystyle E(X)=\\sum_{k=0}^n k\\cdot \\binom{n}{k}p^k(1-p)^{n-k}\\\\ \\displaystyle \\sigma(X)^2=\\sum_{k=0}^n k^2\\cdot \\binom{n}{k}p^k(1-p)^{n-k}-\\Big(\\sum_{k=0}^n k\\cdot \\binom{n}{k}p^k(1-p)^{n-k})^2 \\end{array} \\] Os podéis fiar de nosotros, dan \\(np\\) y \\(np(1-p)\\), respectivamente. El valor de \\(E(X)\\) es razonable. Veamos, si tomáis una muestra aleatoria de \\(n\\) sujetos de una población en la que la proporción de sujetos \\(E\\) es \\(p\\), ¿cuántos sujetos \\(E\\) “esperáis” obtener en vuestra muestra? Pues una proporción \\(p\\) de la muestra, es decir \\(p\\cdot n\\), ¿no? Conocer las propiedades de las variables aleatorias binomiales solo es útil si sabemos reconocer cuándo estamos ante una de ellas. Fijaos que en una variable aleatoria binomial: Contamos cuántas veces ocurre un suceso (el éxito \\(E\\)) en una secuencia de intentos. En cada intento, el suceso que nos interesa pasa o no pasa, sin términos medios. El número de intentos es fijo, \\(n\\). Cada intento es independiente de los otros. En cada intento, la probabilidad de que pase el suceso que nos interesa es siempre la misma, \\(p\\). Por ejemplo: Una mujer tiene 4 hijos. La probabilidad de que un hijo sea niña es fija, 0.51. El sexo de cada hijo es independiente de los otros. Contamos cuántas hijas tiene. Es una variable binomial \\(B(4,0.51)\\). En una aula hay 5 chicos y 45 chicas. Escojo 10 estudiantes, uno tras otro y sin repetirlos, para hacerles una pregunta. Cada elección es independiente de las otras. Cuento cuántos chicos he interrogado. No es una variable binomial: como no podemos repetir, en cada ronda la probabilidad de escoger un chico depende del sexo de los estudiantes elegidos antes que él. Por lo tanto la \\(p\\) no es la misma en cada elección. Por ejemplo, en la primera ronda la probabilidad de elegir un chico es 5/50=0.1. Ahora, si en la primera ronda sale elegido un chico, la probabilidad de que en la segunda ronda volvamos a elegir un chico se reduce a 4/49=0.0816, mientras que si sale elegida una chica, esta probabilidad es 5/49=0.102. En una aula hay 5 chicos y 45 chicas. Escojo 10 estudiantes, uno tras otro pero cada estudiante puede ser elegido más de una vez, para hacerles una pregunta. Cada elección es independiente de las otras. Cuento cuántos chicos he interrogado. Ahora sí que es una variable binomial \\(B(10,0.9)\\). En una aula hay 5 chicos y 45 chicas. Escojo estudiantes uno tras otro y cada estudiante puede ser elegido más de una vez, para hacerles una pregunta. Cada elección es independiente de las otras. Cuento cuántos estudiantes he tenido que elegir para interrogar a 5 chicos. No es una variable binomial: no cuenta el número de éxitos en una secuencia de un número fijo de intentos, sino cuántos intentos necesito para llegar a un número fijo de éxitos. En una aula hay 5 chicos y 45 chicas. Lanzo una moneda equilibrada: si sale cara escojo 10 estudiantes y si sale cruz escojo 20, para hacerles una pregunta. Tanto en un caso como en el otro, los elijo uno tras otro pero cada estudiante puede ser elegido más de una vez y cada elección es independiente de las otras. Cuento cuántos chicos he interrogado. No es una variable binomial: el número de intentos no es fijo. La probabilidad de que un día de noviembre llueva es de un 32%. Escogemos una semana de noviembre y contamos cuántos días ha llovido. No es de una variable binomial. Aunque cada día tenga la misma probabilidad de lluvia, que llueva un día no es independiente de que llueva el anterior. En España hay 46,700,000 personas, de las cuales un 11.7% son diabéticos. Escogemos 100 españoles diferentes al azar (de manera independiente unos de otros) y contamos cuántos son diabéticos. No es binomial, pero prácticamente sí que lo es, porque las probabilidades apenas varían de una elección a la siguiente. En este caso haremos la trampa de considerarla binomial. Recordad que cuando discutíamos sobre muestras aleatorias, decíamos que si tomamos una muestra aleatoria sin reposición de una población muchísimo más grande que la muestra, a efectos prácticos podíamos considerarla simple, porque, total, si hubiéramos permitido repeticiones, casi seguro que no se habrían dado. Pues aquí igual. ¿Cómo efectuar cálculos con una variable aleatoria de una familia dada? Una posibilidad es usar una aplicación de móvil o tablet. Nuestra favorita es Probability distributions, disponible tanto para Android como para iOS. Figura 1.3: La app Probability Distributions. Otra posibilidad es usar R. R conoce todas la distribuciones de variables aleatorias importantes; por ejemplo, para R la binomial es binom. Entonces Añadiendo al nombre de la distribución el prefijo d, tenemos su función de densidad: de la binomial, dbinom. Añadiendo al nombre de la distribución el prefijo p, tenemos su función de distribución: de la binomial, será pbinom. Añadiendo al nombre de la distribución el prefijo q, tenemos sus cuantiles: para la binomial, qbinom. Añadiendo al nombre de la distribución el prefijo r, tenemos una función que produce muestra aleatorias de números con esa distribución de probabilidad: para la binomial, rbinom. Estas funciones se aplican al argumento de la función y los parámetros de la variable aleatoria en su orden usual (todo entre paréntesis y separados por comas). Por ejemplo, para la binomial, se aplican a (argumento, \\(n\\), \\(p\\)). Veamos ejemplos de la binomial. Si lanzamos 20 veces un dado equilibrado (de 6 caras), ¿cuál es la probabilidad de sacar exactamente 5 unos? Si llamamos \\(X\\) a la variable aleatoria que cuenta el número de unos en secuencias de 20 lanzamientos de un dado equilibrado, se trata de una variable binomial \\(B(20,1/6)\\). Nos piden \\(P(X=5)\\), y esta probabilidad nos la da la función de densidad de \\(X\\). Es \\(f_X(5)\\): dbinom(5,20,1/6) ## [1] 0.1294103 Si lanzamos 20 veces un dado equilibrado, ¿cuál es la probabilidad de sacar como máximo 5 unos? Con las notaciones anteriores, nos piden \\(P(X\\leq 5)\\), y esta probabilidad nos la da la función de distribución de \\(X\\). Es \\(F_X(5)\\): pbinom(5,20,1/6) ## [1] 0.8981595 Si lanzamos 20 veces un dado equilibrado, ¿cuál es la probabilidad de sacar 5 unos o más? Con las notaciones anteriores, nos piden \\(P(X\\geq 5)=1-P(X\\leq 4)=1-F_X(4)\\): 1-pbinom(4,20,1/6) ## [1] 0.2312508 Si lanzamos 20 veces un dado equilibrado, ¿cuál es el primer número de unos \\(N\\) para el que la probabilidad de sacar como máximo \\(N\\) unos llega al 25%? Nos piden el primer valor \\(N\\) tal que \\(P(X\\leq N)\\geq 0.25\\), y esto por definición es el 0.25-cuantil de \\(X\\): qbinom(0.25,20,1/6) ## [1] 2 Veamos que en efecto \\(N=2\\) cumple lo pedido: la probabilidad de sacar como máximo 2 unos es pbinom(2,20,1/6) ## [1] 0.3286591 y la probabilidad de sacar como máximo 1 uno es pbinom(1,20,1/6) ## [1] 0.1304203 Vemos por tanto que con 1 uno no llegamos al 25% de probabilidad y con 2 sí. Queremos simular 50 rondas de lanzar 20 veces un dado equilibrado y contar los unos, es decir, queremos una muestra aleatoria de tamaño 50 de nuestra variable \\(X\\): rbinom(50,20,1/6) ## [1] 6 3 5 0 6 2 4 4 4 4 2 5 3 4 2 4 2 3 4 1 5 0 2 3 4 3 2 5 3 5 4 4 1 3 3 2 1 2 ## [39] 2 7 1 4 3 3 2 5 4 2 1 3 Cada vez que repitamos esta instrucción obtendremos una muestra aleatoria nueva: rbinom(50,20,1/6) ## [1] 1 2 4 3 3 5 4 1 5 0 5 6 4 4 2 4 2 3 2 0 3 7 5 2 3 4 4 6 3 1 2 5 8 5 3 0 2 5 ## [39] 2 3 3 3 2 2 3 6 5 3 5 4 rbinom(50,20,1/6) ## [1] 4 6 5 3 4 4 0 2 5 3 2 5 4 4 3 4 2 4 3 4 4 2 3 1 6 4 4 4 3 2 5 3 1 2 4 4 5 7 ## [39] 5 7 3 1 3 4 5 3 4 9 3 5 rbinom(50,20,1/6) ## [1] 8 7 4 2 3 5 5 3 3 4 1 4 4 1 5 3 5 6 0 4 6 7 3 5 7 3 0 3 7 1 4 2 6 5 5 4 4 0 ## [39] 3 6 3 3 2 3 2 2 4 5 3 4 Veamos algunos gráficos de la función densidad de variables aleatorias binomiales. Primero, para \\(n=10\\) y diferentes valores de \\(p\\). Ahora para \\(n=100\\): Por cierto, R también tiene una función para calcular la probabilidad de que se dé alguna repetición en una muestra aleatorias simple de un tamaño dado. En concreto: La instrucción pbirthday(n,N) nos da la probabilidad de que en una muestra aleatoria simple de tamaño n de una población de tamaño N haya algún elemento repetido. La instrucción qbirthday(p,N) nos da el tamaño mínimo de una muestra aleatoria simple de una población de tamaño N para que la probabilidad de que haya algún elemento repetido sea \\(\\geq p\\). El nombre birthday hace referencia a la paradoja del cumpleaños: el típico problema de calcular la probabilidad de que dos estudiantes de una clase celebren el cumpleaños el mismo día y asombrarse de que en una clase de 50 estudiantes haya más de un 95% de probabilidades de que haya algún cumpleaños repetido. En efecto, podemos entender una clase de 50 estudiantes como una muestra aleatoria simple de 50 fechas de nacimiento, escogidas de un conjunto de 366 posibles fechas (los 366 días de un año bisiesto). La probabilidad de que al menos 2 estudiantes celebren el cumpleaños el mismo día es la probabilidad de que se dé al menos una repetición en esta muestra. R lo calcula con: pbirthday(50,366) ## [1] 0.9700731 ¿Cuál es el número mínimo de estudiantes en la clase para que la probabilidad de que se repita una fecha de cumpleaños sea del 95% o más? 1.6.2 Variables aleatorias hipergeométricas Recordad que el paradigma de variable aleatoria binomial es: tengo una población con una proporción \\(p\\) de sujetos que satisfacen una condición \\(E\\), tomo una muestra aleatoria simple de tamaño \\(n\\) y cuento el número de sujetos \\(E\\) en mi muestra. Si cambiamos “muestra aleatoria simple” por “muestra aleatoria sin reposición”, la distribución de la variable aleatoria que obtenemos es otra: la hipergeométrica. Una variable aleatoria es hipergeométrica (o tiene distribución hipergeométrica) de parámetros \\(N\\), \\(M\\) y \\(n\\) (\\(H(N,M,n)\\), para abreviar) es cualquier variable aleatoria \\(X\\) que podáis identificar con el proceso siguiente: Tenemos una población formada por \\(N\\) sujetos que satisfacen una condición \\(E\\) y \\(M\\) sujetos que no la satisfacen (por lo tanto, en total, \\(N+M\\) sujetos en la población), tomo una muestra aleatoria sin reposición de tamaño \\(n\\) y cuento el número de sujetos \\(E\\) en mi muestra. Llamaremos a \\(N\\) el número poblacional de éxitos, a \\(M\\) el número poblacional de fracasos y a \\(n\\) el tamaño de las muestras. Fijaos entonces que \\(N+M\\) el tamaño total de la población (la suma de los que satisfacen \\(E\\) y los que no la satisfacen) y que \\(N/(N+M)\\) es la probabilidad poblacional de éxito (la fracción de sujetos que satisfacen \\(E\\) en el total de la población). Con R, igual que la distribución binomial era binom, la distribución hipergeométrica es hyper y los parámetros que se le han de entrar son \\(N,M,p\\), en este orden. Teorema 1.3 Si \\(X\\) es una variable \\(H(N,M,n)\\): Su dominio es \\(D_X=\\{0,1,\\ldots,\\text{min}(N,n)\\}\\) Su función de densidad es \\[ f_X(k)=\\left\\{\\begin{array}{ll} \\displaystyle\\dfrac{\\binom{N}{k}\\cdot \\binom{M}{n-k}}{\\binom{N+M}{n}} &amp; \\text{ si $k\\in D_X$}\\\\ 0 &amp; \\text{ si $k\\notin D_X$} \\end{array}\\right. \\] Su valor esperado es \\(E(X)=\\dfrac{nN}{N+M}\\) Su varianza es \\(\\sigma(X)^2=\\dfrac{nNM(N+M-n)}{(N+M)^2(N+M-1)}\\) Fijaos que si llamamos \\(p\\) a la probabilidad poblacional de éxito, \\(p=N/(N+M)\\), entonces \\[ E(X)=np. \\] Es la misma fórmula que para las variables binomiales \\(B(n,p)\\) (y si lo pensáis un rato veréis que, de nuevo y por el mismo argumento, es lo razonable). Por otro lado, si llamamos \\(\\mathbf{P}\\) al tamaño de la población, \\(\\mathbf{P}=N+M\\), entonces \\[ \\sigma(X)^2=np(1-p)\\cdot\\dfrac{\\mathbf{P}-n}{\\mathbf{P}-1} \\] que es la varianza de una variable \\(B(n,p)\\) multiplicada por un valor debido al hecho de que ahora tomamos muestras sin repetición y la varianza es más pequeña que si las tomamos con repetición. A este factor \\((\\mathbf{P}-n)/(\\mathbf{P}-1)\\) se le llama factor de población finita. Fijaos en que si \\(\\mathbf{P}\\) es muchísimo más grande que \\(n\\), tendremos que \\(\\mathbf{P}-n\\approx \\mathbf{P}-1\\) y por lo tanto \\((\\mathbf{P}-n)/(\\mathbf{P}-1)\\approx 1\\) y la varianza de la hipergeométrica será aproximadamente la de la binomial. Esto es consistente con lo que ya hemos comentado: si la población es mucho más grande que la muestra, tomar las muestras con o sin reposición no afecta demasiado a las muestra obtenidas, por lo que la distribución de probabilidad ha de ser muy parecida. Recordad los ejemplos siguientes: En España hay 46,700,000 personas, de las cuales un 11.7% son diabéticos. Escogemos 100 españoles y contamos cuántos son diabéticos. Esta variable es, en realidad, hipergeométrica con \\(N=0.117\\cdot 46700000=5463900\\), \\(M=46700000-N=41236100\\) y \\(n=100\\), pero en la práctica la consideramos binomial \\(B(100,0.117)\\). El factor de población finita es \\[ \\frac{46700000-100}{46700000-1}=0.9999979 \\] En cambio: En una aula hay 5 chicos y 45 chicas. Escogemos 10 estudiantes, uno tras otro y sin repetirlos, para hacerles una pregunta. Cada elección es independiente de las otras. Contamos cuántos chicos hemos interrogado. Esta variable es, en realidad, hipergeométrica \\(H(5,45,10)\\). El factor de población finita en esta caso no es aproximadamente 1: da \\[ \\frac{50-10}{50-1}=0.8163 \\] No es correcto aproximarla por una binomial \\(B(10,0.1)\\). El gráfico siguiente compara la función de densidad de una \\(B(10,0.1)\\) con las de unas hipergeométricas \\(H(5,45,10)\\), \\(H(50,450,10)\\) y \\(H(5000,45000,10)\\) para que veáis cómo a medida que el tamaño de la población crece (manteniendo la probabilidad poblacional de éxito), la hipergeométrica se aproxima a la binomial. 1.6.3 Variable aleatorias de Poisson Una variable aleatoria \\(X\\) es de Poisson (o tiene distribución de Poisson) con parámetro \\(\\lambda&gt;0\\) (\\(Po(\\lambda)\\), para abreviar) cuando: Su dominio es \\(D_X=\\mathbb{N}\\), el conjunto de todos los números naturales (es decir, puede tomar como valor cualquier número natural). Su función de densidad es \\[ f_X(k)=\\left\\{\\begin{array}{ll} e^{-\\lambda}\\cdot \\dfrac{\\lambda^k}{k!} &amp; \\text{ si $k\\in \\mathbb{N}$}\\\\ 0 &amp; \\text{ si $k\\notin \\mathbb{N}$} \\end{array}\\right. \\] Para R, la distribución Poisson es pois. Si \\(X\\) es una variable \\(Po(\\lambda)\\), entonces \\[ E(X)= \\sigma(X)^2= \\lambda \\] Es decir, el “parámetro” \\(\\lambda\\) de una variable de Poisson es su valor esperado, y coincide con su varianza. Suponemos que os estáis preguntando: ¿para qué nos sirve definir una variable de Poisson mediante su densidad, si lo que nos interesa es poder clasificar una variable como de Poisson (o binomial, o hipergeométrica etc.) para así saber “gratis” su densidad? Bueno, la respuesta es que la familia Poisson incluye un tipo de variables aleatorias muy común en la práctica. Supongamos que tenemos un tipo de objetos que pueden darse en una región continua de tiempo o espacio. Por ejemplo, defunciones de personas por una determinada enfermedad en el decurso del tiempo, defunciones de personas por una determinada enfermedad en diferentes zonas geográficas de un país, o números de bacterias en trozos de una superficie. Para simplificar el lenguaje, vamos a suponer que observamos apariciones de estos objetos en el tiempo. Supongamos además que las apariciones de estos objetos satisfacen las propiedades siguientes: Las apariciones de los objetos son aleatorias: en cada instante del tiempo, un objeto se da, o no, al azar, con una probabilidad fija y constante. Las apariciones de los objetos son independientes: que se dé un objeto en un instante del tiempo concreto, no depende para nada de que se haya dado o no un objeto en otro instante del tiempo. Las apariciones de los objetos no son simultáneas: es prácticamente imposible que dos objetos de estos se superpongan (aparezcan en el mismo instante exacto del tiempo). En esta situación, la variable \\(X_t\\) que toma un intervalo de tiempo de tamaño \\(t\\) y cuenta el número de objetos en él es \\(Po(\\lambda_t)\\), con \\(\\lambda_t\\) el número esperado de objetos en este intervalo de tiempo (es decir, el número medio de objetos en intervalos de tiempo de este tamaño). Por ejemplo, cuando lo que cuentan ocurre al azar, son variables de Poisson: El número de enfermos admitidos en urgencias en un día (o en 12 horas, o en una semana…) El número de defunciones por una enfermedad concreta en un día (o en una semana, o en un año…) El número de bacterias en un cuadrado de 1 cm de lado (o de 1 m de lado…) Fijaos en que este tipo de conocimiento nos sirve para dos cosas: Si sabemos que estas variables son Poisson, conocemos su densidad y por lo tanto podemos calcular lo que queramos para ellas. Si los datos que observamos no parece que sigan una distribución de Poisson (por ejemplo, porque su varianza sea muy diferente de su media), entonces lo que cuentan no ocurre al azar y es señal de que algo “no aleatorio” está pasando. Ejemplo 1.7 Observad la diferencia entre las dos variables siguientes: Número semanal de defunciones por un tipo de cáncer en un país. El momento exacto de las defunciones se produce al azar, podemos entender que no se dan dos defunciones exactamente en el mismo instante, con precisión infinita, y las defunciones se producen de manera independiente. Es Poisson. Número semanal de defunciones en accidentes de tráfico en un país. De nuevo, el momento exacto de las defunciones se produce al azar y podemos entender que no se dan dos defunciones exactamente en el mismo instante, con precisión infinita. Pero las muertes en accidentes de tráfico no son independientes: en un mismo accidente mortal se pueden producir varias muertes en un corto espacio de tiempo, las condiciones metereológicas o de alguna carretera pueden hacer que aumente durante un cierto período de tiempo la probabilidad de accidente mortal, etc. No es Poisson. Como las apariciones de los objetos que cuenta una variable de Poisson son aleatorias e independientes, el número medio de objetos es lineal en el tamaño de la región. Por ejemplo, si se diagnostican de media 32,240 casos de cáncer de colon anuales en España (y siguen una ley de Poisson), esperamos que de media se diagnostiquen 32240/52=620 casos semanales. 1.7 Test (1) Sea \\(X\\) una variable aleatoria discreta de media \\(\\mu\\) y desviación típica \\(\\sigma\\). ¿Cuáles de las afirmaciones siguientes son siempre verdaderas? \\(E(X+2)=\\mu\\). \\(\\sigma(X+2)^2=\\sigma^2\\). \\(\\sigma(-X)^2=-\\sigma^2\\). \\(\\sigma(-X)^2=-\\sigma\\). \\(\\sigma(X/2)^2=\\sigma^2/4\\). Ninguna de las otras afirmaciones es verdadera. (2) La función de distribución \\(F_X(x)\\) de una variable aleatoria \\(X\\) nos da: La probabilidad de obtener el valor \\(x\\). La probabilidad de obtener un valor entre \\(-x\\) y \\(x\\). La probabilidad de obtener un valor entre \\(0\\) y \\(x\\). La probabilidad de obtener un valor menor o igual que \\(x\\). La probabilidad de obtener un valor estrictamente menor que \\(x\\). (3) La incidencia anual de un cierto accidente laboral sigue una distribución de Poisson. A lo largo del tiempo se ha observado que el 55% de los años no se produce ningún accidente. ¿Qué valor estimáis que tiene el parámetro \\(\\lambda\\) de dicha distribución de Poisson? 0.55 \\(e^{-0.55}\\) \\(\\ln(0.55)\\) \\(-\\ln(0.55)\\) Un valor que no es ninguno de los propuestos en las otras respuestas. (4) Un tratamiento T cura el 20% de los enfermos de una enfermedad X. Marcad todas las afirmaciones verdaderas. La distribución del número de individuos que se curan con el tratamiento T en una muestra aleatoria de 100 enfermos de X es aproximadamente simétrica. La distribución del número de individuos que se curan con el tratamiento T en una muestra aleatoria de 100 enfermos de X es aproximadamente asimétrica a la izquierda. La distribución del número de individuos que se curan con el tratamiento T en una muestra aleatoria de 100 enfermos de X sigue una distribución binomial. La probabilidad de que T cure dos enfermos de X escogidos al azar es 0.4. En una muestra aleatoria de 50 enfermos de X, esperamos que T cure 10. Ninguna de las otras afirmaciones es verdadera. (5) ¿Cuáles de las variables siguientes tienen una distribución binomial? El peso de una persona elegida al azar. Escogemos un número de lanzamientos al azar, lanzamos ese número de veces una moneda al aire, y contamos el número de caras. El número de glóbulos rojos en 1 mm3 de sangre. La proporción de hipertensos en una muestra aleatoria de 50 individuos. Escogemos 10 estudiantes diferentes en una clase de 20, y contamos cuántas mujeres han salido. Ninguna de ellas. (26 ¿Cuáles de las variables siguientes tienen una distribución de Poisson? El peso de una persona elegida al azar. El número de muertes por km2 debidos a una enfermedad infecciosa. El número de glóbulos rojos en 1 mm3 de sangre. La proporción de hipertensos en una muestra aleatoria de 50 individuos. Escogemos 10 estudiantes diferentes en una clase de 20, y contamos cuántas mujeres han salido. "],
["variables-aleatorias-continuas.html", "Lección 2 Variables aleatorias continuas 2.1 Densidad y distribución 2.2 Esperanza, varianza, cuantiles… 2.3 Variables aleatorias normales 2.4 Test", " Lección 2 Variables aleatorias continuas Cuando una variable aleatoria discreta puede tomar pocos valores, la probabilidad de cada valor es relevante. Pero cuando puede tomar muchos valores, las probabilidades individuales pueden ser muy pequeñas y entonces lo que más nos interesa es la probabilidad de intervalos de valores. En este curso nos vamos a restringir variables aleatorias continuas \\(X: \\Omega\\to \\mathbb{R}\\) que satisfacen la siguiente propiedad extra: su función de distribución \\[ \\begin{array}{rcl} F_X: \\mathbb{R} &amp; \\to &amp; [0,1]\\\\ x &amp;\\mapsto &amp;P(X\\leq x) \\end{array} \\] es continua. Resulta entonces que, si \\(X\\) es una variable aleatoria continua, la probabilidad de que tome cada valor concreto es 0 \\[ P(X=a)=0 \\text{ para todo $a\\in \\mathbb{R}$}. \\] En particular, para una variable aleatoria continua: Probabilidad 0 no significa imposible. Cada valor de \\(X\\) tiene probabilidad 0, pero cuando tomamos un sujeto, tendrá algún valor de \\(X\\), ¿no?. Por lo tanto, su valor de \\(X\\) es posible, aunque tenga probabilidad 0. De \\(P(X=a)=0\\) se deduce que la probabilidad de un suceso definido con una desigualdad es exactamente la misma que la del suceso correspondiente definido con una desigualdad estricta, y en particular, contrariamente a lo que comentábamos para las variables aleatorias discretas \\[ P(X\\leq a)=P(X&lt;a) \\] porque \\(P(X\\leq a)=P(X&lt;a)+P(X=a)=P(X&lt;a)+0=P(X&lt;a)\\). Otros ejemplos: \\(P(X\\geq a)=P(X&gt; a)+P(X=a)=P(X&gt; a)\\) \\(P(a\\leq X\\leq a)=P(a&lt;X&lt;b)+P(X=a)+P(X=b)\\) \\(=P(a&lt;X&lt;b)\\) 2.1 Densidad y distribución Sea \\(X\\) una variable aleatoria continua. Como ya hemos dicho, su función de distribución \\(F_X\\) se sigue definiendo como \\[ a\\mapsto F_X(a)=P(X\\leq a) \\] Pero pùesto que tenemos que \\(P(X=a)=0\\), ahora no podemos definir la función de densidad de \\(X\\) como \\(f_X(a)=P(X=a)\\). ¿Qué podemos hacer? Recordad que, en las variables aleatorias discretas \\[ F_X(a)=\\sum_{x\\leq a} f_X(x) \\] En el contexto de matemáticas “continuas”, la suma \\(\\sum\\) se traduce en la integral \\(\\int\\). Se define entonces la función de densidad de una variable aleatoria continua \\(X\\) como la función \\(f_X:\\mathbb{R}\\to \\mathbb{R}\\) tal que \\(f_X(x)\\geq 0\\), para todo \\(x\\in \\mathbb{R}\\), y \\[ F_X(a)=\\int_{-\\infty}^a f_{X}(x)\\, dx\\quad \\text{para todo $a\\in \\mathbb{R}$.} \\] Recordad (o saber por primera vez) que la integral tiene una interpretación sencilla en términos de áreas. En concreto, dados \\(a\\in \\mathbb{R}\\) y una función “integrable” \\(f(x)\\), la integral \\[ \\int_{-\\infty}^a f_{X}(x)\\, dx \\] es igual al área de la región a la izquierda de la recta vertical \\(x=a\\) comprendida entre la curva \\(y=f(x)\\) y el eje de abscisas \\(y=0\\). Por lo tanto, la función de densidad \\(f_X\\) de \\(X\\) es la función tal que para todo \\(a\\in \\mathbb{R}\\), \\(F_X(a)\\) es igual al área bajo la curva \\(y=f_X(x)\\) (entre esta curva y el eje de abscisas) a la izquierda de \\(x=a\\). ¿Cuál es la idea intuitiva que hay bajo esta definición de densidad? Suponed que dibujamos histogramas de frecuencias relativas de los valores de \\(X\\) sobre toda la población. Recordad que, en un histograma de estos, la frecuencia relativa (la probabilidad) de cada clase es la amplitud de la clase por la altura de su barra, a la que llamábamos la densidad de la clase (y por lo tanto, algo tendrá que ver con la densidad de \\(X\\), ¿no creéis?). Si dibujamos los histogramas de \\(X\\) tomando clases cada vez más estrechas, sus polígonos de frecuencias tienden a dibujar una curva, que hemos coloreado en rojo en el último histograma: Cuando hacemos que el ancho de las clases tienda a 0, obtenemos una curva que es el límite de estos polígonos de frecuencias: Esta curva es precisamente \\(y=f_X(x)\\). Es decir, la función de densidad \\(f_X\\) de una variable aleatoria continua \\(X\\) es la función límite de los polígonos de frecuencias de histogramas de \\(X\\) cuando hacemos que la amplitud de las clases tienda a 0. Como \\(P(X\\leq a)\\) es el área bajo la curva \\(y=f_X(x)\\) a la izquierda de \\(x=a\\), \\[ \\begin{array}{rl} P(a\\leq X\\leq b)\\!\\!\\!\\! &amp; =P(X\\leq b)-P(X&lt;a)\\\\ &amp;=P(X\\leq b)-P(X\\leq a) \\end{array} \\] es el área bajo la curva \\(y=f_X(x)\\) a la izquierda de \\(x=b\\) menos el área bajo la curva \\(y=f_X(x)\\) a la izquierda de \\(x=a\\), es decir, \\(P(a\\leq X\\leq b)\\) es igual al área bajo la curva \\(y=f_X(x)\\) entre \\(x=a\\) y \\(x=b\\). Como \\(P(X&lt;\\infty)=P(\\Omega)=1\\), el área total bajo la curva \\(y=f_X(x)\\) es 1. Sabemos que \\(P(X=a)=0\\), pero si \\(\\varepsilon&gt;0\\) es muy, muy pequeño, el área bajo \\(y=f_X(x)\\) entre \\(a-\\varepsilon\\) y \\(a+\\varepsilon\\) es aproximadamente \\(2\\varepsilon\\cdot f_X(a)\\). Por lo tanto \\(f_X(a)\\) nos da una indicación de la probabilidad de que \\(X\\) valga aproximadamente \\(a\\) (pero no es \\(P(X=a)\\), que vale 0). Es decir, por ejemplo, si \\(f_X(a)=0.1\\) y \\(f_X(b)=0.5\\), la probabilidad de que \\(X\\) tome un valor muy cercano a \\(a\\) es 5 veces mayor que la probabilidad de que tome un valor muy cercano a \\(b\\). Pero \\(P(X=a)=P(X=b)=0\\), así que, por favor, evitad decir que “la probabilidad de que \\(X\\) valga \\(a\\) es 5 veces mayor que la probabilidad de que valga \\(b\\)”. Sí, ya sabemos que \\(5\\cdot 0=0\\), pero la frase es engañosa. 2.2 Esperanza, varianza, cuantiles… La esperanza y la varianza de una variable aleatoria continua \\(X\\), con función de densidad \\(f_X\\), se definen como en el caso discreto, substituyendo la suma \\(\\sum_{x\\in D_x}\\) por una integral. La esperanza (media, valor esperado…) de \\(X\\) es \\[ E(X)=\\int_{-\\infty}^{\\infty}x \\cdot f_{X}(x)\\, dx \\] También se escribe \\(\\mu_X\\) o simplemente \\(\\mu\\). Este valor tiene la misma interpretación que en el caso discreto: Representa el valor medio de \\(X\\) sobre el total de la población Es (con probabilidad 1) el límite de la media aritmética de los valores de \\(X\\) sobre muestras aleatorias simples de tamaño \\(n\\), cuando \\(n\\to \\infty\\). Si \\(g:\\mathbb{R}\\to \\mathbb{R}\\) es una función continua, la esperanza de \\(g(X)\\) es \\[ E(g(X))=\\int_{-\\infty}^{+\\infty} g(x) f_X(x)dx \\] La varianza de \\(X\\) es \\[ Var(X)=E((X-E(X))^2) \\] y se puede demostrar que es igual a \\[ Var(X)=E(X^2)-E(X)^2 \\] También se escribe \\(\\sigma_X^2\\) o simplemente \\(\\sigma^2\\). La desviación típica de \\(X\\) es \\[ \\sigma(X)=+\\sqrt{Var(X)} \\] y también se escribe \\(\\sigma_X\\) o \\(\\sigma\\). Como en el caso discreto, la varianza y la desviación típica miden la variabilidad de los resultados de \\(X\\) respecto de su valor medio. Estos parámetros de \\(X\\) tienen las mismas propiedades en el caso continuo que en el discreto. Las recordamos: \\(E(b)=b\\), si \\(b\\) es una variable aleatoria constante. \\(E(a X+b)=a E(X)+b\\). \\(E(X+Y)=E(X)+E(Y)\\). Si \\(X\\leq Y\\), entonces \\(E(X)\\leq E(Y)\\). \\(Var(aX+b)=a^2 Var(X)\\), donde \\(a,b\\) son constantes reales. \\(\\sigma(aX+b)=|a|\\cdot \\sigma(X)\\). \\(Var(b)=0\\), si \\(b\\) es una variable aleatoria constante \\(Var(X+Y)=Var(X)+Var(Y)\\) si \\(X,Y\\) son independientes El cuantil de orden \\(p\\) (o \\(p\\)-cuantil) de una variable aleatoria continua \\(X\\) es el valor \\(x_p\\in \\mathbb{R}\\) más pequeño tal que \\[ F_X(x_p)=P(X\\leq x_p)=p \\] Fijaos en que como \\(F_X\\) es continua, tiende a 0 cuando \\(x\\to -\\infty\\) y tiende a 1 cuando \\(x\\to -\\infty\\), por el Teorema del Valor medio de las funciones continuas (que dice, básicamente, que las funciones continuas no dan saltos) toma todos los valores en (0,1) y por lo tanto dado cualquier \\(p\\in (0,1)\\) existe algún \\(x\\) tal que \\(F_X(x)=p\\). La mediana de \\(X\\) es su 0.5-cuantil, el primer y tercer cuartiles son su 0.25-cuantil y su 0.75-cuantil, etc. 2.3 Variables aleatorias normales 2.3.1 Propiedades básicas Una variable aleatoria continua \\(X\\) es normal (o tiene distribución normal) de parámetros \\(\\mu\\) y \\(\\sigma\\) (\\(N(\\mu,\\sigma)\\), para abreviar) cuando su función de densidad es \\[ f_{X}(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma} e^{{-(x-\\mu)^2}/{2\\sigma^{2}}} \\mbox{ para todo } x\\in \\mathbb{R} \\] Naturalmente, no os tenéis que saber esta fórmula. Pero sí que tenéis que saber que: Una variable aleatoria normal \\(X\\) es continua, y por lo tanto \\(P(X=x)=0\\), \\(P(X\\leq x)=P(X&lt;x)\\) etc. Si \\(X\\) es \\(N(\\mu,\\sigma)\\), entonces su valor esperado es \\(E(X)=\\mu\\) y su desviación típica es \\(\\sigma_X=\\sigma\\). Una variable aleatoria normal es típica (o estándar) cuando tiene \\(\\mu=0\\) y \\(\\sigma=1\\); la denotaremos usualmente por \\(Z\\). Por lo tanto, si \\(Z\\) es \\(N(0,1)\\), \\(E(Z)=0\\) y \\(\\sigma(Z)=1\\). La gráfica de la densidad de una variable aleatoria normal es la conocida campana de Gauss: La distribución normal es una distribución teórica, no la encontraréis exacta en la vida real. Y pese a su nombre, no es más “normal” que otras distribuciones continuas. Su importancia se debe a que muchas distribuciones de la vida real son aproximadamente normales, porque: Toda variable aleatoria que consista en tomar \\(n\\) medidas independientes de una o varias variables aleatorias y sumarlas, tiene distribución aproximadamente normal cuando \\(n\\) es muy grande, aunque las variables aleatorias de partida no sean normales Ejemplo 2.1 Una variable binomial \\(B(n,p)\\) se obtiene tomando \\(n\\) medidas independientes de una variable Bernoulli \\(B(1,p)\\) y sumando los resultados. Por lo tanto, por la “regla” anterior, una \\(B(n,p)\\) tendría que ser aproximadamente normal si \\(n\\) es grande. Pues sí, si \\(n\\) es grande (pongamos mayor que 100, aunque si \\(p\\) está lejos de 0 o 1 el tamaño de las muestras puede ser mucho menor), la distribución de una variable \\(X\\) binomial \\(B(n,p)\\) se acerca mucho a la de una normal \\(N(np,\\sqrt{np(1-p)})\\), donde, recordad que si \\(X\\) es \\(B(n,p)\\), entonces \\(\\mu_X=np\\) y \\(\\sigma_X=\\sqrt{np(1-p)}\\). Por ejemplo, el gráfico siguiente compara las funciones de distribución de una binomial \\(B(50,0.3)\\) y una normal \\(N(50\\cdot 0.3,\\sqrt{50\\cdot 0.3\\cdot 0.7})\\). Para calcular probabilidades de una \\(N(\\mu,\\sigma)\\), hay que calcular las integrales a mano. O podéis usar R o alguna aplicación para móvil o tablet. Par R, la normal es norm. Así, por ejemplo, si \\(X\\) es \\(N(1,2)\\) \\(P(X\\leq 1.5)\\) es pnorm(1.5,1,2) ## [1] 0.5987063 El 0.4-cuantil de \\(X\\), es decir, el valor \\(q\\) tal que \\(P(X\\leq q)=0.4\\) es qnorm(0.4,1,2) ## [1] 0.4933058 \\(P(X=1.5)\\) es dnorm(1.5,1,2) ## [1] 0.1933341 ¡No! Como \\(X\\) es continua, \\(P(X=1.5)=0\\). Lo que os da dnorm(1.5,1,2) es el valor de la función de densidad de \\(X\\) en 1.5, que no creemos que os interese mucho. Una de las propiedades clave de la distribución normal es su simetría: Si \\(X\\) es \\(N(\\mu,\\sigma)\\), su densidad \\(f_X\\) es simétrica respecto de \\(x=\\mu\\), es decir, \\[ f_{X}(\\mu-x)=f_{X}(\\mu+x), \\] y tiene el máximo en \\(x=\\mu\\). Decimos entonces que \\(\\mu\\) es la moda de \\(X\\). Recordemos que no tiene sentido definir la moda de una variable continua \\(X\\) como el valor \\(x_0\\) tal que \\(P(X=x_0)\\) sea máximo, porque \\(P(X=x)=0\\) para todo \\(x\\in \\mathbb{R}\\). Se define entonces la moda de una variable continua \\(X\\) como el valor (o los valores) \\(x_0\\) tal(es) que \\(f_X(x_0)\\) es máximo. Como \\(f_X(x_0)\\) mide, como hemos visto, la probabilidad de que \\(X\\) vala aproximadamente \\(x_0\\), tenemos que la moda de \\(X\\) es el valor que maximiza la probabilidad de que \\(X\\) sea aproximadamente igual a él. En particular, si \\(Z\\) es \\(N(0,1)\\), entonces \\(f_Z\\) es simétrica alrededor de \\(x=0\\), es decir, \\(f_{Z}(-x)=f_{Z}(x)\\), y la moda de \\(Z\\) es \\(x=0\\) Recordad que la función de distribución de una variable aleatoria continua \\(X\\), \\[ P(X\\leq x)=F_X(x) \\] es el área comprendida entre la densidad \\(y=f_X(x)\\) y el eje de abscisas a la izquierda de \\(x\\). Entonces, la simetría de \\(f_X\\) hace que, para todo \\(x\\in \\mathbb{R}\\), las áreas a la izquierda de \\(\\mu-x\\) y a la derecha de \\(\\mu+x\\) sean iguales. Es decir, \\[ P(X\\leq \\mu-x)=P(X\\geq \\mu+x)=1-P(X\\leq \\mu+x) \\] En particular (tomando \\(x=0\\)) \\[ P(X\\leq \\mu)=1-P(X\\leq \\mu)\\Rightarrow P(X\\leq \\mu)=0.5 \\] y por lo tanto, \\(\\mu\\) es también la mediana de \\(X\\). Si \\(X\\) es \\(N(\\mu,\\sigma)\\), \\(\\mu\\) es la media, la mediana y la moda de \\(X\\). En particular, si \\(Z\\) es \\(N(0,1)\\), para cualquier \\(z\\in \\mathbb{R}\\), las áreas a la izquierda de \\(-z\\) y a la derecha de \\(z\\) son iguales \\[ P(Z\\leq -z)=P(Z\\geq z)=1-P(Z\\leq z) \\] y la mediana de \\(Z\\) es 0. Si \\(\\mu\\) crece, desplaza a la derecha el máximo de la densidad, y con él toda la curva. Si \\(\\sigma\\) crece, la curva se aplana: al aumentar la desviación típica, los valores se alejan más del valor medio. El gráfico siguiente muestra el efecto combinado: Denotaremos por \\(z_q\\) el \\(q\\)-cuantil de una variable normal estándar \\(Z\\). Es decir, \\(z_q\\) es el valor tal que \\(P(Z\\leq z_q)=q\\). Aparte del hecho que \\(z_{0.5}=0\\) (la mediana de \\(Z\\) es 0), hay dos cuantiles más de la normal estándar \\(Z\\) que tendríais que recordar: \\(z_{0.95}=1.64\\); es decir, \\(P(Z\\leq 1.64)=0.95\\) y por lo tanto \\(P(Z\\leq -1.64)=P(Z\\geq 1.64)=0.05\\). \\(z_{0.975}=1.96\\); es decir, \\(P(Z\\leq 1.96)=0.975\\) y por lo tanto \\(P(Z\\leq -1.96)=P(Z\\leq 1.96)=0.025\\) Muy a menudo el valor 1.96 de \\(z_{0.975}\\) se aproxima por 2. Tenéis permiso para hacerlo cuando no dispongáis de medios (R, aplis de móvil) para calcular cuantiles; por ejemplo, en un examen. Pero solo en este caso. Ejemplo 2.2 Supongamos que la concentración de un cierto metabolito es una variable aleatoria de distribución normal, pero cuyos parámetros \\(\\mu\\) y \\(\\sigma\\) varían dependen de si la medimos en personas sanas o en personas con una cierta enfermedad. Sean: \\(X_E\\) la variable aleatoria “Mido la concentración de este metabolito en una persona enferma”, y supongamos que es \\(N(\\mu_E, \\sigma_E)\\). \\(X_S\\) la variable aleatoria “Mido la concentración de este metabolito en una persona sana”, y supongamos que es \\(N(\\mu_S, \\sigma_S)\\). Supongamos, para fijar ideas, que \\(\\mu_E&gt;\\mu_S\\): la concentración media de este metabolito en los enfermos es más alta que en las personas sanas. Podemos usar como prueba diagnóstica de la enfermedad la concentración del metabolito. Para cada valor de referencia \\(x_0\\), nuestra prueba dará: Positivo, si la concentración es mayor o igual que \\(x_0\\) Negativo, si la concentración es menor que \\(x_0\\) Entonces: La sensibilidad de esta prueba es \\[ P(+|E) =P(X_E\\geq x_0)=1-P(X_E&lt; x_0)=1-F_{X_E}(x_0) \\] Su especificidad es \\[ P(-|S)=P(X_S&lt; x_0)=F_{X_S}(x_0) \\] Su tasa de falsos positivos es \\[ P(+|S)=P(X_S\\geq x_0)=1-F_{X_S}(x_0) \\] Al variar \\(x_0\\), tenemos valores diferentes de la sensibilidad y la tasa de falsos positivos. Entonces, podemos dibujar su curva ROC y escoger el umbral con algún criterio o, con su AUC, valorar su capacidad diagnóstica global. Por ejemplo, imaginad que la densidad de \\(X_E\\) es la línea discontinua del gráfico de la izquierda de la figura siguiente y la de \\(X_S\\) la línea continua. Ambas son normales y \\(\\mu_E&gt;\\mu_S\\), porque el pico de la densidad de \\(X_E\\) está a la derecha del de \\(X_S\\). Si para cada \\(x\\) dibujamos los puntos \\((1-F_{X_S}(x),1-F_{X_E}(x))\\), obtenemos la curva ROC de la derecha de dicha figura. 2.3.2 Combinaciones lineales Una de las propiedades de la distribución normal que nos facilitan mucho la vida es que toda combinación lineal de variables aleatorias normales independientes es normal. En concreto, tenemos los resultados siguientes: Teorema 2.1 Sea \\(X\\) una variable \\(N(\\mu,\\sigma)\\). Para todos \\(a,b\\in \\mathbb{R}\\), \\(aX+b\\) es \\(N(a\\mu+b,|a|\\cdot\\sigma)\\). En particular, la tipificada de \\(X\\) \\[ Z=\\dfrac{X-\\mu}{\\sigma} \\] es \\(N(0,1)\\). Más en general: Teorema 2.2 Si \\(X_1,\\ldots,X_n\\) son variables aleatorias normales independientes y \\(a_1,\\ldots,a_n,b\\in \\mathbb{R}\\), entonces \\(a_1X_1+\\cdots +a_nX_n+b\\) es \\(N(\\mu,\\sigma)\\) con \\[ \\mu=a_1\\mu_1+\\cdots +a_n\\mu_n+b,\\ \\sigma=\\sqrt{a_1^2\\sigma^2_1+\\cdots +a_n^2\\sigma^2_n} \\] Que toda combinación lineal de variables normales vuelva a ser del mismo tipo, es decir, normal, es una propiedad muy útil de las variables normales que pocas familias de distribuciones comparten. Por ejemplo, si \\(X\\) es una variable binomial \\(B(n,p)\\) con \\(p\\neq 0\\), la variable \\(2X\\) no es binomial, porque solo toma valores pares y una variable binomial \\(B(m,q)\\) puede tomar todos los valores entre 0 y \\(m\\). Las probabilidades de la normal tipificada determinan las de la normal original, porque si \\(X\\) es \\(N(\\mu,\\sigma)\\): \\[ \\begin{array}{rl} P(a\\leq X\\leq b) &amp; \\displaystyle =P\\Big( \\frac{a-\\mu}{\\sigma}\\leq \\frac{X-\\mu}{\\sigma}\\leq \\frac{b-\\mu}{\\sigma}\\Big)\\\\ &amp; \\displaystyle =P\\Big(\\frac{a-\\mu}{\\sigma}\\leq Z\\leq \\frac{b-\\mu}{\\sigma}\\Big) \\end{array} \\] Esto sirve para deducir fórmulas, y vuestros padres lo usaban para calcular probabilidades (con tablas de probabilidades de la normal estándar); ahora es más cómodo usar una aplicación del móvil. 2.3.3 Intervalos de referencia Un intervalo de referencia del \\(100q\\%\\) para una variable aleatoria \\(X\\) es un intervalo \\([a,b]\\) tal que \\[ P(a\\leq X\\leq b)=q. \\] Es decir, un intervalo de referencia del \\(100q\\%\\) para \\(X\\) es un intervalo que contiene los valores de \\(X\\) del \\(100q\\%\\) de los sujetos de la población. Los más comunes son los intervalos de referencia del 95% (\\(q=0.95\\)), que satisfacen que \\[ P(a\\leq X\\leq b)=0.95 \\] y son los, que por ejemplo, os dan como valores de referencia en las analíticas: Cuando se habla de un intervalo de referencia sin dar la probabilidad, se sobreentiende siempre que es el intervalo de referencia del 95%. Cuando \\(X\\sim N(\\mu,\\sigma)\\), estos intervalos de referencia se toman siempre centrados en la media \\(\\mu\\), es decir, de la forma \\([\\mu-\\text{algo},\\mu+\\text{algo}]\\). Para calcularlos fácilmente, podemos emplear el resultado siguiente: Teorema 2.3 Si \\(X\\) es \\(N(\\mu,\\sigma)\\), un intervalo de referencia del \\(100q\\%\\) para \\(X\\) es \\[ [\\mu- z_{(1+q)/2}\\cdot \\sigma, \\mu+ z_{(1+q)/2}\\cdot \\sigma] \\] donde \\(z_{(1+q)/2}\\) denota el \\((1+q)/2\\)-cuantil de la normal estándar \\(Z\\). Normalmente escribiremos este intervalo como \\[ \\mu\\pm z_{(1+q)/2}\\cdot \\sigma. \\] En efecto: \\[ \\begin{array}{l} P(\\mu-x\\leq X\\leq \\mu+x)=q\\\\ \\qquad \\Longleftrightarrow \\displaystyle P\\Big(\\frac{\\mu-x-\\mu}{\\sigma}\\leq \\frac{X-\\mu}{\\sigma}\\leq \\frac{\\mu+x-\\mu}{\\sigma}\\Big)=q\\\\ \\qquad \\Longleftrightarrow \\displaystyle P(-x/{\\sigma}\\leq Z\\leq {x}/{\\sigma})=q\\\\ \\qquad \\Longleftrightarrow \\displaystyle P(Z\\leq {x}/{\\sigma})-P(Z\\leq -{x}/{\\sigma})=q\\\\ \\qquad \\Longleftrightarrow \\displaystyle P(Z\\leq {x}/{\\sigma})-(1-P(Z\\leq {x}/{\\sigma}))=q\\\\ \\qquad \\text{(por la simetría de $f_Z$ alrededor de 0)}\\\\ \\qquad \\Longleftrightarrow \\displaystyle 2P(Z\\leq {x}/{\\sigma})=q+1\\\\ \\qquad \\Longleftrightarrow P(Z\\leq {x}/{\\sigma})=(1+q)/2\\\\ \\qquad \\Longleftrightarrow x/\\sigma= z_{(1+q)/2}\\\\ \\qquad \\Longleftrightarrow x=z_{(1+q)/2}\\cdot \\sigma \\end{array} \\] Si \\(q=0.95\\), entonces \\((1+q)/2=0.975\\) y entonces \\(z_{0.975}=1.96\\). Por lo tanto, el intervalo de referencia del 95% para una variable \\(X\\) normal \\(N(\\mu,\\sigma)\\) es \\[ \\mu\\pm 1.96\\sigma. \\] Y como este 1.96 a menudo se aproxima por 2, el intervalo de referencia del 95% se simplifica a \\[ \\mu\\pm 2\\sigma. \\] Esto dice, básicamente, que si una población sigue una distribución normal \\(N(\\mu,\\sigma)\\), un 95% de sus individuos tienen su valor de \\(X\\) a distancia como a máximo \\(2\\sigma\\) (“a dos sigmas”) de \\(\\mu\\). Ejemplo 2.3 Según la OMS, las alturas de las mujeres europeas de 18 años (en cm) siguen una ley \\(N(163.1,18.53)\\). ¿Cuál es el intervalo de alturas centrado en la media que contiene a la mitad las europeas de 18 años? Fijaos en que, si llamamos \\(X\\) a la variable aleatoria “Altura de una mujer europea de 18 años en cm”, lo que queremos saber es el intervalo centrado en su media, 163.1, tal que la probabilidad de que la altura de una europea de 18 años escogida al azar pertenezca a este intervalo sea 0.5 Es decir, el intervalo de referencia del 50% para \\(X\\). Sabemos que \\(X\\) es \\(N(163.1,18.53)\\). Si \\(q=0.5\\), entonces \\((1+q)/2=0.75\\) y podemos calcular con R o una aplicación el 0.75-cuantil \\(z_{0.75}\\) de una normal estándar. Por ejemplo, con R, qnorm(0.75) ## [1] 0.6744898 Por lo tanto, redondeando a mm, es el intervalo \\(163.1\\pm 0.6745\\cdot 18.53\\), es decir \\([150.6, 175.6]\\). Esto nos dice que la mitad de las mujeres europeas de 18 años miden entre 150.6 y 175.6. El z-score (z-valor, z-puntuación, z-puntaje…) de un valor \\(x_0\\in \\mathbb{R}\\) respecto de una distribución \\(N(\\mu,\\sigma)\\) es \\[ \\frac{x_0-\\mu}{\\sigma} \\] Es decir, el z-score de \\(x_0\\) es el resultado de “tipificar” \\(x_0\\) en el sentido del Teorema 2.1.2. Si la variable poblacional es normal, cuanto mayor es el valor absoluto del z-score de \\(x_0\\), más “raro” es \\(x_0\\); el signo nos dice si es más grande o más pequeño que el valor esperado \\(\\mu\\). Ejemplo 2.4 Recordad que, según la OMS, las alturas de las mujeres europeas de 18 años siguen una ley \\(N(163.1,18.53)\\). ¿Cuál sería el z-score de una jugadora de baloncesto de 18 años que midiera 191 cm? Sería \\[ \\frac{191-163.1}{18.53}=1.5 \\] Esto se suele leer diciendo que la altura de esta jugadora está 1.5 sigmas por encima de la altura media. Ejemplo 2.5 Se acepta que la presión sistólica se distribuye como una variable normal con valor medio y desviación típica que dependen del sexo y la edad. Para la franja de edad 16-24 años, estos valores son: Para hombres, \\(\\mu=124\\) y \\(\\sigma=13.7\\) Para mujeres, \\(\\mu=117\\) y \\(\\sigma=13.7\\) El modelo de hipertensión-hipotensión aceptado es el descrito en la Figura 2.1. Queremos calcular los límites de cada clase para cada sexo en este grupo de edad. Figura 2.1: Modelo de hipertensión-hipotensión. Veamos: El límite superior del grupo de hipotensión será el valor que deja a la izquierda un 5% de las tensiones: el 0.05-cuantil de la distribución. El límite superior del grupo de riesgo de hipotensión será el valor que deja a la izquierda un 10% de las tensiones: el 0.1-cuantil de la distribución. El límite inferior del grupo de riesgo de hipertensión será el valor que deja a la izquierda un 90% de las tensiones: el 0.9-cuantil de la distribución. El límite inferior del grupo de hipertensión será el valor que deja a la izquierda un 95% de las tensiones: el 0.95-cuantil de la distribución. En los hombres, la tensión sistólica es una variable aleatoria \\(N(124,13.7)\\). Podemos usar R o una aplicación para calcular estos cuantiles. Con R: El 0.05-cuantil es qnorm(0.05,124,13.7) ## [1] 101.4655 El 0.1-cuantil es qnorm(0.1,124,13.7) ## [1] 106.4427 El 0.9-cuantil es qnorm(0.9,124,13.7) ## [1] 141.5573 El 0.95-cuantil es qnorm(0.95,124,13.7) ## [1] 146.5345 Hemos trabajado más de lo necesario: por la simetría, el 0.95-cuantil ha de estar a la misma distancia de \\(\\mu\\) que el 0.05-cuantil, pero a la derecha: \\[ 124-101.4655=22.5345\\Longrightarrow 124+22.5345=126.5345 \\] Lo mismo pasa con el 0.9-cuantil y el 0.1-cuantil, comprobadlo. En resumen,entre los hombres de 16 a 24 años: \\[ \\begin{array}{|ll|} \\hline \\text{Grupo} &amp; \\text{Intervalo}\\\\ \\hline \\text{Hipotenso} &amp; &lt;101.5\\\\ \\text{Prehipotenso} &amp; 101.5\\text{ a }106.4\\\\ \\text{Normotenso} &amp; 106.4\\text{ a }141.6\\\\ \\text{Prehipertenso} &amp; 141.6\\text{ a }146.5\\\\ \\text{Hipertenso} &amp; &gt; 146.5\\\\ \\hline \\end{array} \\] Calculad los límites para las mujeres. 2.4 Test (1) Sea \\(X\\) una variable aleatoria continua de función de densidad: \\[ f_X(x)=\\left\\{\\begin{array}{ll} 0 &amp; \\mbox{si $x&lt;0$}\\\\ 2e^{-2x} &amp; \\mbox{si $x&gt;0$} \\end{array} \\right. \\] ¿Es cierto que \\(P(X=1)=2e^{-2}\\)? Sí No: en realidad \\(P(X=1)=\\int_{-\\infty}^1 2e^{-2x}\\,dx\\) pero me da pereza calcularlo Esto no es la función de densidad de una variable aleatoria continua, porque no es una función continua (en el 0 salta de 0 a 1) Todas las otras respuestas son incorrectas (2) \\(X\\) una variable aleatoria continua de media \\(\\mu\\). ¿Qué vale \\(P(X=\\mu)\\)? 0.5 \\(\\mu\\) 0 Depende de la variable aleatoria Todas las otras respuestas son falsas (3) En una variable aleatoria discreta, su función de densidad (marcad una única respuesta): Es la derivada de la función de distribución. Mide lo denso que es su dominio. Aplicada a un par de números real, nos da la probabilidad de obtener valores dentro del intervalo definido por dichos números. Aplicada a un número real, nos da da la probabilidad de obtener dicho número. Aplicada a un número real, nos da la probabilidad de obtener un valor menor o igual que dicho número. (4) Sea \\(Z\\) una variable aleatoria normal estándar. Marcad las afirmaciones verdaderas. Es asimétrica a la izquierda. Su media es 1. Su desviación típica es 0. Su varianza es 1. Su mediana es 0. (5) Sea \\(X\\) una variable aleatoria \\(N(\\mu,\\sigma)\\) y \\(f_X\\) su función de densidad. ¿Qué vale el área entre la curva \\(y=f_X(x)\\) y el eje de abscisas? 0 \\(\\mu\\) \\(\\sigma\\) 1 Todas las otras respuestas son falsas (6) Sea \\(X\\) una variable aleatoria \\(N(\\mu,\\sigma)\\) y \\(f_X\\) su función de densidad. ¿Cuál de las afirmaciones siguientes es correcta? \\(\\mu\\) es la media de \\(X\\), pero no su mediana \\(\\mu\\) es la media y la mediana de \\(X\\), pero no su moda \\(\\mu\\) es la media, la mediana y la moda de \\(X\\), pero no es verdad que \\(P(X=\\mu)&gt;P(X=a)\\) para todo \\(a\\neq \\mu\\) \\(\\mu\\) es la media, la mediana y la moda de \\(X\\) y \\(P(X=\\mu)&gt;P(X=a)\\) para todo \\(a\\neq \\mu\\) (7) Si la concentración de un cierto metabolito tiene un intervalo de referencia (del 95%) entre 0 y 22 mg/dL, ¿qué podemos afirmar de su distribución? (Marcad la única respuesta correcta.) Que es normal. Que es simétrica, pero no necesariamente normal. Que es asimétrica con cola a la derecha. Que es asimétrica con cola a la izquierda. Que es platicúrtica. (8) ¿Qué distribución es la más adecuada para modelar el número anual de fallecimientos entre enfermos de cáncer tratados con una determinada quimioterapia? Marcad una única respuesta. Normal Binomial Poisson Uniforme acotada (todos los números de fallecimientos entre 0 y un cierto valor \\(N\\) tienen la misma probabilidad) (9) El FME (Flujo Máximo de Expiración) de las chicas de 11 años sigue una distribución aproximadamente normal de media 300 l/min y desviación típica 20 l/min. Marcad las afirmaciones verdaderas: Aproximadamente la mitad de las chicas de 11 años tienen un FME entre 280 l/min y 320 l/min. Alrededor del 95% de las chicas de 11 años tienen un FME entre 280 l/min y 320 l/min. Alrededor del 95% de las chicas de 11 años tienen un FME entre 260 l/min y 340 l/min. Alrededor del 5% de las chicas de 11 años tienen un FME inferior a 260 l/min. Ninguna chica de 11 años tiene FME superior a 360 l/min. (10) En una muestra aleatoria extraída de población sana se encuentra que una variable bioquímica tiene como media 90 y desviación típica 10. Si tomamos una muestra de individuos sanos ¿es razonable esperar que aproximadamente el 95% de ellos tengan un valor de esa variable comprendido entre 70 y 110? (marcad todas las respuestas correctas): Sí, siempre. No, nunca. Si la variable tiene distribución normal, entonces sí. Si la muestra es suficientemente grande, entonces sí. Si la variable tiene distribución normal y la muestra es suficientemente grande, entonces sí. "],
["distribuciones-muestrales-de-estimadores.html", "Lección 3 Distribuciones muestrales de estimadores 3.1 Conceptos básicos 3.2 La media muestral 3.3 La proporción muestral 3.4 La varianza muestral 3.5 La media muestral \\(\\overline{X}\\), de nuevo 3.6 Test", " Lección 3 Distribuciones muestrales de estimadores 3.1 Conceptos básicos El problema típico de la estadística inferencial es: Queremos conocer el valor de una característica en el total de una población, pero no podemos medir esta característica en todos los individuos de la población. Extraemos una muestra de la población, medimos la característica en los individuos de esta muestra, calculamos algo con estas medidas e inferimos el valor de la característica en el global de la población. Inmediatamente surgen varias preguntas que responderemos entre esta lección y la próxima. ¿Cómo tiene que ser la muestra? ¿Qué tenemos que calcular? ¿Con qué precisión podemos inferir la característica de la población? ¿Qué tipo de muestra tomamos? Vamos a suponer de ahora en adelante que tomamos muestras aleatorias simples. También permitimos muestras aleatorias sin reposición si la población es mucho más grande que la muestra, ya que entonces no hay diferencia práctica entre permitir y prohibir las repeticiones. Y en algunos casos muy concretos permitiremos muestras aleatorias sin reposición en general. Sí, ya sabemos que en la práctica casi nunca tomamos muestras aleatorias. En este caso, recordad lo que os explicábamos en la Sección ??. Lo que se suele hacer es describir en detalle las características de la muestra para justificar que, pese a no ser aleatoria, es razonablemente representativa de la población y podría pasar por aleatoria. ¿Qué calculamos? Pues un estimador: alguna función adecuada aplicada a los valores de la muestra. Por ejemplo Si queremos estimar la altura media de los estudiantes de la UIB, tomaremos una muestra aleatoria de estudiantes de la UIB, mediremos sus alturas y calcularemos su media aritmética. Si queremos estimar la proporción de estudiantes de la UIB que han pasado la COVID-19, tomaremos una muestra aleatoria de estudiantes de la UIB, les haremos un test de anticuerpos y calcularemos la proporción muestral de positivos en la muestra. Si queremos estimar el riesgo relativo para un estudiante de la UIB de suspender alguna asignatura si es fumador, tomarmos una muestra aleatoria de estudiantes de la UIB, anotaremos si fuman o no y si han suspendido alguna asignatura o no, y calcularemos la diferencia entre las proporciones muestrales de suspensos entre los fumadores y los no fumadores de la muestra. Fijaos que un estimador es una variable aleatoria, definida sobre la población formada por las muestras de la población de partida, y por lo tanto tiene función de distribución (que genéricamente llamaremos distribución muestral, para indicar que mide la probabilidad de que le pase algo al valor del estimador sobre una muestra), esperanza, desviación típica (a la que se suele llamar error típico del estimador), etc. 3.2 La media muestral Cuando queremos estimar el valor medio de una medida sobre una población, tomamos una muestra de valores y calculamos su media aritmética, ¿verdad? Pues eso. Dada una variable aleatoria \\(X\\), llamamos media muestral (de tamaño \\(n\\)) a la variable aleatoria \\(\\overline{X}\\) “Tomamos una muestra aleatoria de tamaño \\(n\\) de \\(X\\) y calculamos la media aritmética de sus valores”. Tenemos el teorema siguiente Teorema 3.1 Sea \\(X\\) una variable aleatoria de media \\(\\mu_X\\) y desviación típica \\(\\sigma_X\\), y sea \\(\\overline{X}\\) la media muestral de tamaño \\(n\\) de \\(X\\). Entonces: \\(E(\\overline{X})=\\mu_X\\) Si las muestras aleatorias son simples, \\(\\sigma(\\overline{X})=\\dfrac{\\sigma_X}{\\sqrt{n}}\\) Si las muestras aleatorias no son simples y \\(N\\) es el tamaño de la población, \\[ \\sigma(\\overline{X})=\\frac{\\sigma_X}{\\sqrt{n}}\\cdot\\sqrt{\\frac{N-n}{N-1}} \\] Al factor \\[ \\sqrt{\\frac{N-n}{N-1}} \\] que transforma \\(\\sigma(\\overline{X})\\) para muestras aleatorias simples a la desviación típica de \\(\\overline{X}\\) para muestras aleatorias sin reposición se le llama el factor de población finita, y si os fijáis, es el que transformaba la desviación típica de una variable binomial (que cuenta éxitos en muestras aleatorias simples) en la desviación típica de una variable hipergeométrica (que cuenta éxitos en muestras aleatorias sin reposición). Y recordad que si el tamaño de la población \\(N\\) es muy grande comparado con \\(n\\), podemos suponer que una muestra aleatoria sin reposición es simple. Que \\(E(\\overline{X})\\) sea \\(\\mu_X\\) nos indica que \\(\\overline{X}\\) sirve para estimar \\(\\mu_X\\), porque su valor esperado es \\(\\mu_X\\): Si calculáramos muchas medias de muestras aleatorias de \\(X\\), es muy probable que, de media, obtuviéramos un valor muy cercano a \\(\\mu_X\\). Cuando el valor esperado de un estimador es precisamente el parámetro poblacional que se quiere estimar, se dice que el estimador es insesgado. Así, el primer punto del teorema anterior nos dice que la media muestral \\(\\overline{X}\\) es un estimador insesgado de la media poblacional \\(\\mu_X\\). Que \\(\\sigma(\\overline{X})\\) sea \\(\\sigma_X/\\sqrt{n}\\) implica que la variabilidad de las medias muestrales crece con la variabilidad de \\(X\\) y decrece si tomamos muestras de mayor tamaño. A \\(\\sigma_X/\\sqrt{n}\\) se le llama el error típico de la media muestral (para la variable aleatoria \\(X\\) y muestras aleatorias simples de tamaño \\(n\\)). La media muestral \\(\\overline{X}\\) de muestras aleatorias simples de tamaño \\(n\\) de una variable aleatoria \\(X\\) se interpreta formalmente como la variable aleatoria obtenida tomando \\(n\\) copias independientes \\(X_1,\\ldots,X_n\\) de \\(X\\) y calculando \\[ \\overline{X}=\\frac{X_1+\\cdots+X_n}{n}. \\] Por lo tanto, es una combinación lineal de \\(n\\) copias independientes de \\(X\\). Recordando que una combinación de variables aleatorias normales independientes es normal, tenemos el resultado siguiente: Teorema 3.2 Si \\(X\\) es \\(N(\\mu_X,\\sigma_X)\\) y las muestras aleatorias son simples, entonces \\[ \\overline{X}\\text{ es }N(\\mu_X,\\sigma_X/\\sqrt{n}) \\] y por lo tanto \\[ Z=\\frac{\\overline{X}-\\mu_X}{\\sigma_X/\\sqrt{n}}\\text{ es }N(0,1) \\] Si \\(X\\) no es normal, la tesis del teorema anterior sigue siendo cierta “aproximadamente” si \\(n\\) es muy grande. Este resultado, llamado el Teorema central del límite es, como su nombre indica, uno de los más importantes en estadística. Teorema 3.3 Sea \\(X\\) una variable aleatoria cualquiera de esperanza \\(\\mu_X\\) y desviación típica \\(\\sigma_X\\). Si las muestras aleatorias son simples, entonces, cuando \\(n\\to \\infty\\), \\[ \\overline{X}\\text{ tiende a ser }N(\\mu_X, {\\sigma_X}/{\\sqrt{n}}) \\] y por lo tanto \\[ Z=\\frac{\\overline{X}-\\mu_X}{{\\sigma_X}/{\\sqrt{n}}}\\text{ tiende a ser }N(0,1) \\] En resumen, para muestras aleatorias simples: Si \\(X\\) es normal, siempre se tiene que \\(\\overline{X}\\) es \\(N(\\mu_X,{\\sigma_X}/{\\sqrt{n}})\\) Si \\(X\\) no es normal pero \\(n\\) es grande (pongamos \\(n\\geq 40\\), aunque puede ser menor si \\(X\\) se parece a una normal y ha de ser mayor si \\(X\\) es muy diferente de una normal), \\(\\overline{X}\\) es aproximadamente \\(N(\\mu_X,{\\sigma_X}/{\\sqrt{n}})\\) Para muestras que no sean (prácticamente) aleatorias simples, ambos resultados son falsos (incluso usando el factor de población finita), pero si no tenemos nada más… 3.3 La proporción muestral Cuando queremos estimar la proporción de sujetos de una población que tienen una determinada característica, tomamos una muestra y calculamos la proporción de sujetos de la muestra con esta característica, ¿verdad? Pues, de nuevo, eso. Dada una variable aleatoria de Bernoulli \\(X\\) con probabilidad poblacional de éxito \\(p_X\\), llamamos proporción muestral, \\(\\widehat{p}_X\\), a la variable aleatoria consistente en tomar una muestra aleatoria de tamaño \\(n\\) de \\(X\\) y calcular la proporción de éxitos en la muestra: es decir, contar el número total de éxitos y dividir el resultado por \\(n\\) Fijaos en que \\(\\widehat{p}_X\\) es un caso particular de media muestral \\(\\overline{X}\\): estamos calculando medias muestrales de muestras de la variable de Bernoulli \\(X\\). Por lo tanto, todo lo que hemos dicho para medias muestrales vale también para proporciones muestrales. Teorema 3.4 Si \\(X\\) es una variable aleatoria de Bernoulli con probabilidad poblacional de éxito \\(p_X\\): \\(E(\\widehat{p}_X)=p_X\\) Por lo tanto, \\(\\widehat{p}_X\\) es un estimador insesgado de \\(p_X\\). Si calculáramos muchas proporciones muestrales de muestras aleatorias de \\(X\\), es muy probable que, de media, obtuviéramos un valor muy cercano a \\(p_X\\). Si las muestras aleatorias son simples, \\(\\sigma({\\widehat{p}_X})=\\sqrt{\\dfrac{p_X(1-p_X)}{n}}\\). En particular, si fijada la \\(X\\) tomamos muestras de tamaño mayor, la variabilidad de los resultados de \\(\\widehat{p}_X\\) disminuye. Si las muestras aleatorias no son simples (y \\(N\\) es el tamaño de la población), \\[ \\sigma({\\widehat{p}_X})=\\sqrt{\\frac{p_X(1-p_X)}{n}}\\cdot \\sqrt{\\frac{N-n}{N-1}} \\] Y como antes, si \\(N\\) es muy grande relativamente a \\(n\\), podemos suponer en la práctica que una muestra aleatoria sin reposición es simple Si tomamos muestras aleatorias simples de tamaño \\(n\\) de una variable aleatoria Bernoulli \\(X\\): \\(\\sqrt{\\dfrac{p_X(1-p_X)}{n}}\\) es el error típico de la variable aleatoria \\(\\widehat{p}_X\\): su desviación típica. Para cada muestra, \\(\\sqrt{\\dfrac{\\widehat{p}_X(1-\\widehat{p}_X)}{n}}\\) es el error típico de la muestra, que estima el error típico de \\(\\widehat{p}_X\\). Como la proporción muestral es un caso particular de media muestral, por el Teorema Central del Límite tenemos el resultado siguiente: Teorema 3.5 Si \\(n\\) es grande y las muestras aleatorias son simples, \\[ \\widehat{p}_X\\text{ es aproximadamente }N\\Big (p_X,\\sqrt{\\frac{p_X(1-p_X)}{n}}\\Big) \\] y por lo tanto \\[ \\frac{\\widehat{p}_X-p_X}{\\sqrt{\\frac{{p}_X(1-{p}_X)}{n}}}\\text{ es aproximadamente }N(0,1) \\] 3.4 La varianza muestral Dada una variable aleatoria \\(X\\), llamamos: Varianza muestral, \\(\\widetilde{S}_{X}^2\\), a la variable aleatoria consistente en tomar una muestra aleatoria de tamaño \\(n\\) de \\(X\\) y calcular la varianza muestral de sus valores. Desviación típica muestral, \\(\\widetilde{S}_{X}\\), a la variable aleatoria consistente en tomar una muestra aleatoria de tamaño \\(n\\) de \\(X\\) y calcular la desviación típica muestral de sus valores. Formalmente, estas variables se definen tomando \\(n\\) copias independientes \\(X_1,\\ldots,X_n\\) de \\(X\\) y calculando \\[ \\widetilde{S}_{X}^2=\\frac{\\sum_{i=1}^n (X_{i}-\\overline{X})^2}{n-1},\\quad \\widetilde{S}_{X}=+\\sqrt{\\widetilde{S}_{X}^2} \\] Tenemos los dos resultados siguientes para variables poblacionales normales. El primero nos dice que en este caso \\(\\widetilde{S}_{X}^2\\) es un estimador insesgado de la varianza poblacional \\(\\sigma_{X}^2\\). Teorema 3.6 Si \\(X\\) es \\(N(\\mu_X,\\sigma_X)\\) y tomamos muestras aleatorias de tamaño \\(n\\), \\[ E(\\widetilde{S}_{X}^2)=\\sigma_{X}^2. \\] Por lo tanto, si \\(X\\) es normal, esperamos que la varianza muestral de una muestra aleatoria simple grande sea \\(\\sigma_{X}^2\\), en el sentido usual de que si tomáramos muchas muestras aleatorias simples de \\(X\\) de tamaño \\(n\\) grande y calculáramos sus varianzas muestrales, es muy probable que la media de estas varianzas muestrales se aproximara mucho a \\(\\sigma_{X}^2\\). El segundo resultado nos dice que un múltiplo de la distribución muestral de \\(\\widetilde{S}_{X}^2\\) es conocida, lo que nos permite calcular probabilidades. Teorema 3.7 Si \\(X\\) es \\(N(\\mu_X,\\sigma_X)\\) y tomamos muestras aleatorias de tamaño \\(n\\),La variable aleatoria \\[ \\dfrac{(n-1)\\widetilde{S}_{X}^2}{\\sigma_{X}^2} \\] tiene distribución conocida: \\(\\chi_{n-1}^2\\). La distribución \\(\\chi_n^2\\) (la letra griega \\(\\chi\\) en castellano se lee ji; en catalán, khi; en inglés, chi, pronunciado “xai”), donde \\(n\\) son los grados de libertad, es la distribución de probabilidad de la suma de los cuadrados de \\(n\\) variables aleatorias normales estándar independientes. Para R es chisq. Os puede interesar recordar que una variable \\(\\chi_n^2\\): Tien valor esperado \\(\\mu=n\\) y varianza \\(\\sigma^2=2 n\\) Tiene una distribución asimétrica a la derecha, como muestra el gráfico siguiente: curve(dchisq(x,1),col=1,lwd=2,xlim=c(0,20),xlab=&quot;&quot;,ylab=&quot;&quot;,ylim=c(0,0.3),main=&quot;Algunas ji quadrado&quot;) curve(dchisq(x,2),col=2,lwd=2,add=TRUE) curve(dchisq(x,3),col=3,lwd=2,add=TRUE) curve(dchisq(x,4),col=4,lwd=2,add=TRUE) curve(dchisq(x,5),col=5,lwd=2,add=TRUE) curve(dchisq(x,10),col=6,lwd=2,add=TRUE) legend(&quot;topright&quot;,col=1:6,lty=c(1,1), lwd=c(2,2),legend=paste(&quot;n=&quot;,c(1:5,10),sep=&quot;&quot;),cex=0.8) Aunque, por el Teorema Central del Límite, si \\(n\\) es grande, la distribución \\(\\chi_n^2\\) se aproxima a la de una variable normal \\(N(n,\\sqrt{2n})\\). curve(dchisq(x,300),xlim=c(150,450),lwd=2,xlab=&quot;&quot;,ylab=&quot;&quot;,main=&quot;Ji quadrado vs Normal&quot;) curve(dnorm(x,300,sqrt(600)),lwd=2,col=&quot;red&quot;,add=TRUE) legend(&quot;topleft&quot;,col=c(&quot;black&quot;,&quot;red&quot;),lty=c(1,1), lwd=c(2,2),legend=c(&quot;Ji quadrado con n=300&quot;,&quot;Normal&quot;),cex=0.7) Tened cuidado: Si la variable poblacional \\(X\\) no es normal o si las muestras aleatorias no son simples, las conclusiones de los dos teoremas anteriores no son verdaderas, ni tan solo aproximadamente o añadiendo factores de corrección tipo el factor de población finita. Aunque \\(X\\) sea normal, \\(E(\\widetilde{S}_{X})\\neq \\sigma_{X}\\). Aunque \\(X\\) sea normal, si \\(S^2_{X}\\) es la varianza a secas (dividiendo por \\(n\\)), \\(E(S^2_{X})\\neq \\sigma^2_{X}\\). Esto lo podéis comprobar fácilmente, porque \\(S_X^2\\) se obtiene a partir de \\(\\widetilde{S}_{X}\\) cambiando el denominador, \\[ S_X^2=\\frac{n-1}{n} \\widetilde{S}_{X} \\] y por lo tanto \\[ E(S_X^2)=\\frac{n-1}{n}E(\\widetilde{S}_{X})=\\frac{n-1}{n}\\sigma^2_{X} \\] 3.5 La media muestral \\(\\overline{X}\\), de nuevo Recordad que si la variable poblacional \\(X\\) es \\(N(\\mu_X,\\sigma_X)\\) y tomamos muestras aleatorias simples de tamaño \\(n\\), entonces la variable \\[ \\frac{\\overline{X}-\\mu}{\\sigma_{X}/\\sqrt{n}} \\] es normal estándar. Desde el prunto de vista teórico, para óbtener fórmulas, esto será útil, pero normalmente no nos sirve para calcular la probabilidad de que a \\(\\overline{X}\\) le pase algo, porque normalmente no conocemos la desviación típica poblacional \\(\\sigma_{X}\\). ¿Qué pasa si la estimamos por medio de \\(\\widetilde{S}_{X}\\) con la misma muestra con la que calculamos \\(\\overline{X}\\)? Pues que el resultado siguiente nos salva el día, porque la variable que resulta tiene distribución conocida. Teorema 3.8 Sea \\(X\\) una variable \\(N(\\mu_X,\\sigma_X)\\). Si tomamos muestras aleatorias simples de tamaño \\(n\\), la variable aleatoria \\[ T=\\frac{\\overline{X}-\\mu_X}{\\widetilde{S}_{X}/\\sqrt{n}} \\] tiene una distribución conocida, llamada t de Student con \\(n-1\\) grados de libertad, \\(t_{n-1}\\). Al denominador \\(\\widetilde{S}_{X}/\\sqrt{n}\\) se le llama el error típico de la muestra, y estima el error típico \\(\\sigma_X/\\sqrt{n}\\) de la media muestral \\(\\overline{X}\\). Algunas propiedades que conviene que recordéis de las variables \\(T_m\\) que tienen distribución \\(t\\) de Student con \\(m\\) grados de libertad, \\(t_m\\): Su valor esperado es \\(E(T_m)=0\\) (si \\(m&gt;1\\)) y su varianza es \\(Var(T_m)=\\dfrac{m}{m-2}\\) (si \\(m&gt;2\\)). Su función de distribución es simétrica respecto de \\(0\\) (como la de una \\(N(0,1)\\)): \\[ P(T_m\\leq -x)=P(T_m\\geq x)=1-P(T_m\\leq x) \\] Si \\(m\\) es grande, \\(T_m\\) es aproximadamente una \\(N(0,1)\\) (pero con un poco más de varianza: un poco más achatada). Esto es consecuencia del Teorema Central del Límite. curve(dnorm(x),col=1,lwd=2,xlim=c(-4,4),xlab=&quot;&quot;,ylab=&quot;&quot;,ylim=c(0,0.4), main=&quot;Algunas t de Student&quot;) curve(dt(x,2),col=2,lwd=2,add=TRUE) curve(dt(x,3),col=3,lwd=2,add=TRUE) curve(dt(x,4),col=4,lwd=2,add=TRUE) curve(dt(x,5),col=5,lwd=2,add=TRUE) curve(dt(x,10),col=6,lwd=2,add=TRUE) legend(&quot;topleft&quot;,col=1:6,lty=rep(1,6), lwd=rep(2,6), legend=c(&quot;Normal estandar&quot;, paste(&quot;Student con g.l.=&quot;,c(2:5,10),sep=&quot;&quot;)),cex=0.7) curve(dnorm(x),col=1,lwd=2,xlim=c(-4,4),xlab=&quot;&quot;,ylab=&quot;&quot;,ylim=c(0,0.4), main=&quot;t vs Normal estandar&quot;) curve(dt(x,50),col=2,lwd=2,add=TRUE) legend(&quot;topleft&quot;,col=1:2,lty=rep(1,2), lwd=rep(2,2), legend=c(&quot;Normal estandar&quot;, &quot;Student con g.l.=50&quot;),cex=0.7) Denotaremos por \\(t_{m,q}\\) el \\(q\\)-cuantil de una variable aleatoria \\(T_{m}\\) con distribución \\(t_m\\). Es decir, \\(t_{m,q}\\) és el valor tal que \\[ P(T_{m}\\leq t_{m,q})=q \\] Entonces: Por la simetría de la distribución \\(t_m\\), \\[ t_{m,q}=-t_{m,1-q}. \\] Exactamente igual que para la normal estándar Si \\(m\\) es grande, como \\(T_m\\) será aproximadamente una \\(N(0,1)\\), \\(t_{m,q}\\approx z_q\\). No confundáis: Desviación típica de una variable aleatoria: El parámetro poblacional, normalmente desconocido. Es \\(\\sigma_X\\) Desviación típica (muestral o no) de una muestra: El estadístico que calculamos sobre la muestra; lo damos cuando describimos la muestra. Es \\(\\widetilde{S}_X\\) (la muestral) o \\({S}_X\\) (la “verdadera”). Error típico de la media muestral: La desviación típica de la variable media muestral. Es \\(\\sigma_X/\\sqrt{n}\\), con \\(n\\) el tamaño de las muestras. Error típico de una muestra: Estimación del error típico del estimador a partir de la muestra. Es \\(\\widetilde{S}_X/\\sqrt{n}\\), con \\(n\\) el tamaño de la muestra. Fijaos en que el denominador \\(\\sqrt{n}\\) hace que, en general, los errores típicos sean mucho más pequeños que las desviaciones típicas. 3.6 Test (1) Si el tamaño de una muestra aleatoria simple de una variable aleatoria aumenta (marcad todas las afirmaciones correctas): La media muestral siempre disminuye. El error típico de la media muestral siempre disminuye. El error típico de la muestra siempre disminuye. La varianza muestral siempre aumenta. El número de grados de libertad del estimador \\(\\chi^2\\) asociado a la varianza muestral siempre aumenta. Ninguna de las otras afirmaciones es correcta (2) Si queremos disminuir a la mitad el error típico de una media muestral (calculada a partir de muestras aleatorias simples): Tenemos que aumentar en un 50% el tamaño de la muestra Tenemos que doblar el tamaño de la muestra. Tenemos que cuadruplicar el tamaño de la muestra. Tenemos que dividir por 2 el tamaño de la muestra. Tenemos que dividir por 4 el tamaño de la muestra. Ninguna de las otras respuestas es correcta. (3) La prevalencia de una afección en una población es del 10%. Si estimamos dicha prevalencia repetidamente a partir de muestras de tamaño 1000, estas estimaciones siguen una distribución que (marcad todas las afirmaciones correctas): Es una distribución muestral. Es aproximadamente normal. Es binomial. Tiene media 0.1. Tiene media 900. Ninguna de las otras afirmaciones es correcta "]
]
