<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Lección 2 Variables aleatorias continuas | Bioestadística (Medicina UIB)</title>
  <meta name="description" content="Apunts Bioestadística per a Medicina bookdown::gitbook." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Lección 2 Variables aleatorias continuas | Bioestadística (Medicina UIB)" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Apunts Bioestadística per a Medicina bookdown::gitbook." />
  <meta name="github-repo" content="AprendeR-UIB/INREMDN" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Lección 2 Variables aleatorias continuas | Bioestadística (Medicina UIB)" />
  
  <meta name="twitter:description" content="Apunts Bioestadística per a Medicina bookdown::gitbook." />
  



<meta name="date" content="2020-11-16" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="variables-aleatorias-discretas.html"/>
<link rel="next" href="distribuciones-muestrales-de-estimadores.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">INREMDN</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Presentación</a></li>
<li class="chapter" data-level="1" data-path="variables-aleatorias-discretas.html"><a href="variables-aleatorias-discretas.html"><i class="fa fa-check"></i><b>1</b> Variables aleatorias discretas</a><ul>
<li class="chapter" data-level="1.1" data-path="variables-aleatorias-discretas.html"><a href="variables-aleatorias-discretas.html#generalidades-sobre-variables-aleatorias"><i class="fa fa-check"></i><b>1.1</b> Generalidades sobre variables aleatorias</a></li>
<li class="chapter" data-level="1.2" data-path="variables-aleatorias-discretas.html"><a href="variables-aleatorias-discretas.html#densidad-y-distribución"><i class="fa fa-check"></i><b>1.2</b> Densidad y distribución</a></li>
<li class="chapter" data-level="1.3" data-path="variables-aleatorias-discretas.html"><a href="variables-aleatorias-discretas.html#esperanza"><i class="fa fa-check"></i><b>1.3</b> Esperanza</a></li>
<li class="chapter" data-level="1.4" data-path="variables-aleatorias-discretas.html"><a href="variables-aleatorias-discretas.html#varianza-y-desviación-típica"><i class="fa fa-check"></i><b>1.4</b> Varianza y desviación típica</a></li>
<li class="chapter" data-level="1.5" data-path="variables-aleatorias-discretas.html"><a href="variables-aleatorias-discretas.html#cuantiles"><i class="fa fa-check"></i><b>1.5</b> Cuantiles</a></li>
<li class="chapter" data-level="1.6" data-path="variables-aleatorias-discretas.html"><a href="variables-aleatorias-discretas.html#familias-importantes-de-variables-aleatorias-discretas"><i class="fa fa-check"></i><b>1.6</b> Familias importantes de variables aleatorias discretas</a><ul>
<li class="chapter" data-level="1.6.1" data-path="variables-aleatorias-discretas.html"><a href="variables-aleatorias-discretas.html#variables-aleatorias-binomiales"><i class="fa fa-check"></i><b>1.6.1</b> Variables aleatorias binomiales</a></li>
<li class="chapter" data-level="1.6.2" data-path="variables-aleatorias-discretas.html"><a href="variables-aleatorias-discretas.html#variables-aleatorias-hipergeométricas"><i class="fa fa-check"></i><b>1.6.2</b> Variables aleatorias hipergeométricas</a></li>
<li class="chapter" data-level="1.6.3" data-path="variables-aleatorias-discretas.html"><a href="variables-aleatorias-discretas.html#variable-aleatorias-de-poisson"><i class="fa fa-check"></i><b>1.6.3</b> Variable aleatorias de Poisson</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="variables-aleatorias-discretas.html"><a href="variables-aleatorias-discretas.html#test"><i class="fa fa-check"></i><b>1.7</b> Test</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="variables-aleatorias-continuas.html"><a href="variables-aleatorias-continuas.html"><i class="fa fa-check"></i><b>2</b> Variables aleatorias continuas</a><ul>
<li class="chapter" data-level="2.1" data-path="variables-aleatorias-discretas.html"><a href="variables-aleatorias-discretas.html#densidad-y-distribución"><i class="fa fa-check"></i><b>2.1</b> Densidad y distribución</a></li>
<li class="chapter" data-level="2.2" data-path="variables-aleatorias-continuas.html"><a href="variables-aleatorias-continuas.html#esperanza-varianza-cuantiles"><i class="fa fa-check"></i><b>2.2</b> Esperanza, varianza, cuantiles…</a></li>
<li class="chapter" data-level="2.3" data-path="variables-aleatorias-continuas.html"><a href="variables-aleatorias-continuas.html#variables-aleatorias-normales"><i class="fa fa-check"></i><b>2.3</b> Variables aleatorias normales</a><ul>
<li class="chapter" data-level="2.3.1" data-path="variables-aleatorias-continuas.html"><a href="variables-aleatorias-continuas.html#propiedades-básicas"><i class="fa fa-check"></i><b>2.3.1</b> Propiedades básicas</a></li>
<li class="chapter" data-level="2.3.2" data-path="variables-aleatorias-continuas.html"><a href="variables-aleatorias-continuas.html#combinaciones-lineales"><i class="fa fa-check"></i><b>2.3.2</b> Combinaciones lineales</a></li>
<li class="chapter" data-level="2.3.3" data-path="variables-aleatorias-continuas.html"><a href="variables-aleatorias-continuas.html#intervalos-de-referencia"><i class="fa fa-check"></i><b>2.3.3</b> Intervalos de referencia</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="variables-aleatorias-discretas.html"><a href="variables-aleatorias-discretas.html#test"><i class="fa fa-check"></i><b>2.4</b> Test</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="distribuciones-muestrales-de-estimadores.html"><a href="distribuciones-muestrales-de-estimadores.html"><i class="fa fa-check"></i><b>3</b> Distribuciones muestrales de estimadores</a><ul>
<li class="chapter" data-level="3.1" data-path="distribuciones-muestrales-de-estimadores.html"><a href="distribuciones-muestrales-de-estimadores.html#conceptos-básicos"><i class="fa fa-check"></i><b>3.1</b> Conceptos básicos</a></li>
<li class="chapter" data-level="3.2" data-path="distribuciones-muestrales-de-estimadores.html"><a href="distribuciones-muestrales-de-estimadores.html#la-media-muestral"><i class="fa fa-check"></i><b>3.2</b> La media muestral</a></li>
<li class="chapter" data-level="3.3" data-path="distribuciones-muestrales-de-estimadores.html"><a href="distribuciones-muestrales-de-estimadores.html#la-proporción-muestral"><i class="fa fa-check"></i><b>3.3</b> La proporción muestral</a></li>
<li class="chapter" data-level="3.4" data-path="distribuciones-muestrales-de-estimadores.html"><a href="distribuciones-muestrales-de-estimadores.html#la-varianza-muestral"><i class="fa fa-check"></i><b>3.4</b> La varianza muestral</a></li>
<li class="chapter" data-level="3.5" data-path="distribuciones-muestrales-de-estimadores.html"><a href="distribuciones-muestrales-de-estimadores.html#la-media-muestral-overlinex-de-nuevo"><i class="fa fa-check"></i><b>3.5</b> La media muestral <span class="math inline">\(\overline{X}\)</span>, de nuevo</a></li>
<li class="chapter" data-level="3.6" data-path="variables-aleatorias-discretas.html"><a href="variables-aleatorias-discretas.html#test"><i class="fa fa-check"></i><b>3.6</b> Test</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bioestadística (Medicina UIB)</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="variables-aleatorias-continuas" class="section level1">
<h1><span class="header-section-number">Lección 2</span> Variables aleatorias continuas</h1>
<p>Cuando una variable aleatoria discreta puede tomar pocos valores, la probabilidad de cada valor es relevante.</p>
<p><img src="INREMDN_files/figure-html/unnamed-chunk-51-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>Pero cuando puede tomar muchos valores, las probabilidades individuales pueden ser muy pequeñas y entonces lo que más nos interesa es la probabilidad de intervalos de valores.</p>
<p><img src="INREMDN_files/figure-html/unnamed-chunk-52-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>En este curso nos vamos a restringir variables aleatorias continuas <span class="math inline">\(X: \Omega\to \mathbb{R}\)</span> que satisfacen la siguiente propiedad extra: su <strong>función de distribución</strong>
<span class="math display">\[
\begin{array}{rcl}
F_X: \mathbb{R} &amp; \to &amp; [0,1]\\
x &amp;\mapsto &amp;P(X\leq x)
\end{array}
\]</span>
es continua.</p>
<p>Resulta entonces que, si <span class="math inline">\(X\)</span> es una variable aleatoria continua, la probabilidad de que tome cada valor concreto es 0
<span class="math display">\[
P(X=a)=0 \text{ para todo $a\in \mathbb{R}$}.
\]</span></p>
<p>En particular, para una variable aleatoria continua:</p>
<blockquote>
<p><strong>Probabilidad 0 no significa imposible.</strong></p>
</blockquote>
<p>Cada valor de <span class="math inline">\(X\)</span> tiene probabilidad 0, pero cuando tomamos un sujeto, tendrá algún valor de <span class="math inline">\(X\)</span>, ¿no?. Por lo tanto, su valor de <span class="math inline">\(X\)</span> es posible, aunque tenga probabilidad 0.</p>
<p>De <span class="math inline">\(P(X=a)=0\)</span> se deduce que la probabilidad de un suceso definido con una desigualdad es exactamente la misma que la del suceso correspondiente definido con una desigualdad estricta, y en particular, contrariamente a lo que comentábamos para las variables aleatorias discretas
<span class="math display">\[
P(X\leq a)=P(X&lt;a)
\]</span>
porque <span class="math inline">\(P(X\leq a)=P(X&lt;a)+P(X=a)=P(X&lt;a)+0=P(X&lt;a)\)</span>.</p>
<p>Otros ejemplos:</p>
<ul>
<li><span class="math inline">\(P(X\geq a)=P(X&gt; a)+P(X=a)=P(X&gt; a)\)</span></li>
<li><span class="math inline">\(P(a\leq X\leq a)=P(a&lt;X&lt;b)+P(X=a)+P(X=b)\)</span> <span class="math inline">\(=P(a&lt;X&lt;b)\)</span></li>
</ul>
<div id="densidad-y-distribución" class="section level2">
<h2><span class="header-section-number">2.1</span> Densidad y distribución</h2>
<p>Sea <span class="math inline">\(X\)</span> una variable aleatoria continua. Como ya hemos dicho, su <strong>función de distribución</strong> <span class="math inline">\(F_X\)</span> se sigue definiendo como
<span class="math display">\[
a\mapsto F_X(a)=P(X\leq a)
\]</span></p>
<p>Pero pùesto que tenemos que <span class="math inline">\(P(X=a)=0\)</span>, ahora no podemos definir la función de densidad de <span class="math inline">\(X\)</span> como <span class="math inline">\(f_X(a)=P(X=a)\)</span>. ¿Qué podemos hacer?</p>
<p>Recordad que, en las variables aleatorias discretas
<span class="math display">\[
F_X(a)=\sum_{x\leq a} f_X(x)
\]</span></p>
<p>En el contexto de matemáticas “continuas”, la suma <span class="math inline">\(\sum\)</span> se traduce en la integral <span class="math inline">\(\int\)</span>. Se define entonces la <strong>función de densidad</strong> de una variable aleatoria continua <span class="math inline">\(X\)</span> como la función <span class="math inline">\(f_X:\mathbb{R}\to \mathbb{R}\)</span> tal que <span class="math inline">\(f_X(x)\geq 0\)</span>, para todo <span class="math inline">\(x\in \mathbb{R}\)</span>, y
<span class="math display">\[
F_X(a)=\int_{-\infty}^a f_{X}(x)\, dx\quad \text{para todo $a\in \mathbb{R}$.}
\]</span></p>
<p><img src="INREMDN_files/figure-html/dontpanic.png" width="40%" style="display: block; margin: auto;" /></p>
<p>Recordad (o saber por primera vez) que la integral tiene una interpretación sencilla en términos de áreas. En concreto, dados <span class="math inline">\(a\in \mathbb{R}\)</span> y una función “integrable” <span class="math inline">\(f(x)\)</span>, la integral
<span class="math display">\[
\int_{-\infty}^a f_{X}(x)\, dx
\]</span>
es igual al área de la región a la izquierda de la recta vertical <span class="math inline">\(x=a\)</span> comprendida entre la curva <span class="math inline">\(y=f(x)\)</span> y el eje de abscisas <span class="math inline">\(y=0\)</span>. Por lo tanto, la función de densidad <span class="math inline">\(f_X\)</span> de <span class="math inline">\(X\)</span> es la función tal que para todo <span class="math inline">\(a\in \mathbb{R}\)</span>, <span class="math inline">\(F_X(a)\)</span> es igual al <strong>área bajo la curva</strong> <span class="math inline">\(y=f_X(x)\)</span> (entre esta curva y el eje de abscisas) a la izquierda de <span class="math inline">\(x=a\)</span>.</p>
<p><img src="INREMDN_files/figure-html/graficadensidad3.png" width="60%" style="display: block; margin: auto;" /></p>
<p>¿Cuál es la idea intuitiva que hay bajo esta definición de densidad? Suponed que dibujamos histogramas de frecuencias relativas de los valores de <span class="math inline">\(X\)</span> sobre toda la población. Recordad que, en un histograma de estos, la frecuencia relativa (la <strong>probabilidad</strong>) de cada clase es la amplitud de la clase por la altura de su barra, a la que llamábamos la <strong>densidad</strong> de la clase (y por lo tanto, algo tendrá que ver con la densidad de <span class="math inline">\(X\)</span>, ¿no creéis?).</p>
<p>Si dibujamos los histogramas de <span class="math inline">\(X\)</span> tomando clases cada vez más estrechas, sus polígonos de frecuencias tienden a dibujar una curva, que hemos coloreado en rojo en el último histograma:</p>
<p><img src="INREMDN_files/figure-html/unnamed-chunk-55-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p>Cuando hacemos que el ancho de las clases tienda a 0, obtenemos una curva que es el límite de estos polígonos de frecuencias:
<img src="INREMDN_files/figure-html/unnamed-chunk-56-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>Esta curva es precisamente <span class="math inline">\(y=f_X(x)\)</span>. Es decir, la <strong>función de densidad</strong> <span class="math inline">\(f_X\)</span> de una variable aleatoria continua <span class="math inline">\(X\)</span> es la función límite de los polígonos de frecuencias de histogramas de <span class="math inline">\(X\)</span> cuando hacemos que la amplitud de las clases tienda a 0.</p>
<p>Como <span class="math inline">\(P(X\leq a)\)</span> es el área bajo la curva <span class="math inline">\(y=f_X(x)\)</span> a la izquierda de <span class="math inline">\(x=a\)</span>,
<span class="math display">\[
\begin{array}{rl}
P(a\leq X\leq b)\!\!\!\! &amp; =P(X\leq b)-P(X&lt;a)\\
&amp;=P(X\leq b)-P(X\leq a)
\end{array}
\]</span>
es el área bajo la curva <span class="math inline">\(y=f_X(x)\)</span> a la izquierda de <span class="math inline">\(x=b\)</span> <strong>menos</strong> el área bajo la curva <span class="math inline">\(y=f_X(x)\)</span> a la izquierda de <span class="math inline">\(x=a\)</span>, es decir, <strong><span class="math inline">\(P(a\leq X\leq b)\)</span> es igual al área bajo la curva <span class="math inline">\(y=f_X(x)\)</span> entre <span class="math inline">\(x=a\)</span> y <span class="math inline">\(x=b\)</span>.</strong></p>
<p><img src="INREMDN_files/figure-html/entreaib.png" width="60%" style="display: block; margin: auto;" /></p>
<p>Como <span class="math inline">\(P(X&lt;\infty)=P(\Omega)=1\)</span>, <strong>el área total bajo la curva <span class="math inline">\(y=f_X(x)\)</span> es 1.</strong></p>
<p>Sabemos que <span class="math inline">\(P(X=a)=0\)</span>, pero si <span class="math inline">\(\varepsilon&gt;0\)</span> es muy, muy pequeño,
el área bajo <span class="math inline">\(y=f_X(x)\)</span> entre <span class="math inline">\(a-\varepsilon\)</span> y <span class="math inline">\(a+\varepsilon\)</span> es aproximadamente <span class="math inline">\(2\varepsilon\cdot f_X(a)\)</span>.</p>
<p><img src="INREMDN_files/figure-html/density.png" width="60%" style="display: block; margin: auto;" /></p>
<p>Por lo tanto <span class="math inline">\(f_X(a)\)</span> nos da una indicación de la probabilidad de que <span class="math inline">\(X\)</span> valga aproximadamente <span class="math inline">\(a\)</span> (pero <strong>no es</strong> <span class="math inline">\(P(X=a)\)</span>, que vale 0). Es decir, por ejemplo, si <span class="math inline">\(f_X(a)=0.1\)</span> y <span class="math inline">\(f_X(b)=0.5\)</span>, <strong>la probabilidad de que <span class="math inline">\(X\)</span> tome un valor muy cercano a <span class="math inline">\(a\)</span> es 5 veces mayor que la probabilidad de que tome un valor muy cercano a <span class="math inline">\(b\)</span></strong>.</p>

<div class="rmdcaution">
Pero <span class="math inline">\(P(X=a)=P(X=b)=0\)</span>, así que, por favor, evitad decir que “la probabilidad de que <span class="math inline">\(X\)</span> valga <span class="math inline">\(a\)</span> es 5 veces <strong>mayor</strong> que la probabilidad de que valga <span class="math inline">\(b\)</span>”. Sí, ya sabemos que <span class="math inline">\(5\cdot 0=0\)</span>, pero la frase es engañosa.
</div>

</div>
<div id="esperanza-varianza-cuantiles" class="section level2">
<h2><span class="header-section-number">2.2</span> Esperanza, varianza, cuantiles…</h2>
<p>La esperanza y la varianza de una variable aleatoria continua <span class="math inline">\(X\)</span>, con función de densidad <span class="math inline">\(f_X\)</span>, se definen como en el caso discreto, substituyendo la suma <span class="math inline">\(\sum_{x\in D_x}\)</span> por una integral.</p>
<p>La <strong>esperanza</strong> (<strong>media</strong>, <strong>valor esperado</strong>…) de <span class="math inline">\(X\)</span> es
<span class="math display">\[
E(X)=\int_{-\infty}^{\infty}x \cdot f_{X}(x)\, dx
\]</span>
También se escribe <span class="math inline">\(\mu_X\)</span> o simplemente <span class="math inline">\(\mu\)</span>.</p>
<p>Este valor tiene la misma interpretación que en el caso discreto:</p>
<ul>
<li><p>Representa el valor medio de <span class="math inline">\(X\)</span> sobre el total de la población</p></li>
<li><p>Es (con probabilidad 1) el límite de la media aritmética de los valores de <span class="math inline">\(X\)</span> sobre muestras aleatorias simples de tamaño <span class="math inline">\(n\)</span>, cuando <span class="math inline">\(n\to \infty\)</span>.</p></li>
</ul>
<p>Si <span class="math inline">\(g:\mathbb{R}\to \mathbb{R}\)</span> es una función continua,
la <strong>esperanza</strong> de <span class="math inline">\(g(X)\)</span> es
<span class="math display">\[
E(g(X))=\int_{-\infty}^{+\infty} g(x) f_X(x)dx
\]</span></p>
<p>La <strong>varianza</strong> de <span class="math inline">\(X\)</span> es
<span class="math display">\[
Var(X)=E((X-E(X))^2)
\]</span>
y se puede demostrar que es igual a
<span class="math display">\[
Var(X)=E(X^2)-E(X)^2
\]</span>
También se escribe <span class="math inline">\(\sigma_X^2\)</span> o simplemente <span class="math inline">\(\sigma^2\)</span>.</p>
<p>La <strong>desviación típica</strong> de <span class="math inline">\(X\)</span> es
<span class="math display">\[
\sigma(X)=+\sqrt{Var(X)}
\]</span>
y también se escribe <span class="math inline">\(\sigma_X\)</span> o <span class="math inline">\(\sigma\)</span>.</p>
<p>Como en el caso discreto, la varianza y la desviación típica miden la variabilidad de los resultados de <span class="math inline">\(X\)</span> respecto de su valor medio.</p>
<p>Estos parámetros de <span class="math inline">\(X\)</span> tienen las <strong>mismas propiedades</strong> en el caso continuo que en el discreto. Las recordamos:</p>
<ul>
<li><p><span class="math inline">\(E(b)=b\)</span>, si <span class="math inline">\(b\)</span> es una variable aleatoria constante.</p></li>
<li><p><span class="math inline">\(E(a X+b)=a E(X)+b\)</span>.</p></li>
<li><p><span class="math inline">\(E(X+Y)=E(X)+E(Y)\)</span>.</p></li>
<li><p>Si <span class="math inline">\(X\leq Y\)</span>, entonces <span class="math inline">\(E(X)\leq E(Y)\)</span>.</p></li>
<li><p><span class="math inline">\(Var(aX+b)=a^2 Var(X)\)</span>, donde <span class="math inline">\(a,b\)</span> son constantes reales.</p></li>
<li><p><span class="math inline">\(\sigma(aX+b)=|a|\cdot \sigma(X)\)</span>.</p></li>
<li><p><span class="math inline">\(Var(b)=0\)</span>, si <span class="math inline">\(b\)</span> es una variable aleatoria constante</p></li>
<li><p><span class="math inline">\(Var(X+Y)=Var(X)+Var(Y)\)</span> si <span class="math inline">\(X,Y\)</span> son <strong>independientes</strong></p></li>
</ul>
<p>El <strong>cuantil de orden <span class="math inline">\(p\)</span></strong> (o <strong><span class="math inline">\(p\)</span>-cuantil</strong>) de una variable aleatoria continua <span class="math inline">\(X\)</span> es el valor <span class="math inline">\(x_p\in \mathbb{R}\)</span> más pequeño tal que
<span class="math display">\[
F_X(x_p)=P(X\leq x_p)=p
\]</span></p>

<div class="rmdcorbes">
Fijaos en que como <span class="math inline">\(F_X\)</span> es continua, tiende a 0 cuando <span class="math inline">\(x\to -\infty\)</span> y tiende a 1 cuando <span class="math inline">\(x\to -\infty\)</span>, por el Teorema del Valor medio de las funciones continuas (que dice, básicamente, que las funciones continuas no dan saltos) toma todos los valores en (0,1) y por lo tanto dado cualquier <span class="math inline">\(p\in (0,1)\)</span> existe algún <span class="math inline">\(x\)</span> tal que <span class="math inline">\(F_X(x)=p\)</span>.
</div>

<p>La <strong>mediana</strong> de <span class="math inline">\(X\)</span> es su 0.5-cuantil, el <strong>primer</strong> y <strong>tercer cuartiles</strong> son su 0.25-cuantil y su 0.75-cuantil, etc.</p>
</div>
<div id="variables-aleatorias-normales" class="section level2">
<h2><span class="header-section-number">2.3</span> Variables aleatorias normales</h2>
<div id="propiedades-básicas" class="section level3">
<h3><span class="header-section-number">2.3.1</span> Propiedades básicas</h3>
<p>Una variable aleatoria continua <span class="math inline">\(X\)</span> es <strong>normal</strong> (o <strong>tiene distribución normal</strong>) de parámetros
<span class="math inline">\(\mu\)</span> y <span class="math inline">\(\sigma\)</span> (<span class="math inline">\(N(\mu,\sigma)\)</span>, para abreviar) cuando su función de densidad es
<span class="math display">\[
f_{X}(x)=\frac{1}{\sqrt{2\pi}\sigma} e^{{-(x-\mu)^2}/{2\sigma^{2}}} \mbox{
para todo } x\in \mathbb{R}
\]</span></p>
<p>Naturalmente, no os tenéis que saber esta fórmula.</p>
<p><img src="INREMDN_files/figure-html/censored.png" width="35%" style="display: block; margin: auto;" /></p>
<p>Pero sí que tenéis que saber que:</p>
<ul>
<li><p>Una variable aleatoria normal <span class="math inline">\(X\)</span> es continua, y por lo tanto <span class="math inline">\(P(X=x)=0\)</span>, <span class="math inline">\(P(X\leq x)=P(X&lt;x)\)</span> etc.</p></li>
<li><p>Si <span class="math inline">\(X\)</span> es <span class="math inline">\(N(\mu,\sigma)\)</span>, entonces su valor esperado es <span class="math inline">\(E(X)=\mu\)</span> y su desviación típica es <span class="math inline">\(\sigma_X=\sigma\)</span>.</p></li>
</ul>
<p>Una variable aleatoria normal es <strong>típica</strong> (o <strong>estándar</strong>) cuando tiene <span class="math inline">\(\mu=0\)</span> y <span class="math inline">\(\sigma=1\)</span>; la denotaremos usualmente por <span class="math inline">\(Z\)</span>. Por lo tanto,
si <span class="math inline">\(Z\)</span> es <span class="math inline">\(N(0,1)\)</span>, <span class="math inline">\(E(Z)=0\)</span> y <span class="math inline">\(\sigma(Z)=1\)</span>.</p>
<p>La gráfica de la densidad de una variable aleatoria normal es la conocida <strong>campana de Gauss</strong>:</p>
<p><img src="INREMDN_files/figure-html/unnamed-chunk-62-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>La distribución normal es una distribución teórica, no la encontraréis exacta en la vida real. Y pese a su nombre, no es más “normal” que otras distribuciones continuas.</p>
<p><img src="INREMDN_files/figure-html/paranormal.png" width="40%" style="display: block; margin: auto;" /></p>
<p>Su importancia se debe a que muchas distribuciones de la vida real son aproximadamente normales, porque:</p>
<blockquote>
<p>Toda variable aleatoria que consista en tomar <span class="math inline">\(n\)</span> medidas independientes de una o varias variables aleatorias y sumarlas, tiene distribución aproximadamente normal <strong>cuando <span class="math inline">\(n\)</span> es muy grande</strong>, aunque las variables aleatorias de partida no sean normales</p>
</blockquote>

<div class="example">
<p><span id="exm:unnamed-chunk-64" class="example"><strong>Ejemplo 2.1  </strong></span>Una variable binomial <span class="math inline">\(B(n,p)\)</span> se obtiene tomando <span class="math inline">\(n\)</span> medidas independientes de una variable Bernoulli <span class="math inline">\(B(1,p)\)</span> y sumando los resultados. Por lo tanto, por la “regla” anterior, una <span class="math inline">\(B(n,p)\)</span> tendría que ser aproximadamente normal si <span class="math inline">\(n\)</span> es grande. Pues sí, si <span class="math inline">\(n\)</span> es grande (pongamos mayor que 100, aunque si <span class="math inline">\(p\)</span> está lejos de 0 o 1 el tamaño de las muestras puede ser mucho menor), la distribución de una variable <span class="math inline">\(X\)</span> binomial <span class="math inline">\(B(n,p)\)</span> se acerca mucho a la de una normal <span class="math inline">\(N(np,\sqrt{np(1-p)})\)</span>, donde, recordad que si <span class="math inline">\(X\)</span> es <span class="math inline">\(B(n,p)\)</span>, entonces <span class="math inline">\(\mu_X=np\)</span> y <span class="math inline">\(\sigma_X=\sqrt{np(1-p)}\)</span>.</p>
</div>

<p>Por ejemplo, el gráfico siguiente compara las funciones de distribución de una binomial <span class="math inline">\(B(50,0.3)\)</span> y una normal <span class="math inline">\(N(50\cdot 0.3,\sqrt{50\cdot 0.3\cdot 0.7})\)</span>.</p>
<p><img src="INREMDN_files/figure-html/unnamed-chunk-65-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>Para calcular probabilidades de una <span class="math inline">\(N(\mu,\sigma)\)</span>, hay que calcular las integrales a mano.</p>
<p><img src="INREMDN_files/figure-html/emorisa.png" width="30%" style="display: block; margin: auto;" /></p>
<p>O podéis usar R o alguna aplicación para móvil o tablet. Par R, la normal es <code>norm</code>. Así, por ejemplo, si <span class="math inline">\(X\)</span> es <span class="math inline">\(N(1,2)\)</span></p>
<ul>
<li><span class="math inline">\(P(X\leq 1.5)\)</span> es</li>
</ul>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="variables-aleatorias-continuas.html#cb23-1"></a><span class="kw">pnorm</span>(<span class="fl">1.5</span>,<span class="dv">1</span>,<span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 0.5987063</code></pre>
<ul>
<li>El 0.4-cuantil de <span class="math inline">\(X\)</span>, es decir, el valor <span class="math inline">\(q\)</span> tal que <span class="math inline">\(P(X\leq q)=0.4\)</span> es</li>
</ul>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="variables-aleatorias-continuas.html#cb25-1"></a><span class="kw">qnorm</span>(<span class="fl">0.4</span>,<span class="dv">1</span>,<span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 0.4933058</code></pre>
<ul>
<li><span class="math inline">\(P(X=1.5)\)</span> es</li>
</ul>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="variables-aleatorias-continuas.html#cb27-1"></a><span class="kw">dnorm</span>(<span class="fl">1.5</span>,<span class="dv">1</span>,<span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 0.1933341</code></pre>

<div class="rmderror">
¡No! Como <span class="math inline">\(X\)</span> es continua, <span class="math inline">\(P(X=1.5)=0\)</span>. Lo que os da <code>dnorm(1.5,1,2)</code> es el valor de la función de densidad de <span class="math inline">\(X\)</span> en 1.5, que no creemos que os interese mucho.
</div>

<p>Una de las propiedades clave de la distribución normal es su simetría:</p>

<div class="rmdimportant">
Si <span class="math inline">\(X\)</span> es <span class="math inline">\(N(\mu,\sigma)\)</span>, su densidad <span class="math inline">\(f_X\)</span> es simétrica respecto de <span class="math inline">\(x=\mu\)</span>, es decir,
<span class="math display">\[
f_{X}(\mu-x)=f_{X}(\mu+x),
\]</span>
y tiene el máximo en <span class="math inline">\(x=\mu\)</span>. Decimos entonces que <span class="math inline">\(\mu\)</span> es la <strong>moda</strong> de <span class="math inline">\(X\)</span>.
</div>

<p><img src="INREMDN_files/figure-html/unnamed-chunk-72-1.png" width="80%" style="display: block; margin: auto;" /></p>

<div class="rmdnote">
Recordemos que no tiene sentido definir la moda de una variable continua <span class="math inline">\(X\)</span> como el valor <span class="math inline">\(x_0\)</span> tal que <span class="math inline">\(P(X=x_0)\)</span> sea máximo, porque <span class="math inline">\(P(X=x)=0\)</span> para todo <span class="math inline">\(x\in \mathbb{R}\)</span>. Se define entonces la <strong>moda</strong> de una variable continua <span class="math inline">\(X\)</span> como el valor (o los valores) <span class="math inline">\(x_0\)</span> tal(es) que <span class="math inline">\(f_X(x_0)\)</span> es máximo. Como <span class="math inline">\(f_X(x_0)\)</span> mide, como hemos visto, la probabilidad de que <span class="math inline">\(X\)</span> vala aproximadamente <span class="math inline">\(x_0\)</span>, tenemos que la moda de <span class="math inline">\(X\)</span> es el valor que maximiza la probabilidad de que <span class="math inline">\(X\)</span> sea aproximadamente igual a él.
</div>

<p>En particular, si <span class="math inline">\(Z\)</span> es <span class="math inline">\(N(0,1)\)</span>, entonces <span class="math inline">\(f_Z\)</span> es simétrica alrededor de <span class="math inline">\(x=0\)</span>, es decir, <span class="math inline">\(f_{Z}(-x)=f_{Z}(x)\)</span>, y la moda de <span class="math inline">\(Z\)</span> es <span class="math inline">\(x=0\)</span></p>
<p>Recordad que la función de distribución de una variable aleatoria continua <span class="math inline">\(X\)</span>,
<span class="math display">\[
P(X\leq x)=F_X(x)
\]</span>
es el área comprendida entre la densidad <span class="math inline">\(y=f_X(x)\)</span> y el eje de abscisas a la izquierda de <span class="math inline">\(x\)</span>.</p>
<p><img src="INREMDN_files/figure-html/unnamed-chunk-74-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>Entonces, la simetría de <span class="math inline">\(f_X\)</span> hace que, para todo <span class="math inline">\(x\in \mathbb{R}\)</span>, las áreas a la izquierda de <span class="math inline">\(\mu-x\)</span> y a la derecha de <span class="math inline">\(\mu+x\)</span> sean iguales.</p>
<p><img src="INREMDN_files/figure-html/unnamed-chunk-75-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>Es decir,
<span class="math display">\[
P(X\leq \mu-x)=P(X\geq \mu+x)=1-P(X\leq \mu+x)
\]</span></p>
<p>En particular (tomando <span class="math inline">\(x=0\)</span>)
<span class="math display">\[
P(X\leq \mu)=1-P(X\leq \mu)\Rightarrow P(X\leq \mu)=0.5
\]</span>
y por lo tanto, <span class="math inline">\(\mu\)</span> es también la <strong>mediana</strong> de <span class="math inline">\(X\)</span>.</p>

<div class="rmdimportant">
Si <span class="math inline">\(X\)</span> es <span class="math inline">\(N(\mu,\sigma)\)</span>, <span class="math inline">\(\mu\)</span> es la media, la mediana y la moda de <span class="math inline">\(X\)</span>.
</div>

<p>En particular, si <span class="math inline">\(Z\)</span> es <span class="math inline">\(N(0,1)\)</span>, para cualquier <span class="math inline">\(z\in \mathbb{R}\)</span>, las áreas a la izquierda de <span class="math inline">\(-z\)</span> y a la derecha de <span class="math inline">\(z\)</span> son iguales
<span class="math display">\[
P(Z\leq -z)=P(Z\geq z)=1-P(Z\leq z)
\]</span>
y la mediana de <span class="math inline">\(Z\)</span> es 0.</p>
<p>Si <span class="math inline">\(\mu\)</span> crece, desplaza a la derecha el máximo de la densidad, y con él toda la curva.</p>
<p><img src="INREMDN_files/figure-html/unnamed-chunk-78-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>Si <span class="math inline">\(\sigma\)</span> crece, la curva se aplana: al aumentar la desviación típica, los valores se alejan más del valor medio.</p>
<p><img src="INREMDN_files/figure-html/unnamed-chunk-79-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>El gráfico siguiente muestra el efecto combinado:</p>
<p><img src="INREMDN_files/figure-html/unnamed-chunk-80-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>Denotaremos por <span class="math inline">\(z_q\)</span> el <strong><span class="math inline">\(q\)</span>-cuantil</strong> de una variable normal estándar <span class="math inline">\(Z\)</span>. Es decir, <span class="math inline">\(z_q\)</span> es el valor tal que <span class="math inline">\(P(Z\leq z_q)=q\)</span>.</p>
<p>Aparte del hecho que <span class="math inline">\(z_{0.5}=0\)</span> (la mediana de <span class="math inline">\(Z\)</span> es 0), hay dos cuantiles más de la normal estándar <span class="math inline">\(Z\)</span> que tendríais que recordar:</p>
<ul>
<li><p><span class="math inline">\(z_{0.95}=1.64\)</span>; es decir, <span class="math inline">\(P(Z\leq 1.64)=0.95\)</span> y por lo tanto <span class="math inline">\(P(Z\leq -1.64)=P(Z\geq 1.64)=0.05\)</span>.</p></li>
<li><p><span class="math inline">\(z_{0.975}=1.96\)</span>; es decir, <span class="math inline">\(P(Z\leq 1.96)=0.975\)</span> y por lo tanto <span class="math inline">\(P(Z\leq -1.96)=P(Z\leq 1.96)=0.025\)</span></p></li>
</ul>

<div class="rmdmercifulgod">
Muy a menudo el valor 1.96 de <span class="math inline">\(z_{0.975}\)</span> se aproxima por 2. Tenéis permiso para hacerlo cuando no dispongáis de medios (R, aplis de móvil) para calcular cuantiles; por ejemplo, en un examen. Pero solo en este caso.
</div>


<div class="example">
<p><span id="exm:unnamed-chunk-82" class="example"><strong>Ejemplo 2.2  </strong></span>Supongamos que la concentración de un cierto metabolito es una variable aleatoria de distribución normal, pero cuyos parámetros <span class="math inline">\(\mu\)</span> y <span class="math inline">\(\sigma\)</span> varían dependen de si la medimos en personas sanas o en personas con una cierta enfermedad. Sean:</p>
</div>

<ul>
<li><p><span class="math inline">\(X_E\)</span> la variable aleatoria “Mido la concentración de este metabolito en una persona enferma”, y supongamos que es <span class="math inline">\(N(\mu_E, \sigma_E)\)</span>.</p></li>
<li><p><span class="math inline">\(X_S\)</span> la variable aleatoria “Mido la concentración de este metabolito en una persona sana”, y supongamos que es <span class="math inline">\(N(\mu_S, \sigma_S)\)</span>.</p></li>
<li><p>Supongamos, para fijar ideas, que <span class="math inline">\(\mu_E&gt;\mu_S\)</span>: la concentración media de este metabolito en los enfermos es más alta que en las personas sanas.</p></li>
</ul>
<p>Podemos usar como prueba diagnóstica de la enfermedad la concentración del metabolito. Para cada valor de referencia <span class="math inline">\(x_0\)</span>, nuestra prueba dará:</p>
<ul>
<li><p><strong>Positivo</strong>, si la concentración es mayor o igual que <span class="math inline">\(x_0\)</span></p></li>
<li><p><strong>Negativo</strong>, si la concentración es menor que <span class="math inline">\(x_0\)</span></p></li>
</ul>
<p>Entonces:</p>
<ul>
<li><p>La <strong>sensibilidad</strong> de esta prueba es
<span class="math display">\[
P(+|E)  =P(X_E\geq x_0)=1-P(X_E&lt; x_0)=1-F_{X_E}(x_0)
\]</span></p></li>
<li><p>Su <strong>especificidad</strong> es
<span class="math display">\[
P(-|S)=P(X_S&lt; x_0)=F_{X_S}(x_0)
\]</span></p></li>
<li><p>Su <strong>tasa de falsos positivos</strong> es
<span class="math display">\[
P(+|S)=P(X_S\geq  x_0)=1-F_{X_S}(x_0)
\]</span></p></li>
</ul>
<p>Al variar <span class="math inline">\(x_0\)</span>, tenemos valores diferentes de la sensibilidad y la tasa de falsos positivos. Entonces, podemos dibujar su curva ROC y escoger el umbral con algún criterio o, con su AUC, valorar su capacidad diagnóstica global.</p>
<p>Por ejemplo, imaginad que la densidad de <span class="math inline">\(X_E\)</span> es la línea discontinua del gráfico de la izquierda de la figura siguiente y la de <span class="math inline">\(X_S\)</span> la línea continua. Ambas son normales y <span class="math inline">\(\mu_E&gt;\mu_S\)</span>, porque el pico de la densidad de <span class="math inline">\(X_E\)</span> está a la derecha del de <span class="math inline">\(X_S\)</span>.</p>
<p><img src="INREMDN_files/figure-html/rocnormal.png" width="60%" style="display: block; margin: auto;" /></p>
<p>Si para cada <span class="math inline">\(x\)</span> dibujamos los puntos <span class="math inline">\((1-F_{X_S}(x),1-F_{X_E}(x))\)</span>, obtenemos la curva ROC de la derecha de dicha figura.</p>
</div>
<div id="combinaciones-lineales" class="section level3">
<h3><span class="header-section-number">2.3.2</span> Combinaciones lineales</h3>
<p>Una de las propiedades de la distribución normal que nos facilitan mucho la vida es que <strong>toda combinación lineal de variables aleatorias normales independientes es normal</strong>. En concreto, tenemos los resultados siguientes:</p>

<div class="theorem">
<p><span id="thm:comblinnormals" class="theorem"><strong>Teorema 2.1  </strong></span>Sea <span class="math inline">\(X\)</span> una variable <span class="math inline">\(N(\mu,\sigma)\)</span>.</p>
<ol style="list-style-type: decimal">
<li><p>Para todos <span class="math inline">\(a,b\in \mathbb{R}\)</span>, <span class="math inline">\(aX+b\)</span> es <span class="math inline">\(N(a\mu+b,|a|\cdot\sigma)\)</span>.</p></li>
<li><p>En particular, la <strong>tipificada</strong> de <span class="math inline">\(X\)</span>
<span class="math display">\[
Z=\dfrac{X-\mu}{\sigma}
\]</span>
es <span class="math inline">\(N(0,1)\)</span>.</p>
</div></li>
</ol>
<p>Más en general:</p>

<div class="theorem">
<span id="thm:comblinnormals2" class="theorem"><strong>Teorema 2.2  </strong></span>Si <span class="math inline">\(X_1,\ldots,X_n\)</span> son variables aleatorias normales <strong>independientes</strong> y <span class="math inline">\(a_1,\ldots,a_n,b\in \mathbb{R}\)</span>, entonces
<span class="math inline">\(a_1X_1+\cdots +a_nX_n+b\)</span> es <span class="math inline">\(N(\mu,\sigma)\)</span> con
<span class="math display">\[
\mu=a_1\mu_1+\cdots +a_n\mu_n+b,\ 
\sigma=\sqrt{a_1^2\sigma^2_1+\cdots +a_n^2\sigma^2_n}
\]</span>
</div>


<div class="rmdnote">
Que toda combinación lineal de variables normales vuelva a ser del mismo tipo, es decir, normal, es una propiedad muy útil de las variables normales que pocas familias de distribuciones comparten. Por ejemplo, si <span class="math inline">\(X\)</span> es una variable binomial <span class="math inline">\(B(n,p)\)</span> con <span class="math inline">\(p\neq 0\)</span>, la variable <span class="math inline">\(2X\)</span> no es binomial, porque solo toma valores pares y una variable binomial <span class="math inline">\(B(m,q)\)</span> puede tomar todos los valores entre 0 y <span class="math inline">\(m\)</span>.
</div>

<p>Las probabilidades de la normal tipificada determinan las de la normal original, porque si <span class="math inline">\(X\)</span> es <span class="math inline">\(N(\mu,\sigma)\)</span>:
<span class="math display">\[
\begin{array}{rl}
P(a\leq X\leq b) &amp; \displaystyle  =P\Big( \frac{a-\mu}{\sigma}\leq \frac{X-\mu}{\sigma}\leq \frac{b-\mu}{\sigma}\Big)\\ &amp; \displaystyle =P\Big(\frac{a-\mu}{\sigma}\leq Z\leq \frac{b-\mu}{\sigma}\Big)
\end{array}
\]</span>
Esto sirve para deducir fórmulas, y vuestros padres lo usaban para calcular probabilidades (con tablas de probabilidades de la normal estándar); ahora es más cómodo usar una aplicación del móvil.</p>
</div>
<div id="intervalos-de-referencia" class="section level3">
<h3><span class="header-section-number">2.3.3</span> Intervalos de referencia</h3>
<p>Un <strong>intervalo de referencia</strong> del <span class="math inline">\(100q\%\)</span> para una variable aleatoria <span class="math inline">\(X\)</span> es un intervalo <span class="math inline">\([a,b]\)</span> tal que
<span class="math display">\[
P(a\leq X\leq b)=q.
\]</span>
Es decir, un intervalo de referencia del <span class="math inline">\(100q\%\)</span> para <span class="math inline">\(X\)</span> es un intervalo que contiene los valores de <span class="math inline">\(X\)</span> del <span class="math inline">\(100q\%\)</span> de los sujetos de la población.</p>
<p>Los más comunes son los intervalos de referencia del 95% (<span class="math inline">\(q=0.95\)</span>), que satisfacen que
<span class="math display">\[
P(a\leq X\leq b)=0.95
\]</span>
y son los, que por ejemplo, os dan como valores de referencia en las analíticas:</p>
<p><img src="INREMDN_files/figure-html/analit.png" width="80%" style="display: block; margin: auto;" /></p>

<div class="rmdnote">
Cuando se habla de un <strong>intervalo de referencia</strong> sin dar la probabilidad, se sobreentiende siempre que es el intervalo de referencia del 95%.
</div>

<p>Cuando <span class="math inline">\(X\sim N(\mu,\sigma)\)</span>, estos intervalos de referencia se toman siempre <strong>centrados en la media</strong> <span class="math inline">\(\mu\)</span>, es decir, de la forma <span class="math inline">\([\mu-\text{algo},\mu+\text{algo}]\)</span>. Para calcularlos fácilmente, podemos emplear el resultado siguiente:</p>

<div class="theorem">
<p><span id="thm:unnamed-chunk-86" class="theorem"><strong>Teorema 2.3  </strong></span>Si <span class="math inline">\(X\)</span> es <span class="math inline">\(N(\mu,\sigma)\)</span>, un intervalo de referencia del <span class="math inline">\(100q\%\)</span> para <span class="math inline">\(X\)</span> es
<span class="math display">\[
[\mu- z_{(1+q)/2}\cdot \sigma, \mu+ z_{(1+q)/2}\cdot \sigma]
\]</span>
donde <span class="math inline">\(z_{(1+q)/2}\)</span> denota el <span class="math inline">\((1+q)/2\)</span>-cuantil de la normal estándar <span class="math inline">\(Z\)</span>. Normalmente escribiremos este intervalo como
<span class="math display">\[
\mu\pm z_{(1+q)/2}\cdot \sigma.
\]</span></p>
</div>


<div class="rmdcorbes">
<p>En efecto:
<span class="math display">\[
\begin{array}{l}
P(\mu-x\leq X\leq \mu+x)=q\\
\qquad \Longleftrightarrow \displaystyle P\Big(\frac{\mu-x-\mu}{\sigma}\leq \frac{X-\mu}{\sigma}\leq \frac{\mu+x-\mu}{\sigma}\Big)=q\\
\qquad \Longleftrightarrow \displaystyle P(-x/{\sigma}\leq Z\leq {x}/{\sigma})=q\\
\qquad \Longleftrightarrow \displaystyle P(Z\leq {x}/{\sigma})-P(Z\leq -{x}/{\sigma})=q\\
\qquad \Longleftrightarrow \displaystyle P(Z\leq {x}/{\sigma})-(1-P(Z\leq {x}/{\sigma}))=q\\
\qquad \text{(por la simetría de $f_Z$ alrededor de 0)}\\
\qquad \Longleftrightarrow \displaystyle 2P(Z\leq {x}/{\sigma})=q+1\\
\qquad \Longleftrightarrow P(Z\leq {x}/{\sigma})=(1+q)/2\\
\qquad \Longleftrightarrow x/\sigma=
z_{(1+q)/2}\\
\qquad \Longleftrightarrow x=z_{(1+q)/2}\cdot \sigma
\end{array}
\]</span></p>
</div>

<p>Si <span class="math inline">\(q=0.95\)</span>, entonces <span class="math inline">\((1+q)/2=0.975\)</span> y entonces <span class="math inline">\(z_{0.975}=1.96\)</span>. Por lo tanto, el intervalo de referencia del 95% para una variable <span class="math inline">\(X\)</span> normal <span class="math inline">\(N(\mu,\sigma)\)</span> es
<span class="math display">\[
\mu\pm 1.96\sigma.
\]</span>
Y como este 1.96 a menudo se aproxima por 2, el intervalo de referencia del 95% se simplifica a
<span class="math display">\[
\mu\pm 2\sigma.
\]</span>
Esto dice, básicamente, que</p>
<blockquote>
<p>si una población sigue una distribución normal <span class="math inline">\(N(\mu,\sigma)\)</span>, un 95% de sus individuos tienen su valor de <span class="math inline">\(X\)</span> a distancia como a máximo <span class="math inline">\(2\sigma\)</span> (“a dos sigmas”) de <span class="math inline">\(\mu\)</span>.</p>
</blockquote>

<div class="example">
<p><span id="exm:unnamed-chunk-88" class="example"><strong>Ejemplo 2.3  </strong></span>Según la OMS, las alturas de las mujeres europeas de 18 años (en cm) siguen una ley <span class="math inline">\(N(163.1,18.53)\)</span>. ¿Cuál es el intervalo de alturas centrado en la media que contiene a la mitad las europeas de 18 años?</p>
</div>

<p>Fijaos en que, si llamamos <span class="math inline">\(X\)</span> a la variable aleatoria “Altura de una mujer europea de 18 años en cm”, lo que queremos saber es el intervalo centrado en su media, 163.1, tal que la probabilidad de que la altura de una europea de 18 años escogida al azar pertenezca a este intervalo sea 0.5 Es decir, el intervalo de referencia del 50% para <span class="math inline">\(X\)</span>.</p>
<p>Sabemos que <span class="math inline">\(X\)</span> es <span class="math inline">\(N(163.1,18.53)\)</span>. Si <span class="math inline">\(q=0.5\)</span>, entonces <span class="math inline">\((1+q)/2=0.75\)</span> y podemos calcular con R o una aplicación el 0.75-cuantil <span class="math inline">\(z_{0.75}\)</span> de una normal estándar. Por ejemplo, con R,</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="variables-aleatorias-continuas.html#cb29-1"></a><span class="kw">qnorm</span>(<span class="fl">0.75</span>)</span></code></pre></div>
<pre><code>## [1] 0.6744898</code></pre>
<p>Por lo tanto, redondeando a mm, es el intervalo <span class="math inline">\(163.1\pm 0.6745\cdot 18.53\)</span>, es decir <span class="math inline">\([150.6, 175.6]\)</span>. Esto nos dice que la mitad de las mujeres europeas de 18 años miden entre 150.6 y 175.6.</p>
<p>El <strong>z-score</strong> (<strong>z-valor</strong>, <strong>z-puntuación</strong>, <strong>z-puntaje</strong>…) de un valor <span class="math inline">\(x_0\in \mathbb{R}\)</span> respecto de una distribución <span class="math inline">\(N(\mu,\sigma)\)</span> es
<span class="math display">\[
\frac{x_0-\mu}{\sigma}
\]</span></p>
<p>Es decir, el z-score de <span class="math inline">\(x_0\)</span> es el resultado de “tipificar” <span class="math inline">\(x_0\)</span> en el sentido del Teorema <a href="variables-aleatorias-continuas.html#thm:comblinnormals">2.1</a>.2.</p>
<p>Si la variable poblacional es normal, cuanto mayor es el valor absoluto del z-score de <span class="math inline">\(x_0\)</span>, más “raro” es <span class="math inline">\(x_0\)</span>; el signo nos dice si es más grande o más pequeño que el valor esperado <span class="math inline">\(\mu\)</span>.</p>

<div class="example">
<p><span id="exm:unnamed-chunk-90" class="example"><strong>Ejemplo 2.4  </strong></span>Recordad que, según la OMS, las alturas de las mujeres europeas de 18 años siguen una ley <span class="math inline">\(N(163.1,18.53)\)</span>. ¿Cuál sería el z-score de una jugadora de baloncesto de 18 años que midiera 191 cm?</p>
</div>

<p>Sería
<span class="math display">\[ 
\frac{191-163.1}{18.53}=1.5
\]</span></p>
<p>Esto se suele leer diciendo que la altura de esta jugadora está <em>1.5 sigmas por encima de la altura media</em>.</p>

<div class="example">
<p><span id="exm:unnamed-chunk-91" class="example"><strong>Ejemplo 2.5  </strong></span>Se acepta que la presión sistólica se distribuye como una variable normal con valor medio y desviación típica que dependen del sexo y la edad. Para la franja de edad 16-24 años, estos valores son:</p>
<ul>
<li>Para hombres, <span class="math inline">\(\mu=124\)</span> y <span class="math inline">\(\sigma=13.7\)</span></li>
<li>Para mujeres, <span class="math inline">\(\mu=117\)</span> y <span class="math inline">\(\sigma=13.7\)</span></li>
</ul>
<p>El modelo de hipertensión-hipotensión aceptado es el descrito en la Figura <a href="variables-aleatorias-continuas.html#fig:hiperhipo">2.1</a>. Queremos calcular los límites de cada clase para cada sexo en este grupo de edad.</p>
</div>

<div class="figure" style="text-align: center"><span id="fig:hiperhipo"></span>
<img src="INREMDN_files/figure-html/hiperhipo.png" alt="Modelo de hipertensión-hipotensión." width="80%" />
<p class="caption">
Figura 2.1: Modelo de hipertensión-hipotensión.
</p>
</div>
<p>Veamos:</p>
<ul>
<li>El límite superior del grupo de hipotensión será el valor que deja a la izquierda un 5% de las tensiones: el 0.05-cuantil de la distribución.</li>
<li>El límite superior del grupo de riesgo de hipotensión será el valor que deja a la izquierda un 10% de las tensiones: el 0.1-cuantil de la distribución.</li>
<li>El límite inferior del grupo de riesgo de hipertensión será el valor que deja a la izquierda un 90% de las tensiones: el 0.9-cuantil de la distribución.</li>
<li>El límite inferior del grupo de hipertensión será el valor que deja a la izquierda un 95% de las tensiones: el 0.95-cuantil de la distribución.</li>
</ul>
<p>En los hombres, la tensión sistólica es una variable aleatoria <span class="math inline">\(N(124,13.7)\)</span>. Podemos usar R o una aplicación para calcular estos cuantiles. Con R:</p>
<ul>
<li>El 0.05-cuantil es</li>
</ul>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="variables-aleatorias-continuas.html#cb31-1"></a><span class="kw">qnorm</span>(<span class="fl">0.05</span>,<span class="dv">124</span>,<span class="fl">13.7</span>)</span></code></pre></div>
<pre><code>## [1] 101.4655</code></pre>
<ul>
<li>El 0.1-cuantil es</li>
</ul>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="variables-aleatorias-continuas.html#cb33-1"></a><span class="kw">qnorm</span>(<span class="fl">0.1</span>,<span class="dv">124</span>,<span class="fl">13.7</span>)</span></code></pre></div>
<pre><code>## [1] 106.4427</code></pre>
<ul>
<li>El 0.9-cuantil es</li>
</ul>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="variables-aleatorias-continuas.html#cb35-1"></a><span class="kw">qnorm</span>(<span class="fl">0.9</span>,<span class="dv">124</span>,<span class="fl">13.7</span>)</span></code></pre></div>
<pre><code>## [1] 141.5573</code></pre>
<ul>
<li>El 0.95-cuantil es</li>
</ul>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="variables-aleatorias-continuas.html#cb37-1"></a><span class="kw">qnorm</span>(<span class="fl">0.95</span>,<span class="dv">124</span>,<span class="fl">13.7</span>)</span></code></pre></div>
<pre><code>## [1] 146.5345</code></pre>

<div class="rmdromans">
Hemos trabajado más de lo necesario: por la simetría, el 0.95-cuantil ha de estar a la misma distancia de <span class="math inline">\(\mu\)</span> que el 0.05-cuantil, pero a la derecha:
<span class="math display">\[
124-101.4655=22.5345\Longrightarrow  124+22.5345=126.5345
\]</span>
Lo mismo pasa con el 0.9-cuantil y el 0.1-cuantil, comprobadlo.
</div>

<p>En resumen,entre los hombres de 16 a 24 años:
<span class="math display">\[
\begin{array}{|ll|}
\hline
\text{Grupo} &amp; \text{Intervalo}\\ \hline
\text{Hipotenso} &amp; &lt;101.5\\
\text{Prehipotenso} &amp; 101.5\text{ a }106.4\\
\text{Normotenso} &amp; 106.4\text{ a }141.6\\
\text{Prehipertenso} &amp; 141.6\text{ a }146.5\\
\text{Hipertenso} &amp; &gt; 146.5\\ \hline
\end{array}
\]</span></p>

<div class="rmdexercici">
Calculad los límites para las mujeres.
</div>

</div>
</div>
<div id="test" class="section level2">
<h2><span class="header-section-number">2.4</span> Test</h2>
<p><strong>(1)</strong> Sea <span class="math inline">\(X\)</span> una variable aleatoria continua de función de densidad:
<span class="math display">\[
f_X(x)=\left\{\begin{array}{ll}
0 &amp; \mbox{si $x&lt;0$}\\
2e^{-2x} &amp; \mbox{si $x&gt;0$}
\end{array}
\right.
\]</span>
¿Es cierto que <span class="math inline">\(P(X=1)=2e^{-2}\)</span>?</p>
<ol style="list-style-type: decimal">
<li>Sí</li>
<li>No: en realidad <span class="math inline">\(P(X=1)=\int_{-\infty}^1 2e^{-2x}\,dx\)</span> pero me da pereza calcularlo</li>
<li>Esto no es la función de densidad de una variable aleatoria continua, porque no es una función continua (en el 0 salta de 0 a 1)</li>
<li>Todas las otras respuestas son incorrectas</li>
</ol>
<p><strong>(2)</strong> <span class="math inline">\(X\)</span> una variable aleatoria continua de media <span class="math inline">\(\mu\)</span>. ¿Qué vale <span class="math inline">\(P(X=\mu)\)</span>?</p>
<ol style="list-style-type: decimal">
<li>0.5</li>
<li><span class="math inline">\(\mu\)</span></li>
<li>0</li>
<li>Depende de la variable aleatoria</li>
<li>Todas las otras respuestas son falsas</li>
</ol>
<p><strong>(3)</strong> En una variable aleatoria discreta, su función de densidad (marcad una única respuesta):</p>
<ol style="list-style-type: decimal">
<li>Es la derivada de la función de distribución.</li>
<li>Mide lo denso que es su dominio.</li>
<li>Aplicada a un par de números real, nos da la probabilidad de obtener valores dentro del intervalo definido por dichos números.</li>
<li>Aplicada a un número real, nos da da la probabilidad de obtener dicho número.</li>
<li>Aplicada a un número real, nos da la probabilidad de obtener un valor menor o igual que dicho número.</li>
</ol>
<p><strong>(4)</strong> Sea <span class="math inline">\(Z\)</span> una variable aleatoria normal estándar. Marcad las afirmaciones verdaderas.</p>
<ol style="list-style-type: decimal">
<li>Es asimétrica a la izquierda.</li>
<li>Su media es 1.</li>
<li>Su desviación típica es 0.</li>
<li>Su varianza es 1.</li>
<li>Su mediana es 0.</li>
</ol>
<p><strong>(5)</strong> Sea <span class="math inline">\(X\)</span> una variable aleatoria <span class="math inline">\(N(\mu,\sigma)\)</span> y <span class="math inline">\(f_X\)</span> su función de densidad. ¿Qué vale el área entre la curva <span class="math inline">\(y=f_X(x)\)</span> y el eje de abscisas?</p>
<ol style="list-style-type: decimal">
<li>0</li>
<li><span class="math inline">\(\mu\)</span></li>
<li><span class="math inline">\(\sigma\)</span></li>
<li>1<br />
</li>
<li>Todas las otras respuestas son falsas</li>
</ol>
<p><strong>(6)</strong> Sea <span class="math inline">\(X\)</span> una variable aleatoria <span class="math inline">\(N(\mu,\sigma)\)</span> y <span class="math inline">\(f_X\)</span> su función de densidad. ¿Cuál de las afirmaciones siguientes es correcta?</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\mu\)</span> es la media de <span class="math inline">\(X\)</span>, pero no su mediana</li>
<li><span class="math inline">\(\mu\)</span> es la media y la mediana de <span class="math inline">\(X\)</span>, pero no su moda</li>
<li><span class="math inline">\(\mu\)</span> es la media, la mediana y la moda de <span class="math inline">\(X\)</span>, pero no es verdad que <span class="math inline">\(P(X=\mu)&gt;P(X=a)\)</span> para todo <span class="math inline">\(a\neq \mu\)</span><br />
</li>
<li><span class="math inline">\(\mu\)</span> es la media, la mediana y la moda de <span class="math inline">\(X\)</span> y <span class="math inline">\(P(X=\mu)&gt;P(X=a)\)</span> para todo <span class="math inline">\(a\neq \mu\)</span></li>
</ol>
<p><strong>(7)</strong> Si la concentración de un cierto metabolito tiene un intervalo de referencia (del 95%) entre 0 y 22 mg/dL, ¿qué podemos afirmar de su distribución? (Marcad la única respuesta correcta.)</p>
<ol style="list-style-type: decimal">
<li>Que es normal.</li>
<li>Que es simétrica, pero no necesariamente normal.</li>
<li>Que es asimétrica con cola a la derecha.</li>
<li>Que es asimétrica con cola a la izquierda.</li>
<li>Que es platicúrtica.</li>
</ol>
<p><strong>(8)</strong> ¿Qué distribución es la más adecuada para modelar el número anual de fallecimientos entre enfermos de cáncer tratados con una determinada quimioterapia? Marcad una única respuesta.</p>
<ol style="list-style-type: decimal">
<li>Normal</li>
<li>Binomial</li>
<li>Poisson</li>
<li>Uniforme acotada (todos los números de fallecimientos entre 0 y un cierto valor <span class="math inline">\(N\)</span> tienen la misma probabilidad)</li>
</ol>
<p><strong>(9)</strong> El FME (Flujo Máximo de Expiración) de las chicas de 11 años sigue una distribución aproximadamente normal de media 300 l/min y desviación típica 20 l/min. Marcad las afirmaciones verdaderas:</p>
<ol style="list-style-type: decimal">
<li>Aproximadamente la mitad de las chicas de 11 años tienen un FME entre 280 l/min y 320 l/min.</li>
<li>Alrededor del 95% de las chicas de 11 años tienen un FME entre 280 l/min y 320 l/min.</li>
<li>Alrededor del 95% de las chicas de 11 años tienen un FME entre 260 l/min y 340 l/min.</li>
<li>Alrededor del 5% de las chicas de 11 años tienen un FME inferior a 260 l/min.</li>
<li>Ninguna chica de 11 años tiene FME superior a 360 l/min.</li>
</ol>
<p><strong>(10)</strong> En una muestra aleatoria extraída de población sana se encuentra que una variable bioquímica tiene como media 90 y desviación típica 10. Si tomamos una muestra de individuos sanos ¿es razonable esperar que aproximadamente el 95% de ellos tengan un valor de esa variable comprendido entre 70 y 110? (marcad todas las respuestas correctas):</p>
<ol style="list-style-type: decimal">
<li>Sí, siempre.</li>
<li>No, nunca.</li>
<li>Si la variable tiene distribución normal, entonces sí.</li>
<li>Si la muestra es suficientemente grande, entonces sí.</li>
<li>Si la variable tiene distribución normal y la muestra es suficientemente grande, entonces sí.</li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="variables-aleatorias-discretas.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="distribuciones-muestrales-de-estimadores.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection",
"download": ["pdf", "epub"]
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
